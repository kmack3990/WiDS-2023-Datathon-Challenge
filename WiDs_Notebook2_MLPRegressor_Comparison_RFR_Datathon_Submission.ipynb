{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4e8e4659",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Notebook-2---Introduction\" data-toc-modified-id=\"Notebook-2---Introduction-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Notebook 2 - Introduction</a></span><ul class=\"toc-item\"><li><ul class=\"toc-item\"><li><span><a href=\"#Data-Reduction-Function\" data-toc-modified-id=\"Data-Reduction-Function-1.0.1\"><span class=\"toc-item-num\">1.0.1&nbsp;&nbsp;</span>Data Reduction Function</a></span></li><li><span><a href=\"#Set-up-X-and-y\" data-toc-modified-id=\"Set-up-X-and-y-1.0.2\"><span class=\"toc-item-num\">1.0.2&nbsp;&nbsp;</span>Set up X and y</a></span></li><li><span><a href=\"#Time-Series-Split\" data-toc-modified-id=\"Time-Series-Split-1.0.3\"><span class=\"toc-item-num\">1.0.3&nbsp;&nbsp;</span>Time Series Split</a></span></li><li><span><a href=\"#Scaling-Training-Set-and-Test-Set\" data-toc-modified-id=\"Scaling-Training-Set-and-Test-Set-1.0.4\"><span class=\"toc-item-num\">1.0.4&nbsp;&nbsp;</span>Scaling Training Set and Test Set</a></span></li><li><span><a href=\"#Initial-MLPRegressor-Evaluation\" data-toc-modified-id=\"Initial-MLPRegressor-Evaluation-1.0.5\"><span class=\"toc-item-num\">1.0.5&nbsp;&nbsp;</span>Initial MLPRegressor Evaluation</a></span><ul class=\"toc-item\"><li><span><a href=\"#Loss-vs.-Iteration-Analysis\" data-toc-modified-id=\"Loss-vs.-Iteration-Analysis-1.0.5.1\"><span class=\"toc-item-num\">1.0.5.1&nbsp;&nbsp;</span>Loss vs. Iteration Analysis</a></span></li></ul></li><li><span><a href=\"#MLPRegressor-Grid-Search----Best-Parameters-and-Run-Time\" data-toc-modified-id=\"MLPRegressor-Grid-Search----Best-Parameters-and-Run-Time-1.0.6\"><span class=\"toc-item-num\">1.0.6&nbsp;&nbsp;</span>MLPRegressor Grid Search -  Best Parameters and Run Time</a></span></li><li><span><a href=\"#MLP-Grid-Search-Evaluation\" data-toc-modified-id=\"MLP-Grid-Search-Evaluation-1.0.7\"><span class=\"toc-item-num\">1.0.7&nbsp;&nbsp;</span>MLP Grid Search Evaluation</a></span></li><li><span><a href=\"#MLPRegressor---200-Iterations-with-&quot;Optimized-Parameters&quot;\" data-toc-modified-id=\"MLPRegressor---200-Iterations-with-&quot;Optimized-Parameters&quot;-1.0.8\"><span class=\"toc-item-num\">1.0.8&nbsp;&nbsp;</span>MLPRegressor - 200 Iterations with \"Optimized Parameters\"</a></span></li></ul></li><li><span><a href=\"#RFR---Submission\" data-toc-modified-id=\"RFR---Submission-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>RFR - Submission</a></span><ul class=\"toc-item\"><li><span><a href=\"#Transform-Test_Data\" data-toc-modified-id=\"Transform-Test_Data-1.1.1\"><span class=\"toc-item-num\">1.1.1&nbsp;&nbsp;</span>Transform Test_Data</a></span></li><li><span><a href=\"#Fit-RFR-on-Full-training_data-from-WiDS\" data-toc-modified-id=\"Fit-RFR-on-Full-training_data-from-WiDS-1.1.2\"><span class=\"toc-item-num\">1.1.2&nbsp;&nbsp;</span>Fit RFR on Full training_data from WiDS</a></span></li></ul></li></ul></li><li><span><a href=\"#Notebook-2-Conclusion\" data-toc-modified-id=\"Notebook-2-Conclusion-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Notebook 2 Conclusion</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e8a428c",
   "metadata": {},
   "source": [
    "# Notebook 2 - Introduction\n",
    "Workflow Initial MLPRegressor --> MLPRegressor Optimization --> MLPR vs. RFR Comparison--> RFR Baseline WiDS Submission\n",
    "\n",
    "We have refined our ability to prepare and evaluate models in this notebook.  We adopted a data reduction strategy to reduce each parameter's data type to minimize memory load and run times. We will run our MLPRegressor on Google Collab and on my local machine to compare run times to establish more efficient methodology moving forward. \n",
    "\n",
    "We will optimize a Multi-layer Perceptron Regressor(MLPR or MLPRegressor) as they have shown success in predictive modeling with times-series data. We will compare our scoring metrics from the MLPRegressor to our optimized Random Forest Regressor(RFR) from Notebook 1. \n",
    "\n",
    "We will make a submission to WiDS Kaggle Submission page to give ourselves a true baseline to improve upon moving forward on this project. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1c382113",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import python libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "from math import sqrt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ca7cef77",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import scoring metrics\n",
    "from sklearn.ensemble import RandomForestRegressor \n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import r2_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cc95d8f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in clean training set with start date as datetime index\n",
    "time_training_data = pd.read_csv('data/time_training_data_clean.csv', index_col='startdate')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "48e56d4e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>lat</th>\n",
       "      <th>lon</th>\n",
       "      <th>contest-pevpr-sfc-gauss-14d__pevpr</th>\n",
       "      <th>nmme0-tmp2m-34w__cancm30</th>\n",
       "      <th>nmme0-tmp2m-34w__cancm40</th>\n",
       "      <th>nmme0-tmp2m-34w__ccsm30</th>\n",
       "      <th>nmme0-tmp2m-34w__ccsm40</th>\n",
       "      <th>nmme0-tmp2m-34w__cfsv20</th>\n",
       "      <th>nmme0-tmp2m-34w__gfdlflora0</th>\n",
       "      <th>...</th>\n",
       "      <th>Csb</th>\n",
       "      <th>Dfa</th>\n",
       "      <th>Dfb</th>\n",
       "      <th>Dfc</th>\n",
       "      <th>Dsb</th>\n",
       "      <th>Dsc</th>\n",
       "      <th>Dwa</th>\n",
       "      <th>Dwb</th>\n",
       "      <th>month_number</th>\n",
       "      <th>season_number</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>startdate</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2014-09-01</th>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>237.00</td>\n",
       "      <td>29.02</td>\n",
       "      <td>31.64</td>\n",
       "      <td>29.57</td>\n",
       "      <td>30.73</td>\n",
       "      <td>29.71</td>\n",
       "      <td>31.52</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-09-01</th>\n",
       "      <td>290938</td>\n",
       "      <td>0.818182</td>\n",
       "      <td>0.633333</td>\n",
       "      <td>323.63</td>\n",
       "      <td>24.18</td>\n",
       "      <td>26.75</td>\n",
       "      <td>21.09</td>\n",
       "      <td>23.43</td>\n",
       "      <td>21.15</td>\n",
       "      <td>24.08</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-09-01</th>\n",
       "      <td>35819</td>\n",
       "      <td>0.227273</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>385.92</td>\n",
       "      <td>31.16</td>\n",
       "      <td>32.19</td>\n",
       "      <td>33.26</td>\n",
       "      <td>29.80</td>\n",
       "      <td>28.08</td>\n",
       "      <td>33.64</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-09-01</th>\n",
       "      <td>290207</td>\n",
       "      <td>0.818182</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>303.36</td>\n",
       "      <td>23.34</td>\n",
       "      <td>25.66</td>\n",
       "      <td>20.46</td>\n",
       "      <td>23.00</td>\n",
       "      <td>21.76</td>\n",
       "      <td>24.03</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-09-01</th>\n",
       "      <td>289476</td>\n",
       "      <td>0.818182</td>\n",
       "      <td>0.566667</td>\n",
       "      <td>319.97</td>\n",
       "      <td>22.50</td>\n",
       "      <td>24.57</td>\n",
       "      <td>19.67</td>\n",
       "      <td>22.34</td>\n",
       "      <td>21.61</td>\n",
       "      <td>21.83</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 261 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             index       lat       lon  contest-pevpr-sfc-gauss-14d__pevpr  \\\n",
       "startdate                                                                    \n",
       "2014-09-01       0  0.000000  0.833333                              237.00   \n",
       "2014-09-01  290938  0.818182  0.633333                              323.63   \n",
       "2014-09-01   35819  0.227273  0.900000                              385.92   \n",
       "2014-09-01  290207  0.818182  0.600000                              303.36   \n",
       "2014-09-01  289476  0.818182  0.566667                              319.97   \n",
       "\n",
       "            nmme0-tmp2m-34w__cancm30  nmme0-tmp2m-34w__cancm40  \\\n",
       "startdate                                                        \n",
       "2014-09-01                     29.02                     31.64   \n",
       "2014-09-01                     24.18                     26.75   \n",
       "2014-09-01                     31.16                     32.19   \n",
       "2014-09-01                     23.34                     25.66   \n",
       "2014-09-01                     22.50                     24.57   \n",
       "\n",
       "            nmme0-tmp2m-34w__ccsm30  nmme0-tmp2m-34w__ccsm40  \\\n",
       "startdate                                                      \n",
       "2014-09-01                    29.57                    30.73   \n",
       "2014-09-01                    21.09                    23.43   \n",
       "2014-09-01                    33.26                    29.80   \n",
       "2014-09-01                    20.46                    23.00   \n",
       "2014-09-01                    19.67                    22.34   \n",
       "\n",
       "            nmme0-tmp2m-34w__cfsv20  nmme0-tmp2m-34w__gfdlflora0  ...  Csb  \\\n",
       "startdate                                                         ...        \n",
       "2014-09-01                    29.71                        31.52  ...    0   \n",
       "2014-09-01                    21.15                        24.08  ...    0   \n",
       "2014-09-01                    28.08                        33.64  ...    0   \n",
       "2014-09-01                    21.76                        24.03  ...    0   \n",
       "2014-09-01                    21.61                        21.83  ...    0   \n",
       "\n",
       "            Dfa  Dfb  Dfc  Dsb  Dsc  Dwa  Dwb  month_number  season_number  \n",
       "startdate                                                                   \n",
       "2014-09-01    0    0    0    0    0    0    0             9              4  \n",
       "2014-09-01    0    0    0    0    0    0    0             9              4  \n",
       "2014-09-01    0    0    0    0    0    0    0             9              4  \n",
       "2014-09-01    0    0    0    0    0    0    0             9              4  \n",
       "2014-09-01    0    0    0    0    0    0    0             9              4  \n",
       "\n",
       "[5 rows x 261 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "time_training_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e5ea46c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "index                                   int64\n",
       "lat                                   float64\n",
       "lon                                   float64\n",
       "contest-pevpr-sfc-gauss-14d__pevpr    float64\n",
       "nmme0-tmp2m-34w__cancm30              float64\n",
       "                                       ...   \n",
       "Dsc                                     int64\n",
       "Dwa                                     int64\n",
       "Dwb                                     int64\n",
       "month_number                            int64\n",
       "season_number                           int64\n",
       "Length: 261, dtype: object"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "time_training_data.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "92c95f85",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Change index of startdate to date time\n",
    "time_training_data.index =  pd.to_datetime(time_training_data.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9046d14b",
   "metadata": {},
   "source": [
    "### Data Reduction Function\n",
    "- We will reduce run times of models using this data reduction function converting all data types to the data type of lowest memory usage.\n",
    "- Function acquired from my esteemed teammate in the contest Daniel Logan."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fd3dade5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function acquired from Daniel Logan - WiDS teammate\n",
    "def reduce_mem_usage(df, verbose=True):\n",
    "    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
    "    start_mem = df.memory_usage().sum() / 1024**2    \n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtypes\n",
    "        if col_type in numerics:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)  \n",
    "            else:\n",
    "                if c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)    \n",
    "    end_mem = df.memory_usage().sum() / 1024**2\n",
    "    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) / start_mem))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9ae47935",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mem. usage decreased to 355.46 Mb (52.7% reduction)\n"
     ]
    }
   ],
   "source": [
    "time_training_data = reduce_mem_usage(time_training_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbf38de5",
   "metadata": {},
   "source": [
    "We were able to reduce our data set by 52.7% in memory usage."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9b2f9ba",
   "metadata": {},
   "source": [
    "### Set up X and y\n",
    "- X - All features but our target\n",
    "- y - Our target `contest-tmp2m-14d__tmp2m`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ab6cd781",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up independent variables of all columns except target\n",
    "X = time_training_data.drop(['contest-tmp2m-14d__tmp2m'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c279b68b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up array for target data - dependent variable\n",
    "y = time_training_data['contest-tmp2m-14d__tmp2m']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1d7c2af4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(375734, 260)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b69aefac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(375734,)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9472aba3",
   "metadata": {},
   "source": [
    "### Time Series Split\n",
    "- We will run the TimeSeriesSplit to set up our training and test data or really our train and validation data as our real test set will be used for our submission to the WiDS Datathon. \n",
    "- This TimeSeriesSplit method was used for our train test split for the Random Forest Regressor in the last notebook. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c818b5e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.neural_network import MLPRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "f35f2b97",
   "metadata": {},
   "outputs": [],
   "source": [
    "for train_idx, test_idx in TimeSeriesSplit(n_splits=5).split(X):\n",
    "    X_train, X_test = X.iloc[train_idx, :], X.iloc[test_idx,:]\n",
    "    y_train, y_test = y.iloc[train_idx], y.iloc[test_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "5677c77d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>lat</th>\n",
       "      <th>lon</th>\n",
       "      <th>contest-pevpr-sfc-gauss-14d__pevpr</th>\n",
       "      <th>nmme0-tmp2m-34w__cancm30</th>\n",
       "      <th>nmme0-tmp2m-34w__cancm40</th>\n",
       "      <th>nmme0-tmp2m-34w__ccsm30</th>\n",
       "      <th>nmme0-tmp2m-34w__ccsm40</th>\n",
       "      <th>nmme0-tmp2m-34w__cfsv20</th>\n",
       "      <th>nmme0-tmp2m-34w__gfdlflora0</th>\n",
       "      <th>...</th>\n",
       "      <th>Csb</th>\n",
       "      <th>Dfa</th>\n",
       "      <th>Dfb</th>\n",
       "      <th>Dfc</th>\n",
       "      <th>Dsb</th>\n",
       "      <th>Dsc</th>\n",
       "      <th>Dwa</th>\n",
       "      <th>Dwb</th>\n",
       "      <th>month_number</th>\n",
       "      <th>season_number</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>startdate</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2014-09-01</th>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>237.000000</td>\n",
       "      <td>29.020000</td>\n",
       "      <td>31.639999</td>\n",
       "      <td>29.570000</td>\n",
       "      <td>30.730000</td>\n",
       "      <td>29.709999</td>\n",
       "      <td>31.520000</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-09-01</th>\n",
       "      <td>290938</td>\n",
       "      <td>0.818182</td>\n",
       "      <td>0.633333</td>\n",
       "      <td>323.630005</td>\n",
       "      <td>24.180000</td>\n",
       "      <td>26.750000</td>\n",
       "      <td>21.090000</td>\n",
       "      <td>23.430000</td>\n",
       "      <td>21.150000</td>\n",
       "      <td>24.080000</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-09-01</th>\n",
       "      <td>35819</td>\n",
       "      <td>0.227273</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>385.920013</td>\n",
       "      <td>31.160000</td>\n",
       "      <td>32.189999</td>\n",
       "      <td>33.259998</td>\n",
       "      <td>29.799999</td>\n",
       "      <td>28.080000</td>\n",
       "      <td>33.639999</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-09-01</th>\n",
       "      <td>290207</td>\n",
       "      <td>0.818182</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>303.359985</td>\n",
       "      <td>23.340000</td>\n",
       "      <td>25.660000</td>\n",
       "      <td>20.459999</td>\n",
       "      <td>23.000000</td>\n",
       "      <td>21.760000</td>\n",
       "      <td>24.030001</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-09-01</th>\n",
       "      <td>289476</td>\n",
       "      <td>0.818182</td>\n",
       "      <td>0.566667</td>\n",
       "      <td>319.970001</td>\n",
       "      <td>22.500000</td>\n",
       "      <td>24.570000</td>\n",
       "      <td>19.670000</td>\n",
       "      <td>22.340000</td>\n",
       "      <td>21.610001</td>\n",
       "      <td>21.830000</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-05-02</th>\n",
       "      <td>285699</td>\n",
       "      <td>0.818182</td>\n",
       "      <td>0.366667</td>\n",
       "      <td>235.639999</td>\n",
       "      <td>3.780000</td>\n",
       "      <td>4.280000</td>\n",
       "      <td>0.080000</td>\n",
       "      <td>2.670000</td>\n",
       "      <td>4.260000</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-05-02</th>\n",
       "      <td>49586</td>\n",
       "      <td>0.272727</td>\n",
       "      <td>0.733333</td>\n",
       "      <td>573.750000</td>\n",
       "      <td>16.160000</td>\n",
       "      <td>18.850000</td>\n",
       "      <td>15.110000</td>\n",
       "      <td>17.620001</td>\n",
       "      <td>17.250000</td>\n",
       "      <td>16.350000</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-05-02</th>\n",
       "      <td>306167</td>\n",
       "      <td>0.863636</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>202.020004</td>\n",
       "      <td>3.760000</td>\n",
       "      <td>3.940000</td>\n",
       "      <td>0.680000</td>\n",
       "      <td>3.540000</td>\n",
       "      <td>5.380000</td>\n",
       "      <td>2.650000</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-05-02</th>\n",
       "      <td>34235</td>\n",
       "      <td>0.227273</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>555.150024</td>\n",
       "      <td>18.139999</td>\n",
       "      <td>20.209999</td>\n",
       "      <td>17.320000</td>\n",
       "      <td>19.049999</td>\n",
       "      <td>17.990000</td>\n",
       "      <td>17.580000</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-05-02</th>\n",
       "      <td>107335</td>\n",
       "      <td>0.409091</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>522.510010</td>\n",
       "      <td>15.030000</td>\n",
       "      <td>17.459999</td>\n",
       "      <td>14.500000</td>\n",
       "      <td>17.320000</td>\n",
       "      <td>14.630000</td>\n",
       "      <td>14.690000</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>313112 rows × 260 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             index       lat       lon  contest-pevpr-sfc-gauss-14d__pevpr  \\\n",
       "startdate                                                                    \n",
       "2014-09-01       0  0.000000  0.833333                          237.000000   \n",
       "2014-09-01  290938  0.818182  0.633333                          323.630005   \n",
       "2014-09-01   35819  0.227273  0.900000                          385.920013   \n",
       "2014-09-01  290207  0.818182  0.600000                          303.359985   \n",
       "2014-09-01  289476  0.818182  0.566667                          319.970001   \n",
       "...            ...       ...       ...                                 ...   \n",
       "2016-05-02  285699  0.818182  0.366667                          235.639999   \n",
       "2016-05-02   49586  0.272727  0.733333                          573.750000   \n",
       "2016-05-02  306167  0.863636  0.400000                          202.020004   \n",
       "2016-05-02   34235  0.227273  0.800000                          555.150024   \n",
       "2016-05-02  107335  0.409091  0.800000                          522.510010   \n",
       "\n",
       "            nmme0-tmp2m-34w__cancm30  nmme0-tmp2m-34w__cancm40  \\\n",
       "startdate                                                        \n",
       "2014-09-01                 29.020000                 31.639999   \n",
       "2014-09-01                 24.180000                 26.750000   \n",
       "2014-09-01                 31.160000                 32.189999   \n",
       "2014-09-01                 23.340000                 25.660000   \n",
       "2014-09-01                 22.500000                 24.570000   \n",
       "...                              ...                       ...   \n",
       "2016-05-02                  3.780000                  4.280000   \n",
       "2016-05-02                 16.160000                 18.850000   \n",
       "2016-05-02                  3.760000                  3.940000   \n",
       "2016-05-02                 18.139999                 20.209999   \n",
       "2016-05-02                 15.030000                 17.459999   \n",
       "\n",
       "            nmme0-tmp2m-34w__ccsm30  nmme0-tmp2m-34w__ccsm40  \\\n",
       "startdate                                                      \n",
       "2014-09-01                29.570000                30.730000   \n",
       "2014-09-01                21.090000                23.430000   \n",
       "2014-09-01                33.259998                29.799999   \n",
       "2014-09-01                20.459999                23.000000   \n",
       "2014-09-01                19.670000                22.340000   \n",
       "...                             ...                      ...   \n",
       "2016-05-02                 0.080000                 2.670000   \n",
       "2016-05-02                15.110000                17.620001   \n",
       "2016-05-02                 0.680000                 3.540000   \n",
       "2016-05-02                17.320000                19.049999   \n",
       "2016-05-02                14.500000                17.320000   \n",
       "\n",
       "            nmme0-tmp2m-34w__cfsv20  nmme0-tmp2m-34w__gfdlflora0  ...  Csb  \\\n",
       "startdate                                                         ...        \n",
       "2014-09-01                29.709999                    31.520000  ...    0   \n",
       "2014-09-01                21.150000                    24.080000  ...    0   \n",
       "2014-09-01                28.080000                    33.639999  ...    0   \n",
       "2014-09-01                21.760000                    24.030001  ...    0   \n",
       "2014-09-01                21.610001                    21.830000  ...    0   \n",
       "...                             ...                          ...  ...  ...   \n",
       "2016-05-02                 4.260000                     0.010000  ...    0   \n",
       "2016-05-02                17.250000                    16.350000  ...    0   \n",
       "2016-05-02                 5.380000                     2.650000  ...    0   \n",
       "2016-05-02                17.990000                    17.580000  ...    0   \n",
       "2016-05-02                14.630000                    14.690000  ...    0   \n",
       "\n",
       "            Dfa  Dfb  Dfc  Dsb  Dsc  Dwa  Dwb  month_number  season_number  \n",
       "startdate                                                                   \n",
       "2014-09-01    0    0    0    0    0    0    0             9              4  \n",
       "2014-09-01    0    0    0    0    0    0    0             9              4  \n",
       "2014-09-01    0    0    0    0    0    0    0             9              4  \n",
       "2014-09-01    0    0    0    0    0    0    0             9              4  \n",
       "2014-09-01    0    0    0    0    0    0    0             9              4  \n",
       "...         ...  ...  ...  ...  ...  ...  ...           ...            ...  \n",
       "2016-05-02    0    0    0    0    0    0    0             5              2  \n",
       "2016-05-02    0    0    0    0    0    0    0             5              2  \n",
       "2016-05-02    0    0    0    0    0    0    0             5              2  \n",
       "2016-05-02    0    0    0    0    0    0    0             5              2  \n",
       "2016-05-02    0    0    0    0    0    0    0             5              2  \n",
       "\n",
       "[313112 rows x 260 columns]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Check\n",
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "8c558798",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1000x700 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Visualize test set - Make sure testing data is after training data.\n",
    "plt.figure()\n",
    "sns.set(rc={'figure.figsize':(10,7)})\n",
    "y_train.groupby('startdate').mean().plot(label=' training data')\n",
    "y_test.groupby('startdate').mean().plot(label = 'test data')\n",
    "plt.xlabel('Start Date')\n",
    "plt.ylabel('Target - Mean Temp - contest-tmp2m-14d__tmp2m')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1947e98",
   "metadata": {},
   "source": [
    "### Scaling Training Set and Test Set\n",
    "- Unlike the Random Forest Regressor model in Notebook 1, we will need to scale our data for the MLPRegressor. \n",
    "- We will move ahead with a standard scale.\n",
    "- Scaling was carried out after the train test split or TimeSeriesSplit to prevent data leakage caused by scaling data set together. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "9fc08d54",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "d5f9bbf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc_X = StandardScaler()\n",
    "X_trainscaled=sc_X.fit_transform(X_train)\n",
    "X_testscaled=sc_X.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "28fb66fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 2.04532533\n",
      "Iteration 2, loss = 0.39310842\n",
      "Iteration 3, loss = 0.32106602\n",
      "Iteration 4, loss = 0.28316622\n",
      "Iteration 5, loss = 0.26270250\n",
      "Iteration 6, loss = 0.24549686\n",
      "Iteration 7, loss = 0.23248149\n",
      "Iteration 8, loss = 0.22277903\n",
      "Iteration 9, loss = 0.21456016\n",
      "Iteration 10, loss = 0.20773738\n",
      "Iteration 11, loss = 0.20164685\n",
      "Iteration 12, loss = 0.19679936\n",
      "Iteration 13, loss = 0.19283130\n",
      "Iteration 14, loss = 0.18799976\n",
      "Iteration 15, loss = 0.18445011\n",
      "Iteration 16, loss = 0.18186433\n",
      "Iteration 17, loss = 0.17850235\n",
      "Iteration 18, loss = 0.17571395\n",
      "Iteration 19, loss = 0.17379656\n",
      "Iteration 20, loss = 0.17130016\n",
      "Iteration 21, loss = 0.16885780\n",
      "Iteration 22, loss = 0.16706870\n",
      "Iteration 23, loss = 0.16405897\n",
      "Iteration 24, loss = 0.16283499\n",
      "Iteration 25, loss = 0.16018233\n",
      "Iteration 26, loss = 0.15931057\n",
      "Iteration 27, loss = 0.15841832\n",
      "Iteration 28, loss = 0.15551032\n",
      "Iteration 29, loss = 0.15468517\n",
      "Iteration 30, loss = 0.15318294\n",
      "Iteration 31, loss = 0.15269531\n",
      "Iteration 32, loss = 0.15197254\n",
      "Iteration 33, loss = 0.14954753\n",
      "Iteration 34, loss = 0.14837674\n",
      "Iteration 35, loss = 0.14731917\n",
      "Iteration 36, loss = 0.14575367\n",
      "Iteration 37, loss = 0.14561427\n",
      "Iteration 38, loss = 0.14368584\n",
      "Iteration 39, loss = 0.14389476\n",
      "Iteration 40, loss = 0.14371448\n",
      "Iteration 41, loss = 0.14234587\n",
      "Iteration 42, loss = 0.14110936\n",
      "Iteration 43, loss = 0.14091141\n",
      "Iteration 44, loss = 0.14009954\n",
      "Iteration 45, loss = 0.13934019\n",
      "Iteration 46, loss = 0.13808868\n",
      "Iteration 47, loss = 0.13756303\n",
      "Iteration 48, loss = 0.13727737\n",
      "Iteration 49, loss = 0.13567177\n",
      "Iteration 50, loss = 0.13538173\n",
      "Iteration 51, loss = 0.13558051\n",
      "Iteration 52, loss = 0.13396142\n",
      "Iteration 53, loss = 0.13391056\n",
      "Iteration 54, loss = 0.13294026\n",
      "Iteration 55, loss = 0.13290755\n",
      "Iteration 56, loss = 0.13227418\n",
      "Iteration 57, loss = 0.13114962\n",
      "Iteration 58, loss = 0.13103851\n",
      "Iteration 59, loss = 0.13030266\n",
      "Iteration 60, loss = 0.12978693\n",
      "Iteration 61, loss = 0.12898116\n",
      "Iteration 62, loss = 0.12931209\n",
      "Iteration 63, loss = 0.12864215\n",
      "Iteration 64, loss = 0.12793555\n",
      "Iteration 65, loss = 0.12760359\n",
      "Iteration 66, loss = 0.12641830\n",
      "Iteration 67, loss = 0.12601000\n",
      "Iteration 68, loss = 0.12573410\n",
      "Iteration 69, loss = 0.12491419\n",
      "Iteration 70, loss = 0.12536608\n",
      "Iteration 71, loss = 0.12415244\n",
      "Iteration 72, loss = 0.12450612\n",
      "Iteration 73, loss = 0.12508573\n",
      "Iteration 74, loss = 0.12264054\n",
      "Iteration 75, loss = 0.12290639\n",
      "Iteration 76, loss = 0.12248208\n",
      "Iteration 77, loss = 0.12260181\n",
      "Iteration 78, loss = 0.12182968\n",
      "Iteration 79, loss = 0.12076272\n",
      "Iteration 80, loss = 0.12157737\n",
      "Iteration 81, loss = 0.12099624\n",
      "Iteration 82, loss = 0.12104895\n",
      "Iteration 83, loss = 0.11938014\n",
      "Iteration 84, loss = 0.12052817\n",
      "Iteration 85, loss = 0.11964590\n",
      "Iteration 86, loss = 0.11892760\n",
      "Iteration 87, loss = 0.11923068\n",
      "Iteration 88, loss = 0.11852072\n",
      "Iteration 89, loss = 0.11864776\n",
      "Iteration 90, loss = 0.11772069\n",
      "Iteration 91, loss = 0.11760041\n",
      "Iteration 92, loss = 0.11716669\n",
      "Iteration 93, loss = 0.11704229\n",
      "Iteration 94, loss = 0.11621897\n",
      "Iteration 95, loss = 0.11679520\n",
      "Iteration 96, loss = 0.11639352\n",
      "Iteration 97, loss = 0.11534935\n",
      "Iteration 98, loss = 0.11606756\n",
      "Iteration 99, loss = 0.11551397\n",
      "Iteration 100, loss = 0.11484713\n",
      "Iteration 101, loss = 0.11546663\n",
      "Iteration 102, loss = 0.11404456\n",
      "Iteration 103, loss = 0.11389698\n",
      "Iteration 104, loss = 0.11422433\n",
      "Iteration 105, loss = 0.11422343\n",
      "Iteration 106, loss = 0.11337326\n",
      "Iteration 107, loss = 0.11346187\n",
      "Iteration 108, loss = 0.11322951\n",
      "Iteration 109, loss = 0.11330746\n",
      "Iteration 110, loss = 0.11237352\n",
      "Iteration 111, loss = 0.11220486\n",
      "Iteration 112, loss = 0.11201358\n",
      "Iteration 113, loss = 0.11255476\n",
      "Iteration 114, loss = 0.11225842\n",
      "Iteration 115, loss = 0.11136105\n",
      "Iteration 116, loss = 0.11168375\n",
      "Iteration 117, loss = 0.11095250\n",
      "Iteration 118, loss = 0.11132822\n",
      "Iteration 119, loss = 0.11062547\n",
      "Iteration 120, loss = 0.11075992\n",
      "Iteration 121, loss = 0.11077712\n",
      "Iteration 122, loss = 0.10984595\n",
      "Iteration 123, loss = 0.10956232\n",
      "Iteration 124, loss = 0.11004432\n",
      "Iteration 125, loss = 0.10941263\n",
      "Iteration 126, loss = 0.10856401\n",
      "Iteration 127, loss = 0.10933931\n",
      "Iteration 128, loss = 0.10910508\n",
      "Iteration 129, loss = 0.10834796\n",
      "Iteration 130, loss = 0.10887268\n",
      "Iteration 131, loss = 0.10831674\n",
      "Iteration 132, loss = 0.10827360\n",
      "Iteration 133, loss = 0.10775965\n",
      "Iteration 134, loss = 0.10770056\n",
      "Iteration 135, loss = 0.10743483\n",
      "Iteration 136, loss = 0.10704511\n",
      "Iteration 137, loss = 0.10738379\n",
      "Iteration 138, loss = 0.10722922\n",
      "Iteration 139, loss = 0.10659810\n",
      "Iteration 140, loss = 0.10730078\n",
      "Iteration 141, loss = 0.10671529\n",
      "Iteration 142, loss = 0.10628884\n",
      "Iteration 143, loss = 0.10565194\n",
      "Iteration 144, loss = 0.10653304\n",
      "Iteration 145, loss = 0.10580532\n",
      "Iteration 146, loss = 0.10610719\n",
      "Iteration 147, loss = 0.10542191\n",
      "Iteration 148, loss = 0.10558782\n",
      "Iteration 149, loss = 0.10556073\n",
      "Iteration 150, loss = 0.10590409\n",
      "Iteration 151, loss = 0.10457442\n",
      "Iteration 152, loss = 0.10574777\n",
      "Iteration 153, loss = 0.10494768\n",
      "Iteration 154, loss = 0.10451796\n",
      "Iteration 155, loss = 0.10465692\n",
      "Iteration 156, loss = 0.10413120\n",
      "Iteration 157, loss = 0.10422073\n",
      "Iteration 158, loss = 0.10407996\n",
      "Iteration 159, loss = 0.10337566\n",
      "Iteration 160, loss = 0.10405418\n",
      "Iteration 161, loss = 0.10319503\n",
      "Iteration 162, loss = 0.10332270\n",
      "Iteration 163, loss = 0.10346247\n",
      "Iteration 164, loss = 0.10317548\n",
      "Iteration 165, loss = 0.10297489\n",
      "Iteration 166, loss = 0.10293467\n",
      "Iteration 167, loss = 0.10306985\n",
      "Iteration 168, loss = 0.10251854\n",
      "Iteration 169, loss = 0.10204389\n",
      "Iteration 170, loss = 0.10279692\n",
      "Iteration 171, loss = 0.10255768\n",
      "Iteration 172, loss = 0.10228901\n",
      "Iteration 173, loss = 0.10204469\n",
      "Iteration 174, loss = 0.10235418\n",
      "Iteration 175, loss = 0.10167121\n",
      "Iteration 176, loss = 0.10145332\n",
      "Iteration 177, loss = 0.10192758\n",
      "Iteration 178, loss = 0.10190777\n",
      "Iteration 179, loss = 0.10096845\n",
      "Iteration 180, loss = 0.10095728\n",
      "Iteration 181, loss = 0.10145210\n",
      "Iteration 182, loss = 0.10064443\n",
      "Iteration 183, loss = 0.10080530\n",
      "Iteration 184, loss = 0.10117985\n",
      "Iteration 185, loss = 0.10089194\n",
      "Iteration 186, loss = 0.10006934\n",
      "Iteration 187, loss = 0.10030082\n",
      "Iteration 188, loss = 0.09991087\n",
      "Iteration 189, loss = 0.10075584\n",
      "Iteration 190, loss = 0.09948067\n",
      "Iteration 191, loss = 0.09967786\n",
      "Iteration 192, loss = 0.09988814\n",
      "Iteration 193, loss = 0.09981245\n",
      "Iteration 194, loss = 0.09936004\n",
      "Iteration 195, loss = 0.09971038\n",
      "Iteration 196, loss = 0.09971689\n",
      "Iteration 197, loss = 0.09926481\n",
      "Iteration 198, loss = 0.09923469\n",
      "Iteration 199, loss = 0.09930714\n",
      "Iteration 200, loss = 0.09917278\n",
      "MLPRegressor(random_state=32, verbose=1)\n",
      "CPU times: user 1h 21min 51s, sys: 3min 21s, total: 1h 25min 13s\n",
      "Wall time: 15min 41s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kileymack/opt/anaconda3/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#Instantiate\n",
    "MLPmodel = MLPRegressor(verbose = 1, random_state = 32)\n",
    "#Fit Model\n",
    "MLPmodel.fit(X_trainscaled, y_train)\n",
    "print(MLPmodel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "d0dea1b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Predictions from MLP Model with scaled training data\n",
    "pred = MLPmodel.predict(X_testscaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "f591e019",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gather results in dataframe for visualization of expected vs. predictions\n",
    "results_MLP = pd.DataFrame(data = {'Actual':y_test, \\\n",
    "                                   'Predictions':pred}, index=y_test.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "c42afc8d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Actual</th>\n",
       "      <th>Predictions</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>startdate</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2016-01-01</th>\n",
       "      <td>8.568278</td>\n",
       "      <td>7.288152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-01-01</th>\n",
       "      <td>10.872447</td>\n",
       "      <td>10.716764</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-01-01</th>\n",
       "      <td>-7.024219</td>\n",
       "      <td>-7.287463</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-01-01</th>\n",
       "      <td>-6.184810</td>\n",
       "      <td>-5.734422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-01-01</th>\n",
       "      <td>9.002110</td>\n",
       "      <td>8.020726</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-08-31</th>\n",
       "      <td>19.772009</td>\n",
       "      <td>12.712725</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-08-31</th>\n",
       "      <td>19.998930</td>\n",
       "      <td>12.922713</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-08-31</th>\n",
       "      <td>20.392469</td>\n",
       "      <td>12.926745</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-08-31</th>\n",
       "      <td>10.406187</td>\n",
       "      <td>6.491517</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-08-31</th>\n",
       "      <td>15.910995</td>\n",
       "      <td>11.701444</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>125244 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               Actual  Predictions\n",
       "startdate                         \n",
       "2016-01-01   8.568278     7.288152\n",
       "2016-01-01  10.872447    10.716764\n",
       "2016-01-01  -7.024219    -7.287463\n",
       "2016-01-01  -6.184810    -5.734422\n",
       "2016-01-01   9.002110     8.020726\n",
       "...               ...          ...\n",
       "2016-08-31  19.772009    12.712725\n",
       "2016-08-31  19.998930    12.922713\n",
       "2016-08-31  20.392469    12.926745\n",
       "2016-08-31  10.406187     6.491517\n",
       "2016-08-31  15.910995    11.701444\n",
       "\n",
       "[125244 rows x 2 columns]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "488af1ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R-squared = 0.851\n",
      "RMSE = 3.662\n"
     ]
    }
   ],
   "source": [
    "# Error Metrics for initial MLP\n",
    "print('R-squared = {:.3f}'.format(r2_score(results_MLP['Actual'],results_MLP['Predictions'])))\n",
    "print('RMSE = {:.3f}'.format(sqrt(mean_squared_error(results_MLP['Actual'],results_MLP['Predictions'])))) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cfbde04",
   "metadata": {},
   "source": [
    "### Initial MLPRegressor Evaluation\n",
    "Alright we are getting reasonable results when scaling the data.  These metrics are not better than our Random Forest Regressor in the last notebook.  We will try to optimize the MLPRegressor with a grid search and hope to get better scoring metrics. Initial MLPRegressor using Google Collab took 31min 43s. With Jupyter Notebooks on my cpu, the run time was 13 min 24s.  \n",
    "\n",
    "**Optimized RFR**\n",
    "- R-squared = 0.909\n",
    "- RMSE = 1.694\n",
    "\n",
    "**MLPR - Initial attempt** -- 5 splits on Times Series split - With smaller test set in May-Aug 2016\n",
    "- R-squared = 0.828\n",
    "- RMSE = 2.327\n",
    "\n",
    "**MLPR - Initial attempt** -- 2 splits on Times Series split - With larger test set in Jan-\n",
    "- R-squared = 0.851\n",
    "- RMSE = 3.662"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "6d87e8e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1000x700 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure()\n",
    "sns.set(rc={'figure.figsize':(10,7)})\n",
    "plt.plot(MLPmodel.loss_curve_)\n",
    "plt.title(\"Loss Curve\", fontsize=14)\n",
    "plt.xlabel('Iterations')\n",
    "plt.ylabel('Cost')\n",
    "plt.axvline(x=10, color = 'black', linestyle = '--')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3ba1540",
   "metadata": {},
   "source": [
    "#### Loss vs. Iteration Analysis\n",
    "- Ther is a significant reduction in loss within the first 5-10 interations of our MLPRegressor.  Then, there is a incremental improvement in loss until iteration 100 where the model loss plateaus more or less."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b77ddf46",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set up Grid Search to tune hyper parameters of MLPR\n",
    "MLPmodel = MLPRegressor(verbose = 1, random_state = 32)\n",
    "\n",
    "param_grid = {\n",
    "    'hidden_layer_sizes': [(150,100,50), (120,80,40), (100,50,30)],\n",
    "    'max_iter': [50, 100],\n",
    "    'activation': ['tanh', 'relu'],\n",
    "    'solver': ['sgd', 'adam'],\n",
    "    'alpha': [0.0001, 0.05],\n",
    "    'learning_rate': ['constant','adaptive'],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "341d2122",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 1.83822557\n",
      "Iteration 2, loss = 0.38234857\n",
      "Iteration 3, loss = 0.30971924\n",
      "Iteration 4, loss = 0.27343127\n",
      "Iteration 5, loss = 0.25038407\n",
      "Iteration 6, loss = 0.23341592\n",
      "Iteration 7, loss = 0.22035070\n",
      "Iteration 8, loss = 0.20983228\n",
      "Iteration 9, loss = 0.20058795\n",
      "Iteration 10, loss = 0.19276502\n",
      "Iteration 11, loss = 0.18529484\n",
      "Iteration 12, loss = 0.17880647\n",
      "Iteration 13, loss = 0.17340267\n",
      "Iteration 14, loss = 0.16801613\n",
      "Iteration 15, loss = 0.16331782\n",
      "Iteration 16, loss = 0.15915049\n",
      "Iteration 17, loss = 0.15488190\n",
      "Iteration 18, loss = 0.15097114\n",
      "Iteration 19, loss = 0.14787599\n",
      "Iteration 20, loss = 0.14459589\n",
      "Iteration 21, loss = 0.14132549\n",
      "Iteration 22, loss = 0.13841173\n",
      "Iteration 23, loss = 0.13531648\n",
      "Iteration 24, loss = 0.13320000\n",
      "Iteration 25, loss = 0.13094294\n",
      "Iteration 26, loss = 0.12820895\n",
      "Iteration 27, loss = 0.12551969\n",
      "Iteration 28, loss = 0.12396192\n",
      "Iteration 29, loss = 0.12183486\n",
      "Iteration 30, loss = 0.11962543\n",
      "Iteration 31, loss = 0.11754387\n",
      "Iteration 32, loss = 0.11554755\n",
      "Iteration 33, loss = 0.11424073\n",
      "Iteration 34, loss = 0.11264785\n",
      "Iteration 35, loss = 0.11058517\n",
      "Iteration 36, loss = 0.10923633\n",
      "Iteration 37, loss = 0.10755377\n",
      "Iteration 38, loss = 0.10610542\n",
      "Iteration 39, loss = 0.10476968\n",
      "Iteration 40, loss = 0.10302151\n",
      "Iteration 41, loss = 0.10202254\n",
      "Iteration 42, loss = 0.10085443\n",
      "Iteration 43, loss = 0.09968993\n",
      "Iteration 44, loss = 0.09824686\n",
      "Iteration 45, loss = 0.09739712\n",
      "Iteration 46, loss = 0.09573992\n",
      "Iteration 47, loss = 0.09492709\n",
      "Iteration 48, loss = 0.09355740\n",
      "Iteration 49, loss = 0.09225722\n",
      "Iteration 50, loss = 0.09181435\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kileymack/opt/anaconda3/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (50) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 1.65712022\n",
      "Iteration 2, loss = 0.36470407\n",
      "Iteration 3, loss = 0.30452285\n",
      "Iteration 4, loss = 0.27045975\n",
      "Iteration 5, loss = 0.24792828\n",
      "Iteration 6, loss = 0.23021757\n",
      "Iteration 7, loss = 0.21649479\n",
      "Iteration 8, loss = 0.20523018\n",
      "Iteration 9, loss = 0.19564704\n",
      "Iteration 10, loss = 0.18757568\n",
      "Iteration 11, loss = 0.18024403\n",
      "Iteration 12, loss = 0.17386408\n",
      "Iteration 13, loss = 0.16794808\n",
      "Iteration 14, loss = 0.16240636\n",
      "Iteration 15, loss = 0.15726358\n",
      "Iteration 16, loss = 0.15280309\n",
      "Iteration 17, loss = 0.14839971\n",
      "Iteration 18, loss = 0.14455465\n",
      "Iteration 19, loss = 0.14062162\n",
      "Iteration 20, loss = 0.13757709\n",
      "Iteration 21, loss = 0.13383076\n",
      "Iteration 22, loss = 0.13083802\n",
      "Iteration 23, loss = 0.12800712\n",
      "Iteration 24, loss = 0.12513527\n",
      "Iteration 25, loss = 0.12258032\n",
      "Iteration 26, loss = 0.12028404\n",
      "Iteration 27, loss = 0.11771438\n",
      "Iteration 28, loss = 0.11552761\n",
      "Iteration 29, loss = 0.11361252\n",
      "Iteration 30, loss = 0.11143448\n",
      "Iteration 31, loss = 0.10959984\n",
      "Iteration 32, loss = 0.10768833\n",
      "Iteration 33, loss = 0.10590289\n",
      "Iteration 34, loss = 0.10415888\n",
      "Iteration 35, loss = 0.10254619\n",
      "Iteration 36, loss = 0.10066156\n",
      "Iteration 37, loss = 0.09931004\n",
      "Iteration 38, loss = 0.09791917\n",
      "Iteration 39, loss = 0.09674749\n",
      "Iteration 40, loss = 0.09520389\n",
      "Iteration 41, loss = 0.09375280\n",
      "Iteration 42, loss = 0.09267459\n",
      "Iteration 43, loss = 0.09128617\n",
      "Iteration 44, loss = 0.09020865\n",
      "Iteration 45, loss = 0.08897570\n",
      "Iteration 46, loss = 0.08786659\n",
      "Iteration 47, loss = 0.08647433\n",
      "Iteration 48, loss = 0.08562313\n",
      "Iteration 49, loss = 0.08455831\n",
      "Iteration 50, loss = 0.08370582\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kileymack/opt/anaconda3/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (50) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 4.21784197\n",
      "Iteration 2, loss = 0.53670990\n",
      "Iteration 3, loss = 0.32140394\n",
      "Iteration 4, loss = 0.24923310\n",
      "Iteration 5, loss = 0.21409960\n",
      "Iteration 6, loss = 0.19272404\n",
      "Iteration 7, loss = 0.17377965\n",
      "Iteration 8, loss = 0.16108912\n",
      "Iteration 9, loss = 0.14856481\n",
      "Iteration 10, loss = 0.14043594\n",
      "Iteration 11, loss = 0.13118614\n",
      "Iteration 12, loss = 0.12484687\n",
      "Iteration 13, loss = 0.12000148\n",
      "Iteration 14, loss = 0.10961677\n",
      "Iteration 15, loss = 0.10642682\n",
      "Iteration 16, loss = 0.10027691\n",
      "Iteration 17, loss = 0.09821414\n",
      "Iteration 18, loss = 0.09437600\n",
      "Iteration 19, loss = 0.08920445\n",
      "Iteration 20, loss = 0.08814140\n",
      "Iteration 21, loss = 0.08420526\n",
      "Iteration 22, loss = 0.08112398\n",
      "Iteration 23, loss = 0.07370692\n",
      "Iteration 24, loss = 0.07508614\n",
      "Iteration 25, loss = 0.07178792\n",
      "Iteration 26, loss = 0.07041007\n",
      "Iteration 27, loss = 0.06694692\n",
      "Iteration 28, loss = 0.06332870\n",
      "Iteration 29, loss = 0.06449757\n",
      "Iteration 30, loss = 0.06254968\n",
      "Iteration 31, loss = 0.06126823\n",
      "Iteration 32, loss = 0.05866844\n",
      "Iteration 33, loss = 0.05791683\n",
      "Iteration 34, loss = 0.05525904\n",
      "Iteration 35, loss = 0.05598644\n",
      "Iteration 36, loss = 0.05436686\n",
      "Iteration 37, loss = 0.05344361\n",
      "Iteration 38, loss = 0.05175846\n",
      "Iteration 39, loss = 0.05144433\n",
      "Iteration 40, loss = 0.04878767\n",
      "Iteration 41, loss = 0.04998921\n",
      "Iteration 42, loss = 0.04934702\n",
      "Iteration 43, loss = 0.04675970\n",
      "Iteration 44, loss = 0.04623638\n",
      "Iteration 45, loss = 0.04540511\n",
      "Iteration 46, loss = 0.04636908\n",
      "Iteration 47, loss = 0.04470001\n",
      "Iteration 48, loss = 0.04279676\n",
      "Iteration 49, loss = 0.04237063\n",
      "Iteration 50, loss = 0.04190273\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kileymack/opt/anaconda3/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (50) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 6.93073650\n",
      "Iteration 2, loss = 0.45599182\n",
      "Iteration 3, loss = 0.28886073\n",
      "Iteration 4, loss = 0.22973905\n",
      "Iteration 5, loss = 0.19478623\n",
      "Iteration 6, loss = 0.17448536\n",
      "Iteration 7, loss = 0.15844713\n",
      "Iteration 8, loss = 0.14315497\n",
      "Iteration 9, loss = 0.13503007\n",
      "Iteration 10, loss = 0.12304589\n",
      "Iteration 11, loss = 0.11667796\n",
      "Iteration 12, loss = 0.10967141\n",
      "Iteration 13, loss = 0.10258564\n",
      "Iteration 14, loss = 0.09769300\n",
      "Iteration 15, loss = 0.09387318\n",
      "Iteration 16, loss = 0.08959938\n",
      "Iteration 17, loss = 0.08463124\n",
      "Iteration 18, loss = 0.08304690\n",
      "Iteration 19, loss = 0.07924504\n",
      "Iteration 20, loss = 0.07711107\n",
      "Iteration 21, loss = 0.07460753\n",
      "Iteration 22, loss = 0.07239511\n",
      "Iteration 23, loss = 0.07043923\n",
      "Iteration 24, loss = 0.06871954\n",
      "Iteration 25, loss = 0.06685495\n",
      "Iteration 26, loss = 0.06475883\n",
      "Iteration 27, loss = 0.06324273\n",
      "Iteration 28, loss = 0.06316966\n",
      "Iteration 29, loss = 0.06188568\n",
      "Iteration 30, loss = 0.05996672\n",
      "Iteration 31, loss = 0.05889611\n",
      "Iteration 32, loss = 0.05794866\n",
      "Iteration 33, loss = 0.05731312\n",
      "Iteration 34, loss = 0.05648661\n",
      "Iteration 35, loss = 0.05503090\n",
      "Iteration 36, loss = 0.05465997\n",
      "Iteration 37, loss = 0.05417951\n",
      "Iteration 38, loss = 0.05296242\n",
      "Iteration 39, loss = 0.05238155\n",
      "Iteration 40, loss = 0.05072246\n",
      "Iteration 41, loss = 0.05115623\n",
      "Iteration 42, loss = 0.05118778\n",
      "Iteration 43, loss = 0.04995347\n",
      "Iteration 44, loss = 0.04930629\n",
      "Iteration 45, loss = 0.04861428\n",
      "Iteration 46, loss = 0.04849660\n",
      "Iteration 47, loss = 0.04801227\n",
      "Iteration 48, loss = 0.04749235\n",
      "Iteration 49, loss = 0.04736662\n",
      "Iteration 50, loss = 0.04729974\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kileymack/opt/anaconda3/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (50) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 1.83822557\n",
      "Iteration 2, loss = 0.38234857\n",
      "Iteration 3, loss = 0.30971924\n",
      "Iteration 4, loss = 0.27343127\n",
      "Iteration 5, loss = 0.25038407\n",
      "Iteration 6, loss = 0.23341592\n",
      "Iteration 7, loss = 0.22035070\n",
      "Iteration 8, loss = 0.20983228\n",
      "Iteration 9, loss = 0.20058795\n",
      "Iteration 10, loss = 0.19276502\n",
      "Iteration 11, loss = 0.18529484\n",
      "Iteration 12, loss = 0.17880647\n",
      "Iteration 13, loss = 0.17340267\n",
      "Iteration 14, loss = 0.16801613\n",
      "Iteration 15, loss = 0.16331782\n",
      "Iteration 16, loss = 0.15915049\n",
      "Iteration 17, loss = 0.15488190\n",
      "Iteration 18, loss = 0.15097114\n",
      "Iteration 19, loss = 0.14787599\n",
      "Iteration 20, loss = 0.14459589\n",
      "Iteration 21, loss = 0.14132549\n",
      "Iteration 22, loss = 0.13841173\n",
      "Iteration 23, loss = 0.13531648\n",
      "Iteration 24, loss = 0.13320000\n",
      "Iteration 25, loss = 0.13094294\n",
      "Iteration 26, loss = 0.12820895\n",
      "Iteration 27, loss = 0.12551969\n",
      "Iteration 28, loss = 0.12396192\n",
      "Iteration 29, loss = 0.12183486\n",
      "Iteration 30, loss = 0.11962543\n",
      "Iteration 31, loss = 0.11754387\n",
      "Iteration 32, loss = 0.11554755\n",
      "Iteration 33, loss = 0.11424073\n",
      "Iteration 34, loss = 0.11264785\n",
      "Iteration 35, loss = 0.11058517\n",
      "Iteration 36, loss = 0.10923633\n",
      "Iteration 37, loss = 0.10755377\n",
      "Iteration 38, loss = 0.10610542\n",
      "Iteration 39, loss = 0.10476968\n",
      "Iteration 40, loss = 0.10302151\n",
      "Iteration 41, loss = 0.10202254\n",
      "Iteration 42, loss = 0.10085443\n",
      "Iteration 43, loss = 0.09968993\n",
      "Iteration 44, loss = 0.09824686\n",
      "Iteration 45, loss = 0.09739712\n",
      "Iteration 46, loss = 0.09573992\n",
      "Iteration 47, loss = 0.09492709\n",
      "Iteration 48, loss = 0.09355740\n",
      "Iteration 49, loss = 0.09225722\n",
      "Iteration 50, loss = 0.09181435\n",
      "Iteration 51, loss = 0.09047010\n",
      "Iteration 52, loss = 0.08923998\n",
      "Iteration 53, loss = 0.08821661\n",
      "Iteration 54, loss = 0.08773129\n",
      "Iteration 55, loss = 0.08641263\n",
      "Iteration 56, loss = 0.08573619\n",
      "Iteration 57, loss = 0.08470687\n",
      "Iteration 58, loss = 0.08372630\n",
      "Iteration 59, loss = 0.08312695\n",
      "Iteration 60, loss = 0.08211104\n",
      "Iteration 61, loss = 0.08155190\n",
      "Iteration 62, loss = 0.08029304\n",
      "Iteration 63, loss = 0.07982679\n",
      "Iteration 64, loss = 0.07876493\n",
      "Iteration 65, loss = 0.07846036\n",
      "Iteration 66, loss = 0.07739047\n",
      "Iteration 67, loss = 0.07678083\n",
      "Iteration 68, loss = 0.07623083\n",
      "Iteration 69, loss = 0.07503030\n",
      "Iteration 70, loss = 0.07440585\n",
      "Iteration 71, loss = 0.07386585\n",
      "Iteration 72, loss = 0.07358405\n",
      "Iteration 73, loss = 0.07248806\n",
      "Iteration 74, loss = 0.07205310\n",
      "Iteration 75, loss = 0.07154757\n",
      "Iteration 76, loss = 0.07093833\n",
      "Iteration 77, loss = 0.07030462\n",
      "Iteration 78, loss = 0.06955646\n",
      "Iteration 79, loss = 0.06912011\n",
      "Iteration 80, loss = 0.06865550\n",
      "Iteration 81, loss = 0.06778793\n",
      "Iteration 82, loss = 0.06747679\n",
      "Iteration 83, loss = 0.06697107\n",
      "Iteration 84, loss = 0.06634605\n",
      "Iteration 85, loss = 0.06566812\n",
      "Iteration 86, loss = 0.06548995\n",
      "Iteration 87, loss = 0.06506505\n",
      "Iteration 88, loss = 0.06433298\n",
      "Iteration 89, loss = 0.06381786\n",
      "Iteration 90, loss = 0.06365799\n",
      "Iteration 91, loss = 0.06296674\n",
      "Iteration 92, loss = 0.06266930\n",
      "Iteration 93, loss = 0.06173849\n",
      "Iteration 94, loss = 0.06133975\n",
      "Iteration 95, loss = 0.06109433\n",
      "Iteration 96, loss = 0.06065245\n",
      "Iteration 97, loss = 0.06038128\n",
      "Iteration 98, loss = 0.05989850\n",
      "Iteration 99, loss = 0.05964058\n",
      "Iteration 100, loss = 0.05913823\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kileymack/opt/anaconda3/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 1.65712022\n",
      "Iteration 2, loss = 0.36470407\n",
      "Iteration 3, loss = 0.30452285\n",
      "Iteration 4, loss = 0.27045975\n",
      "Iteration 5, loss = 0.24792828\n",
      "Iteration 6, loss = 0.23021757\n",
      "Iteration 7, loss = 0.21649479\n",
      "Iteration 8, loss = 0.20523018\n",
      "Iteration 9, loss = 0.19564704\n",
      "Iteration 10, loss = 0.18757568\n",
      "Iteration 11, loss = 0.18024403\n",
      "Iteration 12, loss = 0.17386408\n",
      "Iteration 13, loss = 0.16794808\n",
      "Iteration 14, loss = 0.16240636\n",
      "Iteration 15, loss = 0.15726358\n",
      "Iteration 16, loss = 0.15280309\n",
      "Iteration 17, loss = 0.14839971\n",
      "Iteration 18, loss = 0.14455465\n",
      "Iteration 19, loss = 0.14062162\n",
      "Iteration 20, loss = 0.13757709\n",
      "Iteration 21, loss = 0.13383076\n",
      "Iteration 22, loss = 0.13083802\n",
      "Iteration 23, loss = 0.12800712\n",
      "Iteration 24, loss = 0.12513527\n",
      "Iteration 25, loss = 0.12258032\n",
      "Iteration 26, loss = 0.12028404\n",
      "Iteration 27, loss = 0.11771438\n",
      "Iteration 28, loss = 0.11552761\n",
      "Iteration 29, loss = 0.11361252\n",
      "Iteration 30, loss = 0.11143448\n",
      "Iteration 31, loss = 0.10959984\n",
      "Iteration 32, loss = 0.10768833\n",
      "Iteration 33, loss = 0.10590289\n",
      "Iteration 34, loss = 0.10415888\n",
      "Iteration 35, loss = 0.10254619\n",
      "Iteration 36, loss = 0.10066156\n",
      "Iteration 37, loss = 0.09931004\n",
      "Iteration 38, loss = 0.09791917\n",
      "Iteration 39, loss = 0.09674749\n",
      "Iteration 40, loss = 0.09520389\n",
      "Iteration 41, loss = 0.09375280\n",
      "Iteration 42, loss = 0.09267459\n",
      "Iteration 43, loss = 0.09128617\n",
      "Iteration 44, loss = 0.09020865\n",
      "Iteration 45, loss = 0.08897570\n",
      "Iteration 46, loss = 0.08786659\n",
      "Iteration 47, loss = 0.08647433\n",
      "Iteration 48, loss = 0.08562313\n",
      "Iteration 49, loss = 0.08455831\n",
      "Iteration 50, loss = 0.08370582\n",
      "Iteration 51, loss = 0.08247239\n",
      "Iteration 52, loss = 0.08148918\n",
      "Iteration 53, loss = 0.08066848\n",
      "Iteration 54, loss = 0.07976093\n",
      "Iteration 55, loss = 0.07865382\n",
      "Iteration 56, loss = 0.07766458\n",
      "Iteration 57, loss = 0.07691845\n",
      "Iteration 58, loss = 0.07615315\n",
      "Iteration 59, loss = 0.07526252\n",
      "Iteration 60, loss = 0.07445427\n",
      "Iteration 61, loss = 0.07348881\n",
      "Iteration 62, loss = 0.07275106\n",
      "Iteration 63, loss = 0.07231305\n",
      "Iteration 64, loss = 0.07161840\n",
      "Iteration 65, loss = 0.07075000\n",
      "Iteration 66, loss = 0.06994220\n",
      "Iteration 67, loss = 0.06914766\n",
      "Iteration 68, loss = 0.06856677\n",
      "Iteration 69, loss = 0.06794785\n",
      "Iteration 70, loss = 0.06730142\n",
      "Iteration 71, loss = 0.06669523\n",
      "Iteration 72, loss = 0.06584151\n",
      "Iteration 73, loss = 0.06546310\n",
      "Iteration 74, loss = 0.06453528\n",
      "Iteration 75, loss = 0.06415886\n",
      "Iteration 76, loss = 0.06370039\n",
      "Iteration 77, loss = 0.06303240\n",
      "Iteration 78, loss = 0.06258360\n",
      "Iteration 79, loss = 0.06181517\n",
      "Iteration 80, loss = 0.06144831\n",
      "Iteration 81, loss = 0.06083240\n",
      "Iteration 82, loss = 0.06023351\n",
      "Iteration 83, loss = 0.05983862\n",
      "Iteration 84, loss = 0.05926417\n",
      "Iteration 85, loss = 0.05884915\n",
      "Iteration 86, loss = 0.05837806\n",
      "Iteration 87, loss = 0.05793672\n",
      "Iteration 88, loss = 0.05745207\n",
      "Iteration 89, loss = 0.05698165\n",
      "Iteration 90, loss = 0.05657595\n",
      "Iteration 91, loss = 0.05608066\n",
      "Iteration 92, loss = 0.05556825\n",
      "Iteration 93, loss = 0.05528933\n",
      "Iteration 94, loss = 0.05487669\n",
      "Iteration 95, loss = 0.05434653\n",
      "Iteration 96, loss = 0.05406523\n",
      "Iteration 97, loss = 0.05369502\n",
      "Iteration 98, loss = 0.05327614\n",
      "Iteration 99, loss = 0.05296385\n",
      "Iteration 100, loss = 0.05259946\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kileymack/opt/anaconda3/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 4.21784197\n",
      "Iteration 2, loss = 0.53670990\n",
      "Iteration 3, loss = 0.32140394\n",
      "Iteration 4, loss = 0.24923310\n",
      "Iteration 5, loss = 0.21409960\n",
      "Iteration 6, loss = 0.19272404\n",
      "Iteration 7, loss = 0.17377965\n",
      "Iteration 8, loss = 0.16108912\n",
      "Iteration 9, loss = 0.14856481\n",
      "Iteration 10, loss = 0.14043594\n",
      "Iteration 11, loss = 0.13118614\n",
      "Iteration 12, loss = 0.12484687\n",
      "Iteration 13, loss = 0.12000148\n",
      "Iteration 14, loss = 0.10961677\n",
      "Iteration 15, loss = 0.10642682\n",
      "Iteration 16, loss = 0.10027691\n",
      "Iteration 17, loss = 0.09821414\n",
      "Iteration 18, loss = 0.09437600\n",
      "Iteration 19, loss = 0.08920445\n",
      "Iteration 20, loss = 0.08814140\n",
      "Iteration 21, loss = 0.08420526\n",
      "Iteration 22, loss = 0.08112398\n",
      "Iteration 23, loss = 0.07370692\n",
      "Iteration 24, loss = 0.07508614\n",
      "Iteration 25, loss = 0.07178792\n",
      "Iteration 26, loss = 0.07041007\n",
      "Iteration 27, loss = 0.06694692\n",
      "Iteration 28, loss = 0.06332870\n",
      "Iteration 29, loss = 0.06449757\n",
      "Iteration 30, loss = 0.06254968\n",
      "Iteration 31, loss = 0.06126823\n",
      "Iteration 32, loss = 0.05866844\n",
      "Iteration 33, loss = 0.05791683\n",
      "Iteration 34, loss = 0.05525904\n",
      "Iteration 35, loss = 0.05598644\n",
      "Iteration 36, loss = 0.05436686\n",
      "Iteration 37, loss = 0.05344361\n",
      "Iteration 38, loss = 0.05175846\n",
      "Iteration 39, loss = 0.05144433\n",
      "Iteration 40, loss = 0.04878767\n",
      "Iteration 41, loss = 0.04998921\n",
      "Iteration 42, loss = 0.04934702\n",
      "Iteration 43, loss = 0.04675970\n",
      "Iteration 44, loss = 0.04623638\n",
      "Iteration 45, loss = 0.04540511\n",
      "Iteration 46, loss = 0.04636908\n",
      "Iteration 47, loss = 0.04470001\n",
      "Iteration 48, loss = 0.04279676\n",
      "Iteration 49, loss = 0.04237063\n",
      "Iteration 50, loss = 0.04190273\n",
      "Iteration 51, loss = 0.04154682\n",
      "Iteration 52, loss = 0.04241054\n",
      "Iteration 53, loss = 0.04089767\n",
      "Iteration 54, loss = 0.04024670\n",
      "Iteration 55, loss = 0.04103595\n",
      "Iteration 56, loss = 0.03911093\n",
      "Iteration 57, loss = 0.03891938\n",
      "Iteration 58, loss = 0.03933842\n",
      "Iteration 59, loss = 0.03891796\n",
      "Iteration 60, loss = 0.03776854\n",
      "Iteration 61, loss = 0.03723065\n",
      "Iteration 62, loss = 0.03616500\n",
      "Iteration 63, loss = 0.03781800\n",
      "Iteration 64, loss = 0.03584322\n",
      "Iteration 65, loss = 0.03603546\n",
      "Iteration 66, loss = 0.03614071\n",
      "Iteration 67, loss = 0.03546366\n",
      "Iteration 68, loss = 0.03471959\n",
      "Iteration 69, loss = 0.03434591\n",
      "Iteration 70, loss = 0.03360070\n",
      "Iteration 71, loss = 0.03369394\n",
      "Iteration 72, loss = 0.03463234\n",
      "Iteration 73, loss = 0.03376344\n",
      "Iteration 74, loss = 0.03313436\n",
      "Iteration 75, loss = 0.03303825\n",
      "Iteration 76, loss = 0.03327269\n",
      "Iteration 77, loss = 0.03203343\n",
      "Iteration 78, loss = 0.03261486\n",
      "Iteration 79, loss = 0.03191883\n",
      "Iteration 80, loss = 0.03098096\n",
      "Iteration 81, loss = 0.03283203\n",
      "Iteration 82, loss = 0.03077798\n",
      "Iteration 83, loss = 0.03178884\n",
      "Iteration 84, loss = 0.03075289\n",
      "Iteration 85, loss = 0.03046843\n",
      "Iteration 86, loss = 0.03096769\n",
      "Iteration 87, loss = 0.03013507\n",
      "Iteration 88, loss = 0.03014758\n",
      "Iteration 89, loss = 0.03054142\n",
      "Iteration 90, loss = 0.03063861\n",
      "Iteration 91, loss = 0.02960791\n",
      "Iteration 92, loss = 0.02968618\n",
      "Iteration 93, loss = 0.02849979\n",
      "Iteration 94, loss = 0.02877563\n",
      "Iteration 95, loss = 0.02934433\n",
      "Iteration 96, loss = 0.02905355\n",
      "Iteration 97, loss = 0.02828600\n",
      "Iteration 98, loss = 0.02872959\n",
      "Iteration 99, loss = 0.02830582\n",
      "Iteration 100, loss = 0.02839229\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kileymack/opt/anaconda3/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 6.93073650\n",
      "Iteration 2, loss = 0.45599182\n",
      "Iteration 3, loss = 0.28886073\n",
      "Iteration 4, loss = 0.22973905\n",
      "Iteration 5, loss = 0.19478623\n",
      "Iteration 6, loss = 0.17448536\n",
      "Iteration 7, loss = 0.15844713\n",
      "Iteration 8, loss = 0.14315497\n",
      "Iteration 9, loss = 0.13503007\n",
      "Iteration 10, loss = 0.12304589\n",
      "Iteration 11, loss = 0.11667796\n",
      "Iteration 12, loss = 0.10967141\n",
      "Iteration 13, loss = 0.10258564\n",
      "Iteration 14, loss = 0.09769300\n",
      "Iteration 15, loss = 0.09387318\n",
      "Iteration 16, loss = 0.08959938\n",
      "Iteration 17, loss = 0.08463124\n",
      "Iteration 18, loss = 0.08304690\n",
      "Iteration 19, loss = 0.07924504\n",
      "Iteration 20, loss = 0.07711107\n",
      "Iteration 21, loss = 0.07460753\n",
      "Iteration 22, loss = 0.07239511\n",
      "Iteration 23, loss = 0.07043923\n",
      "Iteration 24, loss = 0.06871954\n",
      "Iteration 25, loss = 0.06685495\n",
      "Iteration 26, loss = 0.06475883\n",
      "Iteration 27, loss = 0.06324273\n",
      "Iteration 28, loss = 0.06316966\n",
      "Iteration 29, loss = 0.06188568\n",
      "Iteration 30, loss = 0.05996672\n",
      "Iteration 31, loss = 0.05889611\n",
      "Iteration 32, loss = 0.05794866\n",
      "Iteration 33, loss = 0.05731312\n",
      "Iteration 34, loss = 0.05648661\n",
      "Iteration 35, loss = 0.05503090\n",
      "Iteration 36, loss = 0.05465997\n",
      "Iteration 37, loss = 0.05417951\n",
      "Iteration 38, loss = 0.05296242\n",
      "Iteration 39, loss = 0.05238155\n",
      "Iteration 40, loss = 0.05072246\n",
      "Iteration 41, loss = 0.05115623\n",
      "Iteration 42, loss = 0.05118778\n",
      "Iteration 43, loss = 0.04995347\n",
      "Iteration 44, loss = 0.04930629\n",
      "Iteration 45, loss = 0.04861428\n",
      "Iteration 46, loss = 0.04849660\n",
      "Iteration 47, loss = 0.04801227\n",
      "Iteration 48, loss = 0.04749235\n",
      "Iteration 49, loss = 0.04736662\n",
      "Iteration 50, loss = 0.04729974\n",
      "Iteration 51, loss = 0.04583556\n",
      "Iteration 52, loss = 0.04575328\n",
      "Iteration 53, loss = 0.04621617\n",
      "Iteration 54, loss = 0.04500881\n",
      "Iteration 55, loss = 0.04538198\n",
      "Iteration 56, loss = 0.04415334\n",
      "Iteration 57, loss = 0.04397929\n",
      "Iteration 58, loss = 0.04390146\n",
      "Iteration 59, loss = 0.04397152\n",
      "Iteration 60, loss = 0.04336869\n",
      "Iteration 61, loss = 0.04253146\n",
      "Iteration 62, loss = 0.04284411\n",
      "Iteration 63, loss = 0.04273249\n",
      "Iteration 64, loss = 0.04244097\n",
      "Iteration 65, loss = 0.04225319\n",
      "Iteration 66, loss = 0.04176986\n",
      "Iteration 67, loss = 0.04202339\n",
      "Iteration 68, loss = 0.04139967\n",
      "Iteration 69, loss = 0.04140290\n",
      "Iteration 70, loss = 0.04092559\n",
      "Iteration 71, loss = 0.04073589\n",
      "Iteration 72, loss = 0.04032713\n",
      "Iteration 73, loss = 0.04070249\n",
      "Iteration 74, loss = 0.04096447\n",
      "Iteration 75, loss = 0.03988473\n",
      "Iteration 76, loss = 0.03997398\n",
      "Iteration 77, loss = 0.03949007\n",
      "Iteration 78, loss = 0.04012688\n",
      "Iteration 79, loss = 0.03928734\n",
      "Iteration 80, loss = 0.03899547\n",
      "Iteration 81, loss = 0.03942679\n",
      "Iteration 82, loss = 0.03887854\n",
      "Iteration 83, loss = 0.03916713\n",
      "Iteration 84, loss = 0.03851526\n",
      "Iteration 85, loss = 0.03802201\n",
      "Iteration 86, loss = 0.03784236\n",
      "Iteration 87, loss = 0.03862088\n",
      "Iteration 88, loss = 0.03803656\n",
      "Iteration 89, loss = 0.03752089\n",
      "Iteration 90, loss = 0.03746179\n",
      "Iteration 91, loss = 0.03771276\n",
      "Iteration 92, loss = 0.03783044\n",
      "Iteration 93, loss = 0.03718743\n",
      "Iteration 94, loss = 0.03714439\n",
      "Iteration 95, loss = 0.03726564\n",
      "Iteration 96, loss = 0.03667234\n",
      "Iteration 97, loss = 0.03772134\n",
      "Iteration 98, loss = 0.03652367\n",
      "Iteration 99, loss = 0.03678433\n",
      "Iteration 100, loss = 0.03662392\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kileymack/opt/anaconda3/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 1.83822557\n",
      "Iteration 2, loss = 0.38234857\n",
      "Iteration 3, loss = 0.30971924\n",
      "Iteration 4, loss = 0.27343127\n",
      "Iteration 5, loss = 0.25038407\n",
      "Iteration 6, loss = 0.23341592\n",
      "Iteration 7, loss = 0.22035070\n",
      "Iteration 8, loss = 0.20983228\n",
      "Iteration 9, loss = 0.20058795\n",
      "Iteration 10, loss = 0.19276502\n",
      "Iteration 11, loss = 0.18529484\n",
      "Iteration 12, loss = 0.17880647\n",
      "Iteration 13, loss = 0.17340267\n",
      "Iteration 14, loss = 0.16801613\n",
      "Iteration 15, loss = 0.16331782\n",
      "Iteration 16, loss = 0.15915049\n",
      "Iteration 17, loss = 0.15488190\n",
      "Iteration 18, loss = 0.15097114\n",
      "Iteration 19, loss = 0.14787599\n",
      "Iteration 20, loss = 0.14459589\n",
      "Iteration 21, loss = 0.14132549\n",
      "Iteration 22, loss = 0.13841173\n",
      "Iteration 23, loss = 0.13531648\n",
      "Iteration 24, loss = 0.13320000\n",
      "Iteration 25, loss = 0.13094294\n",
      "Iteration 26, loss = 0.12820895\n",
      "Iteration 27, loss = 0.12551969\n",
      "Iteration 28, loss = 0.12396192\n",
      "Iteration 29, loss = 0.12183486\n",
      "Iteration 30, loss = 0.11962543\n",
      "Iteration 31, loss = 0.11754387\n",
      "Iteration 32, loss = 0.11554755\n",
      "Iteration 33, loss = 0.11424073\n",
      "Iteration 34, loss = 0.11264785\n",
      "Iteration 35, loss = 0.11058517\n",
      "Iteration 36, loss = 0.10923633\n",
      "Iteration 37, loss = 0.10755377\n",
      "Iteration 38, loss = 0.10610542\n",
      "Iteration 39, loss = 0.10476968\n",
      "Iteration 40, loss = 0.10302151\n",
      "Iteration 41, loss = 0.10202254\n",
      "Iteration 42, loss = 0.10085443\n",
      "Iteration 43, loss = 0.09968993\n",
      "Iteration 44, loss = 0.09824686\n",
      "Iteration 45, loss = 0.09739712\n",
      "Iteration 46, loss = 0.09573992\n",
      "Iteration 47, loss = 0.09492709\n",
      "Iteration 48, loss = 0.09355740\n",
      "Iteration 49, loss = 0.09225722\n",
      "Iteration 50, loss = 0.09181435\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kileymack/opt/anaconda3/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (50) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 1.65712022\n",
      "Iteration 2, loss = 0.36470407\n",
      "Iteration 3, loss = 0.30452285\n",
      "Iteration 4, loss = 0.27045975\n",
      "Iteration 5, loss = 0.24792828\n",
      "Iteration 6, loss = 0.23021757\n",
      "Iteration 7, loss = 0.21649479\n",
      "Iteration 8, loss = 0.20523018\n",
      "Iteration 9, loss = 0.19564704\n",
      "Iteration 10, loss = 0.18757568\n",
      "Iteration 11, loss = 0.18024403\n",
      "Iteration 12, loss = 0.17386408\n",
      "Iteration 13, loss = 0.16794808\n",
      "Iteration 14, loss = 0.16240636\n",
      "Iteration 15, loss = 0.15726358\n",
      "Iteration 16, loss = 0.15280309\n",
      "Iteration 17, loss = 0.14839971\n",
      "Iteration 18, loss = 0.14455465\n",
      "Iteration 19, loss = 0.14062162\n",
      "Iteration 20, loss = 0.13757709\n",
      "Iteration 21, loss = 0.13383076\n",
      "Iteration 22, loss = 0.13083802\n",
      "Iteration 23, loss = 0.12800712\n",
      "Iteration 24, loss = 0.12513527\n",
      "Iteration 25, loss = 0.12258032\n",
      "Iteration 26, loss = 0.12028404\n",
      "Iteration 27, loss = 0.11771438\n",
      "Iteration 28, loss = 0.11552761\n",
      "Iteration 29, loss = 0.11361252\n",
      "Iteration 30, loss = 0.11143448\n",
      "Iteration 31, loss = 0.10959984\n",
      "Iteration 32, loss = 0.10768833\n",
      "Iteration 33, loss = 0.10590289\n",
      "Iteration 34, loss = 0.10415888\n",
      "Iteration 35, loss = 0.10254619\n",
      "Iteration 36, loss = 0.10066156\n",
      "Iteration 37, loss = 0.09931004\n",
      "Iteration 38, loss = 0.09791917\n",
      "Iteration 39, loss = 0.09674749\n",
      "Iteration 40, loss = 0.09520389\n",
      "Iteration 41, loss = 0.09375280\n",
      "Iteration 42, loss = 0.09267459\n",
      "Iteration 43, loss = 0.09128617\n",
      "Iteration 44, loss = 0.09020865\n",
      "Iteration 45, loss = 0.08897570\n",
      "Iteration 46, loss = 0.08786659\n",
      "Iteration 47, loss = 0.08647433\n",
      "Iteration 48, loss = 0.08562313\n",
      "Iteration 49, loss = 0.08455831\n",
      "Iteration 50, loss = 0.08370582\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kileymack/opt/anaconda3/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (50) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 4.21784197\n",
      "Iteration 2, loss = 0.53670990\n",
      "Iteration 3, loss = 0.32140394\n",
      "Iteration 4, loss = 0.24923310\n",
      "Iteration 5, loss = 0.21409960\n",
      "Iteration 6, loss = 0.19272404\n",
      "Iteration 7, loss = 0.17377965\n",
      "Iteration 8, loss = 0.16108912\n",
      "Iteration 9, loss = 0.14856481\n",
      "Iteration 10, loss = 0.14043594\n",
      "Iteration 11, loss = 0.13118614\n",
      "Iteration 12, loss = 0.12484687\n",
      "Iteration 13, loss = 0.12000148\n",
      "Iteration 14, loss = 0.10961677\n",
      "Iteration 15, loss = 0.10642682\n",
      "Iteration 16, loss = 0.10027691\n",
      "Iteration 17, loss = 0.09821414\n",
      "Iteration 18, loss = 0.09437600\n",
      "Iteration 19, loss = 0.08920445\n",
      "Iteration 20, loss = 0.08814140\n",
      "Iteration 21, loss = 0.08420526\n",
      "Iteration 22, loss = 0.08112398\n",
      "Iteration 23, loss = 0.07370692\n",
      "Iteration 24, loss = 0.07508614\n",
      "Iteration 25, loss = 0.07178792\n",
      "Iteration 26, loss = 0.07041007\n",
      "Iteration 27, loss = 0.06694692\n",
      "Iteration 28, loss = 0.06332870\n",
      "Iteration 29, loss = 0.06449757\n",
      "Iteration 30, loss = 0.06254968\n",
      "Iteration 31, loss = 0.06126823\n",
      "Iteration 32, loss = 0.05866844\n",
      "Iteration 33, loss = 0.05791683\n",
      "Iteration 34, loss = 0.05525904\n",
      "Iteration 35, loss = 0.05598644\n",
      "Iteration 36, loss = 0.05436686\n",
      "Iteration 37, loss = 0.05344361\n",
      "Iteration 38, loss = 0.05175846\n",
      "Iteration 39, loss = 0.05144433\n",
      "Iteration 40, loss = 0.04878767\n",
      "Iteration 41, loss = 0.04998921\n",
      "Iteration 42, loss = 0.04934702\n",
      "Iteration 43, loss = 0.04675970\n",
      "Iteration 44, loss = 0.04623638\n",
      "Iteration 45, loss = 0.04540511\n",
      "Iteration 46, loss = 0.04636908\n",
      "Iteration 47, loss = 0.04470001\n",
      "Iteration 48, loss = 0.04279676\n",
      "Iteration 49, loss = 0.04237063\n",
      "Iteration 50, loss = 0.04190273\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kileymack/opt/anaconda3/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (50) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 6.93073650\n",
      "Iteration 2, loss = 0.45599182\n",
      "Iteration 3, loss = 0.28886073\n",
      "Iteration 4, loss = 0.22973905\n",
      "Iteration 5, loss = 0.19478623\n",
      "Iteration 6, loss = 0.17448536\n",
      "Iteration 7, loss = 0.15844713\n",
      "Iteration 8, loss = 0.14315497\n",
      "Iteration 9, loss = 0.13503007\n",
      "Iteration 10, loss = 0.12304589\n",
      "Iteration 11, loss = 0.11667796\n",
      "Iteration 12, loss = 0.10967141\n",
      "Iteration 13, loss = 0.10258564\n",
      "Iteration 14, loss = 0.09769300\n",
      "Iteration 15, loss = 0.09387318\n",
      "Iteration 16, loss = 0.08959938\n",
      "Iteration 17, loss = 0.08463124\n",
      "Iteration 18, loss = 0.08304690\n",
      "Iteration 19, loss = 0.07924504\n",
      "Iteration 20, loss = 0.07711107\n",
      "Iteration 21, loss = 0.07460753\n",
      "Iteration 22, loss = 0.07239511\n",
      "Iteration 23, loss = 0.07043923\n",
      "Iteration 24, loss = 0.06871954\n",
      "Iteration 25, loss = 0.06685495\n",
      "Iteration 26, loss = 0.06475883\n",
      "Iteration 27, loss = 0.06324273\n",
      "Iteration 28, loss = 0.06316966\n",
      "Iteration 29, loss = 0.06188568\n",
      "Iteration 30, loss = 0.05996672\n",
      "Iteration 31, loss = 0.05889611\n",
      "Iteration 32, loss = 0.05794866\n",
      "Iteration 33, loss = 0.05731312\n",
      "Iteration 34, loss = 0.05648661\n",
      "Iteration 35, loss = 0.05503090\n",
      "Iteration 36, loss = 0.05465997\n",
      "Iteration 37, loss = 0.05417951\n",
      "Iteration 38, loss = 0.05296242\n",
      "Iteration 39, loss = 0.05238155\n",
      "Iteration 40, loss = 0.05072246\n",
      "Iteration 41, loss = 0.05115623\n",
      "Iteration 42, loss = 0.05118778\n",
      "Iteration 43, loss = 0.04995347\n",
      "Iteration 44, loss = 0.04930629\n",
      "Iteration 45, loss = 0.04861428\n",
      "Iteration 46, loss = 0.04849660\n",
      "Iteration 47, loss = 0.04801227\n",
      "Iteration 48, loss = 0.04749235\n",
      "Iteration 49, loss = 0.04736662\n",
      "Iteration 50, loss = 0.04729974\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kileymack/opt/anaconda3/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (50) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 1.83822557\n",
      "Iteration 2, loss = 0.38234857\n",
      "Iteration 3, loss = 0.30971924\n",
      "Iteration 4, loss = 0.27343127\n",
      "Iteration 5, loss = 0.25038407\n",
      "Iteration 6, loss = 0.23341592\n",
      "Iteration 7, loss = 0.22035070\n",
      "Iteration 8, loss = 0.20983228\n",
      "Iteration 9, loss = 0.20058795\n",
      "Iteration 10, loss = 0.19276502\n",
      "Iteration 11, loss = 0.18529484\n",
      "Iteration 12, loss = 0.17880647\n",
      "Iteration 13, loss = 0.17340267\n",
      "Iteration 14, loss = 0.16801613\n",
      "Iteration 15, loss = 0.16331782\n",
      "Iteration 16, loss = 0.15915049\n",
      "Iteration 17, loss = 0.15488190\n",
      "Iteration 18, loss = 0.15097114\n",
      "Iteration 19, loss = 0.14787599\n",
      "Iteration 20, loss = 0.14459589\n",
      "Iteration 21, loss = 0.14132549\n",
      "Iteration 22, loss = 0.13841173\n",
      "Iteration 23, loss = 0.13531648\n",
      "Iteration 24, loss = 0.13320000\n",
      "Iteration 25, loss = 0.13094294\n",
      "Iteration 26, loss = 0.12820895\n",
      "Iteration 27, loss = 0.12551969\n",
      "Iteration 28, loss = 0.12396192\n",
      "Iteration 29, loss = 0.12183486\n",
      "Iteration 30, loss = 0.11962543\n",
      "Iteration 31, loss = 0.11754387\n",
      "Iteration 32, loss = 0.11554755\n",
      "Iteration 33, loss = 0.11424073\n",
      "Iteration 34, loss = 0.11264785\n",
      "Iteration 35, loss = 0.11058517\n",
      "Iteration 36, loss = 0.10923633\n",
      "Iteration 37, loss = 0.10755377\n",
      "Iteration 38, loss = 0.10610542\n",
      "Iteration 39, loss = 0.10476968\n",
      "Iteration 40, loss = 0.10302151\n",
      "Iteration 41, loss = 0.10202254\n",
      "Iteration 42, loss = 0.10085443\n",
      "Iteration 43, loss = 0.09968993\n",
      "Iteration 44, loss = 0.09824686\n",
      "Iteration 45, loss = 0.09739712\n",
      "Iteration 46, loss = 0.09573992\n",
      "Iteration 47, loss = 0.09492709\n",
      "Iteration 48, loss = 0.09355740\n",
      "Iteration 49, loss = 0.09225722\n",
      "Iteration 50, loss = 0.09181435\n",
      "Iteration 51, loss = 0.09047010\n",
      "Iteration 52, loss = 0.08923998\n",
      "Iteration 53, loss = 0.08821661\n",
      "Iteration 54, loss = 0.08773129\n",
      "Iteration 55, loss = 0.08641263\n",
      "Iteration 56, loss = 0.08573619\n",
      "Iteration 57, loss = 0.08470687\n",
      "Iteration 58, loss = 0.08372630\n",
      "Iteration 59, loss = 0.08312695\n",
      "Iteration 60, loss = 0.08211104\n",
      "Iteration 61, loss = 0.08155190\n",
      "Iteration 62, loss = 0.08029304\n",
      "Iteration 63, loss = 0.07982679\n",
      "Iteration 64, loss = 0.07876493\n",
      "Iteration 65, loss = 0.07846036\n",
      "Iteration 66, loss = 0.07739047\n",
      "Iteration 67, loss = 0.07678083\n",
      "Iteration 68, loss = 0.07623083\n",
      "Iteration 69, loss = 0.07503030\n",
      "Iteration 70, loss = 0.07440585\n",
      "Iteration 71, loss = 0.07386585\n",
      "Iteration 72, loss = 0.07358405\n",
      "Iteration 73, loss = 0.07248806\n",
      "Iteration 74, loss = 0.07205310\n",
      "Iteration 75, loss = 0.07154757\n",
      "Iteration 76, loss = 0.07093833\n",
      "Iteration 77, loss = 0.07030462\n",
      "Iteration 78, loss = 0.06955646\n",
      "Iteration 79, loss = 0.06912011\n",
      "Iteration 80, loss = 0.06865550\n",
      "Iteration 81, loss = 0.06778793\n",
      "Iteration 82, loss = 0.06747679\n",
      "Iteration 83, loss = 0.06697107\n",
      "Iteration 84, loss = 0.06634605\n",
      "Iteration 85, loss = 0.06566812\n",
      "Iteration 86, loss = 0.06548995\n",
      "Iteration 87, loss = 0.06506505\n",
      "Iteration 88, loss = 0.06433298\n",
      "Iteration 89, loss = 0.06381786\n",
      "Iteration 90, loss = 0.06365799\n",
      "Iteration 91, loss = 0.06296674\n",
      "Iteration 92, loss = 0.06266930\n",
      "Iteration 93, loss = 0.06173849\n",
      "Iteration 94, loss = 0.06133975\n",
      "Iteration 95, loss = 0.06109433\n",
      "Iteration 96, loss = 0.06065245\n",
      "Iteration 97, loss = 0.06038128\n",
      "Iteration 98, loss = 0.05989850\n",
      "Iteration 99, loss = 0.05964058\n",
      "Iteration 100, loss = 0.05913823\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kileymack/opt/anaconda3/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 1.65712022\n",
      "Iteration 2, loss = 0.36470407\n",
      "Iteration 3, loss = 0.30452285\n",
      "Iteration 4, loss = 0.27045975\n",
      "Iteration 5, loss = 0.24792828\n",
      "Iteration 6, loss = 0.23021757\n",
      "Iteration 7, loss = 0.21649479\n",
      "Iteration 8, loss = 0.20523018\n",
      "Iteration 9, loss = 0.19564704\n",
      "Iteration 10, loss = 0.18757568\n",
      "Iteration 11, loss = 0.18024403\n",
      "Iteration 12, loss = 0.17386408\n",
      "Iteration 13, loss = 0.16794808\n",
      "Iteration 14, loss = 0.16240636\n",
      "Iteration 15, loss = 0.15726358\n",
      "Iteration 16, loss = 0.15280309\n",
      "Iteration 17, loss = 0.14839971\n",
      "Iteration 18, loss = 0.14455465\n",
      "Iteration 19, loss = 0.14062162\n",
      "Iteration 20, loss = 0.13757709\n",
      "Iteration 21, loss = 0.13383076\n",
      "Iteration 22, loss = 0.13083802\n",
      "Iteration 23, loss = 0.12800712\n",
      "Iteration 24, loss = 0.12513527\n",
      "Iteration 25, loss = 0.12258032\n",
      "Iteration 26, loss = 0.12028404\n",
      "Iteration 27, loss = 0.11771438\n",
      "Iteration 28, loss = 0.11552761\n",
      "Iteration 29, loss = 0.11361252\n",
      "Iteration 30, loss = 0.11143448\n",
      "Iteration 31, loss = 0.10959984\n",
      "Iteration 32, loss = 0.10768833\n",
      "Iteration 33, loss = 0.10590289\n",
      "Iteration 34, loss = 0.10415888\n",
      "Iteration 35, loss = 0.10254619\n",
      "Iteration 36, loss = 0.10066156\n",
      "Iteration 37, loss = 0.09931004\n",
      "Iteration 38, loss = 0.09791917\n",
      "Iteration 39, loss = 0.09674749\n",
      "Iteration 40, loss = 0.09520389\n",
      "Iteration 41, loss = 0.09375280\n",
      "Iteration 42, loss = 0.09267459\n",
      "Iteration 43, loss = 0.09128617\n",
      "Iteration 44, loss = 0.09020865\n",
      "Iteration 45, loss = 0.08897570\n",
      "Iteration 46, loss = 0.08786659\n",
      "Iteration 47, loss = 0.08647433\n",
      "Iteration 48, loss = 0.08562313\n",
      "Iteration 49, loss = 0.08455831\n",
      "Iteration 50, loss = 0.08370582\n",
      "Iteration 51, loss = 0.08247239\n",
      "Iteration 52, loss = 0.08148918\n",
      "Iteration 53, loss = 0.08066848\n",
      "Iteration 54, loss = 0.07976093\n",
      "Iteration 55, loss = 0.07865382\n",
      "Iteration 56, loss = 0.07766458\n",
      "Iteration 57, loss = 0.07691845\n",
      "Iteration 58, loss = 0.07615315\n",
      "Iteration 59, loss = 0.07526252\n",
      "Iteration 60, loss = 0.07445427\n",
      "Iteration 61, loss = 0.07348881\n",
      "Iteration 62, loss = 0.07275106\n",
      "Iteration 63, loss = 0.07231305\n",
      "Iteration 64, loss = 0.07161840\n",
      "Iteration 65, loss = 0.07075000\n",
      "Iteration 66, loss = 0.06994220\n",
      "Iteration 67, loss = 0.06914766\n",
      "Iteration 68, loss = 0.06856677\n",
      "Iteration 69, loss = 0.06794785\n",
      "Iteration 70, loss = 0.06730142\n",
      "Iteration 71, loss = 0.06669523\n",
      "Iteration 72, loss = 0.06584151\n",
      "Iteration 73, loss = 0.06546310\n",
      "Iteration 74, loss = 0.06453528\n",
      "Iteration 75, loss = 0.06415886\n",
      "Iteration 76, loss = 0.06370039\n",
      "Iteration 77, loss = 0.06303240\n",
      "Iteration 78, loss = 0.06258360\n",
      "Iteration 79, loss = 0.06181517\n",
      "Iteration 80, loss = 0.06144831\n",
      "Iteration 81, loss = 0.06083240\n",
      "Iteration 82, loss = 0.06023351\n",
      "Iteration 83, loss = 0.05983862\n",
      "Iteration 84, loss = 0.05926417\n",
      "Iteration 85, loss = 0.05884915\n",
      "Iteration 86, loss = 0.05837806\n",
      "Iteration 87, loss = 0.05793672\n",
      "Iteration 88, loss = 0.05745207\n",
      "Iteration 89, loss = 0.05698165\n",
      "Iteration 90, loss = 0.05657595\n",
      "Iteration 91, loss = 0.05608066\n",
      "Iteration 92, loss = 0.05556825\n",
      "Iteration 93, loss = 0.05528933\n",
      "Iteration 94, loss = 0.05487669\n",
      "Iteration 95, loss = 0.05434653\n",
      "Iteration 96, loss = 0.05406523\n",
      "Iteration 97, loss = 0.05369502\n",
      "Iteration 98, loss = 0.05327614\n",
      "Iteration 99, loss = 0.05296385\n",
      "Iteration 100, loss = 0.05259946\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kileymack/opt/anaconda3/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 4.21784197\n",
      "Iteration 2, loss = 0.53670990\n",
      "Iteration 3, loss = 0.32140394\n",
      "Iteration 4, loss = 0.24923310\n",
      "Iteration 5, loss = 0.21409960\n",
      "Iteration 6, loss = 0.19272404\n",
      "Iteration 7, loss = 0.17377965\n",
      "Iteration 8, loss = 0.16108912\n",
      "Iteration 9, loss = 0.14856481\n",
      "Iteration 10, loss = 0.14043594\n",
      "Iteration 11, loss = 0.13118614\n",
      "Iteration 12, loss = 0.12484687\n",
      "Iteration 13, loss = 0.12000148\n",
      "Iteration 14, loss = 0.10961677\n",
      "Iteration 15, loss = 0.10642682\n",
      "Iteration 16, loss = 0.10027691\n",
      "Iteration 17, loss = 0.09821414\n",
      "Iteration 18, loss = 0.09437600\n",
      "Iteration 19, loss = 0.08920445\n",
      "Iteration 20, loss = 0.08814140\n",
      "Iteration 21, loss = 0.08420526\n",
      "Iteration 22, loss = 0.08112398\n",
      "Iteration 23, loss = 0.07370692\n",
      "Iteration 24, loss = 0.07508614\n",
      "Iteration 25, loss = 0.07178792\n",
      "Iteration 26, loss = 0.07041007\n",
      "Iteration 27, loss = 0.06694692\n",
      "Iteration 28, loss = 0.06332870\n",
      "Iteration 29, loss = 0.06449757\n",
      "Iteration 30, loss = 0.06254968\n",
      "Iteration 31, loss = 0.06126823\n",
      "Iteration 32, loss = 0.05866844\n",
      "Iteration 33, loss = 0.05791683\n",
      "Iteration 34, loss = 0.05525904\n",
      "Iteration 35, loss = 0.05598644\n",
      "Iteration 36, loss = 0.05436686\n",
      "Iteration 37, loss = 0.05344361\n",
      "Iteration 38, loss = 0.05175846\n",
      "Iteration 39, loss = 0.05144433\n",
      "Iteration 40, loss = 0.04878767\n",
      "Iteration 41, loss = 0.04998921\n",
      "Iteration 42, loss = 0.04934702\n",
      "Iteration 43, loss = 0.04675970\n",
      "Iteration 44, loss = 0.04623638\n",
      "Iteration 45, loss = 0.04540511\n",
      "Iteration 46, loss = 0.04636908\n",
      "Iteration 47, loss = 0.04470001\n",
      "Iteration 48, loss = 0.04279676\n",
      "Iteration 49, loss = 0.04237063\n",
      "Iteration 50, loss = 0.04190273\n",
      "Iteration 51, loss = 0.04154682\n",
      "Iteration 52, loss = 0.04241054\n",
      "Iteration 53, loss = 0.04089767\n",
      "Iteration 54, loss = 0.04024670\n",
      "Iteration 55, loss = 0.04103595\n",
      "Iteration 56, loss = 0.03911093\n",
      "Iteration 57, loss = 0.03891938\n",
      "Iteration 58, loss = 0.03933842\n",
      "Iteration 59, loss = 0.03891796\n",
      "Iteration 60, loss = 0.03776854\n",
      "Iteration 61, loss = 0.03723065\n",
      "Iteration 62, loss = 0.03616500\n",
      "Iteration 63, loss = 0.03781800\n",
      "Iteration 64, loss = 0.03584322\n",
      "Iteration 65, loss = 0.03603546\n",
      "Iteration 66, loss = 0.03614071\n",
      "Iteration 67, loss = 0.03546366\n",
      "Iteration 68, loss = 0.03471959\n",
      "Iteration 69, loss = 0.03434591\n",
      "Iteration 70, loss = 0.03360070\n",
      "Iteration 71, loss = 0.03369394\n",
      "Iteration 72, loss = 0.03463234\n",
      "Iteration 73, loss = 0.03376344\n",
      "Iteration 74, loss = 0.03313436\n",
      "Iteration 75, loss = 0.03303825\n",
      "Iteration 76, loss = 0.03327269\n",
      "Iteration 77, loss = 0.03203343\n",
      "Iteration 78, loss = 0.03261486\n",
      "Iteration 79, loss = 0.03191883\n",
      "Iteration 80, loss = 0.03098096\n",
      "Iteration 81, loss = 0.03283203\n",
      "Iteration 82, loss = 0.03077798\n",
      "Iteration 83, loss = 0.03178884\n",
      "Iteration 84, loss = 0.03075289\n",
      "Iteration 85, loss = 0.03046843\n",
      "Iteration 86, loss = 0.03096769\n",
      "Iteration 87, loss = 0.03013507\n",
      "Iteration 88, loss = 0.03014758\n",
      "Iteration 89, loss = 0.03054142\n",
      "Iteration 90, loss = 0.03063861\n",
      "Iteration 91, loss = 0.02960791\n",
      "Iteration 92, loss = 0.02968618\n",
      "Iteration 93, loss = 0.02849979\n",
      "Iteration 94, loss = 0.02877563\n",
      "Iteration 95, loss = 0.02934433\n",
      "Iteration 96, loss = 0.02905355\n",
      "Iteration 97, loss = 0.02828600\n",
      "Iteration 98, loss = 0.02872959\n",
      "Iteration 99, loss = 0.02830582\n",
      "Iteration 100, loss = 0.02839229\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kileymack/opt/anaconda3/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 6.93073650\n",
      "Iteration 2, loss = 0.45599182\n",
      "Iteration 3, loss = 0.28886073\n",
      "Iteration 4, loss = 0.22973905\n",
      "Iteration 5, loss = 0.19478623\n",
      "Iteration 6, loss = 0.17448536\n",
      "Iteration 7, loss = 0.15844713\n",
      "Iteration 8, loss = 0.14315497\n",
      "Iteration 9, loss = 0.13503007\n",
      "Iteration 10, loss = 0.12304589\n",
      "Iteration 11, loss = 0.11667796\n",
      "Iteration 12, loss = 0.10967141\n",
      "Iteration 13, loss = 0.10258564\n",
      "Iteration 14, loss = 0.09769300\n",
      "Iteration 15, loss = 0.09387318\n",
      "Iteration 16, loss = 0.08959938\n",
      "Iteration 17, loss = 0.08463124\n",
      "Iteration 18, loss = 0.08304690\n",
      "Iteration 19, loss = 0.07924504\n",
      "Iteration 20, loss = 0.07711107\n",
      "Iteration 21, loss = 0.07460753\n",
      "Iteration 22, loss = 0.07239511\n",
      "Iteration 23, loss = 0.07043923\n",
      "Iteration 24, loss = 0.06871954\n",
      "Iteration 25, loss = 0.06685495\n",
      "Iteration 26, loss = 0.06475883\n",
      "Iteration 27, loss = 0.06324273\n",
      "Iteration 28, loss = 0.06316966\n",
      "Iteration 29, loss = 0.06188568\n",
      "Iteration 30, loss = 0.05996672\n",
      "Iteration 31, loss = 0.05889611\n",
      "Iteration 32, loss = 0.05794866\n",
      "Iteration 33, loss = 0.05731312\n",
      "Iteration 34, loss = 0.05648661\n",
      "Iteration 35, loss = 0.05503090\n",
      "Iteration 36, loss = 0.05465997\n",
      "Iteration 37, loss = 0.05417951\n",
      "Iteration 38, loss = 0.05296242\n",
      "Iteration 39, loss = 0.05238155\n",
      "Iteration 40, loss = 0.05072246\n",
      "Iteration 41, loss = 0.05115623\n",
      "Iteration 42, loss = 0.05118778\n",
      "Iteration 43, loss = 0.04995347\n",
      "Iteration 44, loss = 0.04930629\n",
      "Iteration 45, loss = 0.04861428\n",
      "Iteration 46, loss = 0.04849660\n",
      "Iteration 47, loss = 0.04801227\n",
      "Iteration 48, loss = 0.04749235\n",
      "Iteration 49, loss = 0.04736662\n",
      "Iteration 50, loss = 0.04729974\n",
      "Iteration 51, loss = 0.04583556\n",
      "Iteration 52, loss = 0.04575328\n",
      "Iteration 53, loss = 0.04621617\n",
      "Iteration 54, loss = 0.04500881\n",
      "Iteration 55, loss = 0.04538198\n",
      "Iteration 56, loss = 0.04415334\n",
      "Iteration 57, loss = 0.04397929\n",
      "Iteration 58, loss = 0.04390146\n",
      "Iteration 59, loss = 0.04397152\n",
      "Iteration 60, loss = 0.04336869\n",
      "Iteration 61, loss = 0.04253146\n",
      "Iteration 62, loss = 0.04284411\n",
      "Iteration 63, loss = 0.04273249\n",
      "Iteration 64, loss = 0.04244097\n",
      "Iteration 65, loss = 0.04225319\n",
      "Iteration 66, loss = 0.04176986\n",
      "Iteration 67, loss = 0.04202339\n",
      "Iteration 68, loss = 0.04139967\n",
      "Iteration 69, loss = 0.04140290\n",
      "Iteration 70, loss = 0.04092559\n",
      "Iteration 71, loss = 0.04073589\n",
      "Iteration 72, loss = 0.04032713\n",
      "Iteration 73, loss = 0.04070249\n",
      "Iteration 74, loss = 0.04096447\n",
      "Iteration 75, loss = 0.03988473\n",
      "Iteration 76, loss = 0.03997398\n",
      "Iteration 77, loss = 0.03949007\n",
      "Iteration 78, loss = 0.04012688\n",
      "Iteration 79, loss = 0.03928734\n",
      "Iteration 80, loss = 0.03899547\n",
      "Iteration 81, loss = 0.03942679\n",
      "Iteration 82, loss = 0.03887854\n",
      "Iteration 83, loss = 0.03916713\n",
      "Iteration 84, loss = 0.03851526\n",
      "Iteration 85, loss = 0.03802201\n",
      "Iteration 86, loss = 0.03784236\n",
      "Iteration 87, loss = 0.03862088\n",
      "Iteration 88, loss = 0.03803656\n",
      "Iteration 89, loss = 0.03752089\n",
      "Iteration 90, loss = 0.03746179\n",
      "Iteration 91, loss = 0.03771276\n",
      "Iteration 92, loss = 0.03783044\n",
      "Iteration 93, loss = 0.03718743\n",
      "Iteration 94, loss = 0.03714439\n",
      "Iteration 95, loss = 0.03726564\n",
      "Iteration 96, loss = 0.03667234\n",
      "Iteration 97, loss = 0.03772134\n",
      "Iteration 98, loss = 0.03652367\n",
      "Iteration 99, loss = 0.03678433\n",
      "Iteration 100, loss = 0.03662392\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kileymack/opt/anaconda3/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 2.15532577\n",
      "Iteration 2, loss = 0.39343972\n",
      "Iteration 3, loss = 0.31877239\n",
      "Iteration 4, loss = 0.28081219\n",
      "Iteration 5, loss = 0.25778406\n",
      "Iteration 6, loss = 0.23950796\n",
      "Iteration 7, loss = 0.22597633\n",
      "Iteration 8, loss = 0.21423855\n",
      "Iteration 9, loss = 0.20529436\n",
      "Iteration 10, loss = 0.19730490\n",
      "Iteration 11, loss = 0.18962429\n",
      "Iteration 12, loss = 0.18351231\n",
      "Iteration 13, loss = 0.17793223\n",
      "Iteration 14, loss = 0.17227001\n",
      "Iteration 15, loss = 0.16799199\n",
      "Iteration 16, loss = 0.16330386\n",
      "Iteration 17, loss = 0.15934790\n",
      "Iteration 18, loss = 0.15570746\n",
      "Iteration 19, loss = 0.15197342\n",
      "Iteration 20, loss = 0.14879394\n",
      "Iteration 21, loss = 0.14569451\n",
      "Iteration 22, loss = 0.14277227\n",
      "Iteration 23, loss = 0.14011437\n",
      "Iteration 24, loss = 0.13742010\n",
      "Iteration 25, loss = 0.13520361\n",
      "Iteration 26, loss = 0.13247948\n",
      "Iteration 27, loss = 0.13033816\n",
      "Iteration 28, loss = 0.12884792\n",
      "Iteration 29, loss = 0.12616039\n",
      "Iteration 30, loss = 0.12453787\n",
      "Iteration 31, loss = 0.12229934\n",
      "Iteration 32, loss = 0.12019452\n",
      "Iteration 33, loss = 0.11837287\n",
      "Iteration 34, loss = 0.11687369\n",
      "Iteration 35, loss = 0.11539931\n",
      "Iteration 36, loss = 0.11328846\n",
      "Iteration 37, loss = 0.11219901\n",
      "Iteration 38, loss = 0.11018904\n",
      "Iteration 39, loss = 0.10892576\n",
      "Iteration 40, loss = 0.10752466\n",
      "Iteration 41, loss = 0.10600019\n",
      "Iteration 42, loss = 0.10469339\n",
      "Iteration 43, loss = 0.10332750\n",
      "Iteration 44, loss = 0.10222080\n",
      "Iteration 45, loss = 0.10089640\n",
      "Iteration 46, loss = 0.09995207\n",
      "Iteration 47, loss = 0.09858791\n",
      "Iteration 48, loss = 0.09768258\n",
      "Iteration 49, loss = 0.09631769\n",
      "Iteration 50, loss = 0.09527707\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kileymack/opt/anaconda3/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (50) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 1.87154053\n",
      "Iteration 2, loss = 0.36984641\n",
      "Iteration 3, loss = 0.30516072\n",
      "Iteration 4, loss = 0.26994873\n",
      "Iteration 5, loss = 0.24698589\n",
      "Iteration 6, loss = 0.23022427\n",
      "Iteration 7, loss = 0.21633826\n",
      "Iteration 8, loss = 0.20582139\n",
      "Iteration 9, loss = 0.19615374\n",
      "Iteration 10, loss = 0.18765500\n",
      "Iteration 11, loss = 0.18006272\n",
      "Iteration 12, loss = 0.17382802\n",
      "Iteration 13, loss = 0.16796413\n",
      "Iteration 14, loss = 0.16289001\n",
      "Iteration 15, loss = 0.15812997\n",
      "Iteration 16, loss = 0.15364505\n",
      "Iteration 17, loss = 0.14933868\n",
      "Iteration 18, loss = 0.14575272\n",
      "Iteration 19, loss = 0.14237033\n",
      "Iteration 20, loss = 0.13907103\n",
      "Iteration 21, loss = 0.13642913\n",
      "Iteration 22, loss = 0.13356152\n",
      "Iteration 23, loss = 0.13071465\n",
      "Iteration 24, loss = 0.12837162\n",
      "Iteration 25, loss = 0.12605723\n",
      "Iteration 26, loss = 0.12373836\n",
      "Iteration 27, loss = 0.12178743\n",
      "Iteration 28, loss = 0.11997184\n",
      "Iteration 29, loss = 0.11779571\n",
      "Iteration 30, loss = 0.11604227\n",
      "Iteration 31, loss = 0.11430701\n",
      "Iteration 32, loss = 0.11251499\n",
      "Iteration 33, loss = 0.11090184\n",
      "Iteration 34, loss = 0.10925848\n",
      "Iteration 35, loss = 0.10787363\n",
      "Iteration 36, loss = 0.10640827\n",
      "Iteration 37, loss = 0.10508527\n",
      "Iteration 38, loss = 0.10370952\n",
      "Iteration 39, loss = 0.10239924\n",
      "Iteration 40, loss = 0.10104234\n",
      "Iteration 41, loss = 0.09982584\n",
      "Iteration 42, loss = 0.09899792\n",
      "Iteration 43, loss = 0.09763897\n",
      "Iteration 44, loss = 0.09626285\n",
      "Iteration 45, loss = 0.09529664\n",
      "Iteration 46, loss = 0.09427948\n",
      "Iteration 47, loss = 0.09337258\n",
      "Iteration 48, loss = 0.09214950\n",
      "Iteration 49, loss = 0.09132326\n",
      "Iteration 50, loss = 0.09040691\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kileymack/opt/anaconda3/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (50) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 6.95007903\n",
      "Iteration 2, loss = 0.89860534\n",
      "Iteration 3, loss = 0.43090945\n",
      "Iteration 4, loss = 0.29672167\n",
      "Iteration 5, loss = 0.23836571\n",
      "Iteration 6, loss = 0.20469992\n",
      "Iteration 7, loss = 0.18703825\n",
      "Iteration 8, loss = 0.16745081\n",
      "Iteration 9, loss = 0.15797741\n",
      "Iteration 10, loss = 0.14938935\n",
      "Iteration 11, loss = 0.13819736\n",
      "Iteration 12, loss = 0.13012764\n",
      "Iteration 13, loss = 0.12605646\n",
      "Iteration 14, loss = 0.12030603\n",
      "Iteration 15, loss = 0.11454012\n",
      "Iteration 16, loss = 0.11011978\n",
      "Iteration 17, loss = 0.10506112\n",
      "Iteration 18, loss = 0.10146477\n",
      "Iteration 19, loss = 0.09738579\n",
      "Iteration 20, loss = 0.09428762\n",
      "Iteration 21, loss = 0.09163282\n",
      "Iteration 22, loss = 0.08808688\n",
      "Iteration 23, loss = 0.08529721\n",
      "Iteration 24, loss = 0.08556597\n",
      "Iteration 25, loss = 0.08235728\n",
      "Iteration 26, loss = 0.07957660\n",
      "Iteration 27, loss = 0.07545961\n",
      "Iteration 28, loss = 0.07523147\n",
      "Iteration 29, loss = 0.07416414\n",
      "Iteration 30, loss = 0.07205068\n",
      "Iteration 31, loss = 0.07023346\n",
      "Iteration 32, loss = 0.06965688\n",
      "Iteration 33, loss = 0.06781974\n",
      "Iteration 34, loss = 0.06574960\n",
      "Iteration 35, loss = 0.06410801\n",
      "Iteration 36, loss = 0.06377814\n",
      "Iteration 37, loss = 0.06275171\n",
      "Iteration 38, loss = 0.06040437\n",
      "Iteration 39, loss = 0.05970472\n",
      "Iteration 40, loss = 0.05974403\n",
      "Iteration 41, loss = 0.05847945\n",
      "Iteration 42, loss = 0.05616877\n",
      "Iteration 43, loss = 0.05623548\n",
      "Iteration 44, loss = 0.05534542\n",
      "Iteration 45, loss = 0.05468963\n",
      "Iteration 46, loss = 0.05404690\n",
      "Iteration 47, loss = 0.05409210\n",
      "Iteration 48, loss = 0.05374392\n",
      "Iteration 49, loss = 0.05123322\n",
      "Iteration 50, loss = 0.05210727\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kileymack/opt/anaconda3/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (50) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 11.30522517\n",
      "Iteration 2, loss = 0.64824290\n",
      "Iteration 3, loss = 0.33222301\n",
      "Iteration 4, loss = 0.25001003\n",
      "Iteration 5, loss = 0.20885924\n",
      "Iteration 6, loss = 0.18571819\n",
      "Iteration 7, loss = 0.16572888\n",
      "Iteration 8, loss = 0.15166235\n",
      "Iteration 9, loss = 0.14082520\n",
      "Iteration 10, loss = 0.13186499\n",
      "Iteration 11, loss = 0.12484348\n",
      "Iteration 12, loss = 0.11807725\n",
      "Iteration 13, loss = 0.11180171\n",
      "Iteration 14, loss = 0.10692312\n",
      "Iteration 15, loss = 0.10156887\n",
      "Iteration 16, loss = 0.09835988\n",
      "Iteration 17, loss = 0.09496421\n",
      "Iteration 18, loss = 0.08927314\n",
      "Iteration 19, loss = 0.08783164\n",
      "Iteration 20, loss = 0.08511380\n",
      "Iteration 21, loss = 0.08244346\n",
      "Iteration 22, loss = 0.08097415\n",
      "Iteration 23, loss = 0.07784105\n",
      "Iteration 24, loss = 0.07721806\n",
      "Iteration 25, loss = 0.07390274\n",
      "Iteration 26, loss = 0.07296439\n",
      "Iteration 27, loss = 0.07139478\n",
      "Iteration 28, loss = 0.06998576\n",
      "Iteration 29, loss = 0.06868677\n",
      "Iteration 30, loss = 0.06775640\n",
      "Iteration 31, loss = 0.06623529\n",
      "Iteration 32, loss = 0.06536271\n",
      "Iteration 33, loss = 0.06461181\n",
      "Iteration 34, loss = 0.06275234\n",
      "Iteration 35, loss = 0.06256400\n",
      "Iteration 36, loss = 0.06067474\n",
      "Iteration 37, loss = 0.06168964\n",
      "Iteration 38, loss = 0.05935376\n",
      "Iteration 39, loss = 0.05896557\n",
      "Iteration 40, loss = 0.05888834\n",
      "Iteration 41, loss = 0.05778169\n",
      "Iteration 42, loss = 0.05652172\n",
      "Iteration 43, loss = 0.05708312\n",
      "Iteration 44, loss = 0.05646905\n",
      "Iteration 45, loss = 0.05573602\n",
      "Iteration 46, loss = 0.05513469\n",
      "Iteration 47, loss = 0.05435184\n",
      "Iteration 48, loss = 0.05378221\n",
      "Iteration 49, loss = 0.05358814\n",
      "Iteration 50, loss = 0.05394177\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kileymack/opt/anaconda3/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (50) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 2.15532577\n",
      "Iteration 2, loss = 0.39343972\n",
      "Iteration 3, loss = 0.31877239\n",
      "Iteration 4, loss = 0.28081219\n",
      "Iteration 5, loss = 0.25778406\n",
      "Iteration 6, loss = 0.23950796\n",
      "Iteration 7, loss = 0.22597633\n",
      "Iteration 8, loss = 0.21423855\n",
      "Iteration 9, loss = 0.20529436\n",
      "Iteration 10, loss = 0.19730490\n",
      "Iteration 11, loss = 0.18962429\n",
      "Iteration 12, loss = 0.18351231\n",
      "Iteration 13, loss = 0.17793223\n",
      "Iteration 14, loss = 0.17227001\n",
      "Iteration 15, loss = 0.16799199\n",
      "Iteration 16, loss = 0.16330386\n",
      "Iteration 17, loss = 0.15934790\n",
      "Iteration 18, loss = 0.15570746\n",
      "Iteration 19, loss = 0.15197342\n",
      "Iteration 20, loss = 0.14879394\n",
      "Iteration 21, loss = 0.14569451\n",
      "Iteration 22, loss = 0.14277227\n",
      "Iteration 23, loss = 0.14011437\n",
      "Iteration 24, loss = 0.13742010\n",
      "Iteration 25, loss = 0.13520361\n",
      "Iteration 26, loss = 0.13247948\n",
      "Iteration 27, loss = 0.13033816\n",
      "Iteration 28, loss = 0.12884792\n",
      "Iteration 29, loss = 0.12616039\n",
      "Iteration 30, loss = 0.12453787\n",
      "Iteration 31, loss = 0.12229934\n",
      "Iteration 32, loss = 0.12019452\n",
      "Iteration 33, loss = 0.11837287\n",
      "Iteration 34, loss = 0.11687369\n",
      "Iteration 35, loss = 0.11539931\n",
      "Iteration 36, loss = 0.11328846\n",
      "Iteration 37, loss = 0.11219901\n",
      "Iteration 38, loss = 0.11018904\n",
      "Iteration 39, loss = 0.10892576\n",
      "Iteration 40, loss = 0.10752466\n",
      "Iteration 41, loss = 0.10600019\n",
      "Iteration 42, loss = 0.10469339\n",
      "Iteration 43, loss = 0.10332750\n",
      "Iteration 44, loss = 0.10222080\n",
      "Iteration 45, loss = 0.10089640\n",
      "Iteration 46, loss = 0.09995207\n",
      "Iteration 47, loss = 0.09858791\n",
      "Iteration 48, loss = 0.09768258\n",
      "Iteration 49, loss = 0.09631769\n",
      "Iteration 50, loss = 0.09527707\n",
      "Iteration 51, loss = 0.09411867\n",
      "Iteration 52, loss = 0.09315386\n",
      "Iteration 53, loss = 0.09193551\n",
      "Iteration 54, loss = 0.09107593\n",
      "Iteration 55, loss = 0.09029313\n",
      "Iteration 56, loss = 0.08910757\n",
      "Iteration 57, loss = 0.08824472\n",
      "Iteration 58, loss = 0.08729992\n",
      "Iteration 59, loss = 0.08663289\n",
      "Iteration 60, loss = 0.08582608\n",
      "Iteration 61, loss = 0.08487516\n",
      "Iteration 62, loss = 0.08415947\n",
      "Iteration 63, loss = 0.08328396\n",
      "Iteration 64, loss = 0.08252720\n",
      "Iteration 65, loss = 0.08158233\n",
      "Iteration 66, loss = 0.08111944\n",
      "Iteration 67, loss = 0.08008782\n",
      "Iteration 68, loss = 0.07948108\n",
      "Iteration 69, loss = 0.07877411\n",
      "Iteration 70, loss = 0.07816953\n",
      "Iteration 71, loss = 0.07732647\n",
      "Iteration 72, loss = 0.07658329\n",
      "Iteration 73, loss = 0.07640389\n",
      "Iteration 74, loss = 0.07556192\n",
      "Iteration 75, loss = 0.07504514\n",
      "Iteration 76, loss = 0.07447729\n",
      "Iteration 77, loss = 0.07369694\n",
      "Iteration 78, loss = 0.07296104\n",
      "Iteration 79, loss = 0.07247008\n",
      "Iteration 80, loss = 0.07206407\n",
      "Iteration 81, loss = 0.07135928\n",
      "Iteration 82, loss = 0.07089460\n",
      "Iteration 83, loss = 0.07047618\n",
      "Iteration 84, loss = 0.06976830\n",
      "Iteration 85, loss = 0.06900083\n",
      "Iteration 86, loss = 0.06859532\n",
      "Iteration 87, loss = 0.06809592\n",
      "Iteration 88, loss = 0.06787188\n",
      "Iteration 89, loss = 0.06708862\n",
      "Iteration 90, loss = 0.06678453\n",
      "Iteration 91, loss = 0.06628728\n",
      "Iteration 92, loss = 0.06566632\n",
      "Iteration 93, loss = 0.06537726\n",
      "Iteration 94, loss = 0.06500960\n",
      "Iteration 95, loss = 0.06469511\n",
      "Iteration 96, loss = 0.06431144\n",
      "Iteration 97, loss = 0.06358698\n",
      "Iteration 98, loss = 0.06305510\n",
      "Iteration 99, loss = 0.06252799\n",
      "Iteration 100, loss = 0.06219326\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kileymack/opt/anaconda3/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 1.87154053\n",
      "Iteration 2, loss = 0.36984641\n",
      "Iteration 3, loss = 0.30516072\n",
      "Iteration 4, loss = 0.26994873\n",
      "Iteration 5, loss = 0.24698589\n",
      "Iteration 6, loss = 0.23022427\n",
      "Iteration 7, loss = 0.21633826\n",
      "Iteration 8, loss = 0.20582139\n",
      "Iteration 9, loss = 0.19615374\n",
      "Iteration 10, loss = 0.18765500\n",
      "Iteration 11, loss = 0.18006272\n",
      "Iteration 12, loss = 0.17382802\n",
      "Iteration 13, loss = 0.16796413\n",
      "Iteration 14, loss = 0.16289001\n",
      "Iteration 15, loss = 0.15812997\n",
      "Iteration 16, loss = 0.15364505\n",
      "Iteration 17, loss = 0.14933868\n",
      "Iteration 18, loss = 0.14575272\n",
      "Iteration 19, loss = 0.14237033\n",
      "Iteration 20, loss = 0.13907103\n",
      "Iteration 21, loss = 0.13642913\n",
      "Iteration 22, loss = 0.13356152\n",
      "Iteration 23, loss = 0.13071465\n",
      "Iteration 24, loss = 0.12837162\n",
      "Iteration 25, loss = 0.12605723\n",
      "Iteration 26, loss = 0.12373836\n",
      "Iteration 27, loss = 0.12178743\n",
      "Iteration 28, loss = 0.11997184\n",
      "Iteration 29, loss = 0.11779571\n",
      "Iteration 30, loss = 0.11604227\n",
      "Iteration 31, loss = 0.11430701\n",
      "Iteration 32, loss = 0.11251499\n",
      "Iteration 33, loss = 0.11090184\n",
      "Iteration 34, loss = 0.10925848\n",
      "Iteration 35, loss = 0.10787363\n",
      "Iteration 36, loss = 0.10640827\n",
      "Iteration 37, loss = 0.10508527\n",
      "Iteration 38, loss = 0.10370952\n",
      "Iteration 39, loss = 0.10239924\n",
      "Iteration 40, loss = 0.10104234\n",
      "Iteration 41, loss = 0.09982584\n",
      "Iteration 42, loss = 0.09899792\n",
      "Iteration 43, loss = 0.09763897\n",
      "Iteration 44, loss = 0.09626285\n",
      "Iteration 45, loss = 0.09529664\n",
      "Iteration 46, loss = 0.09427948\n",
      "Iteration 47, loss = 0.09337258\n",
      "Iteration 48, loss = 0.09214950\n",
      "Iteration 49, loss = 0.09132326\n",
      "Iteration 50, loss = 0.09040691\n",
      "Iteration 51, loss = 0.08934927\n",
      "Iteration 52, loss = 0.08840254\n",
      "Iteration 53, loss = 0.08769428\n",
      "Iteration 54, loss = 0.08666737\n",
      "Iteration 55, loss = 0.08584739\n",
      "Iteration 56, loss = 0.08505522\n",
      "Iteration 57, loss = 0.08419740\n",
      "Iteration 58, loss = 0.08360420\n",
      "Iteration 59, loss = 0.08275419\n",
      "Iteration 60, loss = 0.08180130\n",
      "Iteration 61, loss = 0.08133939\n",
      "Iteration 62, loss = 0.08039931\n",
      "Iteration 63, loss = 0.07977045\n",
      "Iteration 64, loss = 0.07899364\n",
      "Iteration 65, loss = 0.07844505\n",
      "Iteration 66, loss = 0.07750442\n",
      "Iteration 67, loss = 0.07688027\n",
      "Iteration 68, loss = 0.07634798\n",
      "Iteration 69, loss = 0.07572928\n",
      "Iteration 70, loss = 0.07531053\n",
      "Iteration 71, loss = 0.07449440\n",
      "Iteration 72, loss = 0.07391494\n",
      "Iteration 73, loss = 0.07319661\n",
      "Iteration 74, loss = 0.07282381\n",
      "Iteration 75, loss = 0.07228326\n",
      "Iteration 76, loss = 0.07153390\n",
      "Iteration 77, loss = 0.07101744\n",
      "Iteration 78, loss = 0.07049496\n",
      "Iteration 79, loss = 0.07006452\n",
      "Iteration 80, loss = 0.06961019\n",
      "Iteration 81, loss = 0.06907920\n",
      "Iteration 82, loss = 0.06858898\n",
      "Iteration 83, loss = 0.06798621\n",
      "Iteration 84, loss = 0.06753785\n",
      "Iteration 85, loss = 0.06711620\n",
      "Iteration 86, loss = 0.06671475\n",
      "Iteration 87, loss = 0.06606726\n",
      "Iteration 88, loss = 0.06585792\n",
      "Iteration 89, loss = 0.06540590\n",
      "Iteration 90, loss = 0.06486181\n",
      "Iteration 91, loss = 0.06440600\n",
      "Iteration 92, loss = 0.06394931\n",
      "Iteration 93, loss = 0.06359469\n",
      "Iteration 94, loss = 0.06300005\n",
      "Iteration 95, loss = 0.06274133\n",
      "Iteration 96, loss = 0.06235245\n",
      "Iteration 97, loss = 0.06195429\n",
      "Iteration 98, loss = 0.06178340\n",
      "Iteration 99, loss = 0.06125224\n",
      "Iteration 100, loss = 0.06087035\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kileymack/opt/anaconda3/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 6.95007903\n",
      "Iteration 2, loss = 0.89860534\n",
      "Iteration 3, loss = 0.43090945\n",
      "Iteration 4, loss = 0.29672167\n",
      "Iteration 5, loss = 0.23836571\n",
      "Iteration 6, loss = 0.20469992\n",
      "Iteration 7, loss = 0.18703825\n",
      "Iteration 8, loss = 0.16745081\n",
      "Iteration 9, loss = 0.15797741\n",
      "Iteration 10, loss = 0.14938935\n",
      "Iteration 11, loss = 0.13819736\n",
      "Iteration 12, loss = 0.13012764\n",
      "Iteration 13, loss = 0.12605646\n",
      "Iteration 14, loss = 0.12030603\n",
      "Iteration 15, loss = 0.11454012\n",
      "Iteration 16, loss = 0.11011978\n",
      "Iteration 17, loss = 0.10506112\n",
      "Iteration 18, loss = 0.10146477\n",
      "Iteration 19, loss = 0.09738579\n",
      "Iteration 20, loss = 0.09428762\n",
      "Iteration 21, loss = 0.09163282\n",
      "Iteration 22, loss = 0.08808688\n",
      "Iteration 23, loss = 0.08529721\n",
      "Iteration 24, loss = 0.08556597\n",
      "Iteration 25, loss = 0.08235728\n",
      "Iteration 26, loss = 0.07957660\n",
      "Iteration 27, loss = 0.07545961\n",
      "Iteration 28, loss = 0.07523147\n",
      "Iteration 29, loss = 0.07416414\n",
      "Iteration 30, loss = 0.07205068\n",
      "Iteration 31, loss = 0.07023346\n",
      "Iteration 32, loss = 0.06965688\n",
      "Iteration 33, loss = 0.06781974\n",
      "Iteration 34, loss = 0.06574960\n",
      "Iteration 35, loss = 0.06410801\n",
      "Iteration 36, loss = 0.06377814\n",
      "Iteration 37, loss = 0.06275171\n",
      "Iteration 38, loss = 0.06040437\n",
      "Iteration 39, loss = 0.05970472\n",
      "Iteration 40, loss = 0.05974403\n",
      "Iteration 41, loss = 0.05847945\n",
      "Iteration 42, loss = 0.05616877\n",
      "Iteration 43, loss = 0.05623548\n",
      "Iteration 44, loss = 0.05534542\n",
      "Iteration 45, loss = 0.05468963\n",
      "Iteration 46, loss = 0.05404690\n",
      "Iteration 47, loss = 0.05409210\n",
      "Iteration 48, loss = 0.05374392\n",
      "Iteration 49, loss = 0.05123322\n",
      "Iteration 50, loss = 0.05210727\n",
      "Iteration 51, loss = 0.04964791\n",
      "Iteration 52, loss = 0.05004423\n",
      "Iteration 53, loss = 0.04940226\n",
      "Iteration 54, loss = 0.04817692\n",
      "Iteration 55, loss = 0.04800785\n",
      "Iteration 56, loss = 0.04759017\n",
      "Iteration 57, loss = 0.04736742\n",
      "Iteration 58, loss = 0.04652995\n",
      "Iteration 59, loss = 0.04836212\n",
      "Iteration 60, loss = 0.04561903\n",
      "Iteration 61, loss = 0.04498975\n",
      "Iteration 62, loss = 0.04527102\n",
      "Iteration 63, loss = 0.04475023\n",
      "Iteration 64, loss = 0.04292378\n",
      "Iteration 65, loss = 0.04279393\n",
      "Iteration 66, loss = 0.04399331\n",
      "Iteration 67, loss = 0.04199936\n",
      "Iteration 68, loss = 0.04238465\n",
      "Iteration 69, loss = 0.04200100\n",
      "Iteration 70, loss = 0.04105749\n",
      "Iteration 71, loss = 0.04241084\n",
      "Iteration 72, loss = 0.04074487\n",
      "Iteration 73, loss = 0.03986202\n",
      "Iteration 74, loss = 0.04161642\n",
      "Iteration 75, loss = 0.04112469\n",
      "Iteration 76, loss = 0.04169475\n",
      "Iteration 77, loss = 0.03876561\n",
      "Iteration 78, loss = 0.04001380\n",
      "Iteration 79, loss = 0.03887021\n",
      "Iteration 80, loss = 0.03839580\n",
      "Iteration 81, loss = 0.03833841\n",
      "Iteration 82, loss = 0.03779856\n",
      "Iteration 83, loss = 0.03917616\n",
      "Iteration 84, loss = 0.03734818\n",
      "Iteration 85, loss = 0.03863037\n",
      "Iteration 86, loss = 0.03816627\n",
      "Iteration 87, loss = 0.03526823\n",
      "Iteration 88, loss = 0.03708719\n",
      "Iteration 89, loss = 0.03696573\n",
      "Iteration 90, loss = 0.03801300\n",
      "Iteration 91, loss = 0.03526996\n",
      "Iteration 92, loss = 0.03592328\n",
      "Iteration 93, loss = 0.03550895\n",
      "Iteration 94, loss = 0.03635505\n",
      "Iteration 95, loss = 0.03692918\n",
      "Iteration 96, loss = 0.03491642\n",
      "Iteration 97, loss = 0.03546942\n",
      "Iteration 98, loss = 0.03511079\n",
      "Iteration 99, loss = 0.03565220\n",
      "Iteration 100, loss = 0.03586012\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kileymack/opt/anaconda3/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 11.30522517\n",
      "Iteration 2, loss = 0.64824290\n",
      "Iteration 3, loss = 0.33222301\n",
      "Iteration 4, loss = 0.25001003\n",
      "Iteration 5, loss = 0.20885924\n",
      "Iteration 6, loss = 0.18571819\n",
      "Iteration 7, loss = 0.16572888\n",
      "Iteration 8, loss = 0.15166235\n",
      "Iteration 9, loss = 0.14082520\n",
      "Iteration 10, loss = 0.13186499\n",
      "Iteration 11, loss = 0.12484348\n",
      "Iteration 12, loss = 0.11807725\n",
      "Iteration 13, loss = 0.11180171\n",
      "Iteration 14, loss = 0.10692312\n",
      "Iteration 15, loss = 0.10156887\n",
      "Iteration 16, loss = 0.09835988\n",
      "Iteration 17, loss = 0.09496421\n",
      "Iteration 18, loss = 0.08927314\n",
      "Iteration 19, loss = 0.08783164\n",
      "Iteration 20, loss = 0.08511380\n",
      "Iteration 21, loss = 0.08244346\n",
      "Iteration 22, loss = 0.08097415\n",
      "Iteration 23, loss = 0.07784105\n",
      "Iteration 24, loss = 0.07721806\n",
      "Iteration 25, loss = 0.07390274\n",
      "Iteration 26, loss = 0.07296439\n",
      "Iteration 27, loss = 0.07139478\n",
      "Iteration 28, loss = 0.06998576\n",
      "Iteration 29, loss = 0.06868677\n",
      "Iteration 30, loss = 0.06775640\n",
      "Iteration 31, loss = 0.06623529\n",
      "Iteration 32, loss = 0.06536271\n",
      "Iteration 33, loss = 0.06461181\n",
      "Iteration 34, loss = 0.06275234\n",
      "Iteration 35, loss = 0.06256400\n",
      "Iteration 36, loss = 0.06067474\n",
      "Iteration 37, loss = 0.06168964\n",
      "Iteration 38, loss = 0.05935376\n",
      "Iteration 39, loss = 0.05896557\n",
      "Iteration 40, loss = 0.05888834\n",
      "Iteration 41, loss = 0.05778169\n",
      "Iteration 42, loss = 0.05652172\n",
      "Iteration 43, loss = 0.05708312\n",
      "Iteration 44, loss = 0.05646905\n",
      "Iteration 45, loss = 0.05573602\n",
      "Iteration 46, loss = 0.05513469\n",
      "Iteration 47, loss = 0.05435184\n",
      "Iteration 48, loss = 0.05378221\n",
      "Iteration 49, loss = 0.05358814\n",
      "Iteration 50, loss = 0.05394177\n",
      "Iteration 51, loss = 0.05234197\n",
      "Iteration 52, loss = 0.05203626\n",
      "Iteration 53, loss = 0.05222042\n",
      "Iteration 54, loss = 0.05091496\n",
      "Iteration 55, loss = 0.05117624\n",
      "Iteration 56, loss = 0.05104633\n",
      "Iteration 57, loss = 0.05046523\n",
      "Iteration 58, loss = 0.04967816\n",
      "Iteration 59, loss = 0.05026631\n",
      "Iteration 60, loss = 0.04950935\n",
      "Iteration 61, loss = 0.04901356\n",
      "Iteration 62, loss = 0.04878451\n",
      "Iteration 63, loss = 0.04856701\n",
      "Iteration 64, loss = 0.04822889\n",
      "Iteration 65, loss = 0.04777637\n",
      "Iteration 66, loss = 0.04759901\n",
      "Iteration 67, loss = 0.04731421\n",
      "Iteration 68, loss = 0.04695346\n",
      "Iteration 69, loss = 0.04713318\n",
      "Iteration 70, loss = 0.04705130\n",
      "Iteration 71, loss = 0.04652174\n",
      "Iteration 72, loss = 0.04686025\n",
      "Iteration 73, loss = 0.04609435\n",
      "Iteration 74, loss = 0.04620648\n",
      "Iteration 75, loss = 0.04575359\n",
      "Iteration 76, loss = 0.04529759\n",
      "Iteration 77, loss = 0.04586443\n",
      "Iteration 78, loss = 0.04470437\n",
      "Iteration 79, loss = 0.04539546\n",
      "Iteration 80, loss = 0.04505047\n",
      "Iteration 81, loss = 0.04434726\n",
      "Iteration 82, loss = 0.04487654\n",
      "Iteration 83, loss = 0.04413282\n",
      "Iteration 84, loss = 0.04434732\n",
      "Iteration 85, loss = 0.04448294\n",
      "Iteration 86, loss = 0.04450282\n",
      "Iteration 87, loss = 0.04364103\n",
      "Iteration 88, loss = 0.04335131\n",
      "Iteration 89, loss = 0.04323819\n",
      "Iteration 90, loss = 0.04397529\n",
      "Iteration 91, loss = 0.04336111\n",
      "Iteration 92, loss = 0.04350908\n",
      "Iteration 93, loss = 0.04275669\n",
      "Iteration 94, loss = 0.04281419\n",
      "Iteration 95, loss = 0.04278931\n",
      "Iteration 96, loss = 0.04276710\n",
      "Iteration 97, loss = 0.04232463\n",
      "Iteration 98, loss = 0.04202264\n",
      "Iteration 99, loss = 0.04243852\n",
      "Iteration 100, loss = 0.04236794\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kileymack/opt/anaconda3/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 2.15532577\n",
      "Iteration 2, loss = 0.39343972\n",
      "Iteration 3, loss = 0.31877239\n",
      "Iteration 4, loss = 0.28081219\n",
      "Iteration 5, loss = 0.25778406\n",
      "Iteration 6, loss = 0.23950796\n",
      "Iteration 7, loss = 0.22597633\n",
      "Iteration 8, loss = 0.21423855\n",
      "Iteration 9, loss = 0.20529436\n",
      "Iteration 10, loss = 0.19730490\n",
      "Iteration 11, loss = 0.18962429\n",
      "Iteration 12, loss = 0.18351231\n",
      "Iteration 13, loss = 0.17793223\n",
      "Iteration 14, loss = 0.17227001\n",
      "Iteration 15, loss = 0.16799199\n",
      "Iteration 16, loss = 0.16330386\n",
      "Iteration 17, loss = 0.15934790\n",
      "Iteration 18, loss = 0.15570746\n",
      "Iteration 19, loss = 0.15197342\n",
      "Iteration 20, loss = 0.14879394\n",
      "Iteration 21, loss = 0.14569451\n",
      "Iteration 22, loss = 0.14277227\n",
      "Iteration 23, loss = 0.14011437\n",
      "Iteration 24, loss = 0.13742010\n",
      "Iteration 25, loss = 0.13520361\n",
      "Iteration 26, loss = 0.13247948\n",
      "Iteration 27, loss = 0.13033816\n",
      "Iteration 28, loss = 0.12884792\n",
      "Iteration 29, loss = 0.12616039\n",
      "Iteration 30, loss = 0.12453787\n",
      "Iteration 31, loss = 0.12229934\n",
      "Iteration 32, loss = 0.12019452\n",
      "Iteration 33, loss = 0.11837287\n",
      "Iteration 34, loss = 0.11687369\n",
      "Iteration 35, loss = 0.11539931\n",
      "Iteration 36, loss = 0.11328846\n",
      "Iteration 37, loss = 0.11219901\n",
      "Iteration 38, loss = 0.11018904\n",
      "Iteration 39, loss = 0.10892576\n",
      "Iteration 40, loss = 0.10752466\n",
      "Iteration 41, loss = 0.10600019\n",
      "Iteration 42, loss = 0.10469339\n",
      "Iteration 43, loss = 0.10332750\n",
      "Iteration 44, loss = 0.10222080\n",
      "Iteration 45, loss = 0.10089640\n",
      "Iteration 46, loss = 0.09995207\n",
      "Iteration 47, loss = 0.09858791\n",
      "Iteration 48, loss = 0.09768258\n",
      "Iteration 49, loss = 0.09631769\n",
      "Iteration 50, loss = 0.09527707\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kileymack/opt/anaconda3/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (50) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 1.87154053\n",
      "Iteration 2, loss = 0.36984641\n",
      "Iteration 3, loss = 0.30516072\n",
      "Iteration 4, loss = 0.26994873\n",
      "Iteration 5, loss = 0.24698589\n",
      "Iteration 6, loss = 0.23022427\n",
      "Iteration 7, loss = 0.21633826\n",
      "Iteration 8, loss = 0.20582139\n",
      "Iteration 9, loss = 0.19615374\n",
      "Iteration 10, loss = 0.18765500\n",
      "Iteration 11, loss = 0.18006272\n",
      "Iteration 12, loss = 0.17382802\n",
      "Iteration 13, loss = 0.16796413\n",
      "Iteration 14, loss = 0.16289001\n",
      "Iteration 15, loss = 0.15812997\n",
      "Iteration 16, loss = 0.15364505\n",
      "Iteration 17, loss = 0.14933868\n",
      "Iteration 18, loss = 0.14575272\n",
      "Iteration 19, loss = 0.14237033\n",
      "Iteration 20, loss = 0.13907103\n",
      "Iteration 21, loss = 0.13642913\n",
      "Iteration 22, loss = 0.13356152\n",
      "Iteration 23, loss = 0.13071465\n",
      "Iteration 24, loss = 0.12837162\n",
      "Iteration 25, loss = 0.12605723\n",
      "Iteration 26, loss = 0.12373836\n",
      "Iteration 27, loss = 0.12178743\n",
      "Iteration 28, loss = 0.11997184\n",
      "Iteration 29, loss = 0.11779571\n",
      "Iteration 30, loss = 0.11604227\n",
      "Iteration 31, loss = 0.11430701\n",
      "Iteration 32, loss = 0.11251499\n",
      "Iteration 33, loss = 0.11090184\n",
      "Iteration 34, loss = 0.10925848\n",
      "Iteration 35, loss = 0.10787363\n",
      "Iteration 36, loss = 0.10640827\n",
      "Iteration 37, loss = 0.10508527\n",
      "Iteration 38, loss = 0.10370952\n",
      "Iteration 39, loss = 0.10239924\n",
      "Iteration 40, loss = 0.10104234\n",
      "Iteration 41, loss = 0.09982584\n",
      "Iteration 42, loss = 0.09899792\n",
      "Iteration 43, loss = 0.09763897\n",
      "Iteration 44, loss = 0.09626285\n",
      "Iteration 45, loss = 0.09529664\n",
      "Iteration 46, loss = 0.09427948\n",
      "Iteration 47, loss = 0.09337258\n",
      "Iteration 48, loss = 0.09214950\n",
      "Iteration 49, loss = 0.09132326\n",
      "Iteration 50, loss = 0.09040691\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kileymack/opt/anaconda3/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (50) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 6.95007903\n",
      "Iteration 2, loss = 0.89860534\n",
      "Iteration 3, loss = 0.43090945\n",
      "Iteration 4, loss = 0.29672167\n",
      "Iteration 5, loss = 0.23836571\n",
      "Iteration 6, loss = 0.20469992\n",
      "Iteration 7, loss = 0.18703825\n",
      "Iteration 8, loss = 0.16745081\n",
      "Iteration 9, loss = 0.15797741\n",
      "Iteration 10, loss = 0.14938935\n",
      "Iteration 11, loss = 0.13819736\n",
      "Iteration 12, loss = 0.13012764\n",
      "Iteration 13, loss = 0.12605646\n",
      "Iteration 14, loss = 0.12030603\n",
      "Iteration 15, loss = 0.11454012\n",
      "Iteration 16, loss = 0.11011978\n",
      "Iteration 17, loss = 0.10506112\n",
      "Iteration 18, loss = 0.10146477\n",
      "Iteration 19, loss = 0.09738579\n",
      "Iteration 20, loss = 0.09428762\n",
      "Iteration 21, loss = 0.09163282\n",
      "Iteration 22, loss = 0.08808688\n",
      "Iteration 23, loss = 0.08529721\n",
      "Iteration 24, loss = 0.08556597\n",
      "Iteration 25, loss = 0.08235728\n",
      "Iteration 26, loss = 0.07957660\n",
      "Iteration 27, loss = 0.07545961\n",
      "Iteration 28, loss = 0.07523147\n",
      "Iteration 29, loss = 0.07416414\n",
      "Iteration 30, loss = 0.07205068\n",
      "Iteration 31, loss = 0.07023346\n",
      "Iteration 32, loss = 0.06965688\n",
      "Iteration 33, loss = 0.06781974\n",
      "Iteration 34, loss = 0.06574960\n",
      "Iteration 35, loss = 0.06410801\n",
      "Iteration 36, loss = 0.06377814\n",
      "Iteration 37, loss = 0.06275171\n",
      "Iteration 38, loss = 0.06040437\n",
      "Iteration 39, loss = 0.05970472\n",
      "Iteration 40, loss = 0.05974403\n",
      "Iteration 41, loss = 0.05847945\n",
      "Iteration 42, loss = 0.05616877\n",
      "Iteration 43, loss = 0.05623548\n",
      "Iteration 44, loss = 0.05534542\n",
      "Iteration 45, loss = 0.05468963\n",
      "Iteration 46, loss = 0.05404690\n",
      "Iteration 47, loss = 0.05409210\n",
      "Iteration 48, loss = 0.05374392\n",
      "Iteration 49, loss = 0.05123322\n",
      "Iteration 50, loss = 0.05210727\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kileymack/opt/anaconda3/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (50) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 11.30522517\n",
      "Iteration 2, loss = 0.64824290\n",
      "Iteration 3, loss = 0.33222301\n",
      "Iteration 4, loss = 0.25001003\n",
      "Iteration 5, loss = 0.20885924\n",
      "Iteration 6, loss = 0.18571819\n",
      "Iteration 7, loss = 0.16572888\n",
      "Iteration 8, loss = 0.15166235\n",
      "Iteration 9, loss = 0.14082520\n",
      "Iteration 10, loss = 0.13186499\n",
      "Iteration 11, loss = 0.12484348\n",
      "Iteration 12, loss = 0.11807725\n",
      "Iteration 13, loss = 0.11180171\n",
      "Iteration 14, loss = 0.10692312\n",
      "Iteration 15, loss = 0.10156887\n",
      "Iteration 16, loss = 0.09835988\n",
      "Iteration 17, loss = 0.09496421\n",
      "Iteration 18, loss = 0.08927314\n",
      "Iteration 19, loss = 0.08783164\n",
      "Iteration 20, loss = 0.08511380\n",
      "Iteration 21, loss = 0.08244346\n",
      "Iteration 22, loss = 0.08097415\n",
      "Iteration 23, loss = 0.07784105\n",
      "Iteration 24, loss = 0.07721806\n",
      "Iteration 25, loss = 0.07390274\n",
      "Iteration 26, loss = 0.07296439\n",
      "Iteration 27, loss = 0.07139478\n",
      "Iteration 28, loss = 0.06998576\n",
      "Iteration 29, loss = 0.06868677\n",
      "Iteration 30, loss = 0.06775640\n",
      "Iteration 31, loss = 0.06623529\n",
      "Iteration 32, loss = 0.06536271\n",
      "Iteration 33, loss = 0.06461181\n",
      "Iteration 34, loss = 0.06275234\n",
      "Iteration 35, loss = 0.06256400\n",
      "Iteration 36, loss = 0.06067474\n",
      "Iteration 37, loss = 0.06168964\n",
      "Iteration 38, loss = 0.05935376\n",
      "Iteration 39, loss = 0.05896557\n",
      "Iteration 40, loss = 0.05888834\n",
      "Iteration 41, loss = 0.05778169\n",
      "Iteration 42, loss = 0.05652172\n",
      "Iteration 43, loss = 0.05708312\n",
      "Iteration 44, loss = 0.05646905\n",
      "Iteration 45, loss = 0.05573602\n",
      "Iteration 46, loss = 0.05513469\n",
      "Iteration 47, loss = 0.05435184\n",
      "Iteration 48, loss = 0.05378221\n",
      "Iteration 49, loss = 0.05358814\n",
      "Iteration 50, loss = 0.05394177\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kileymack/opt/anaconda3/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (50) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 2.15532577\n",
      "Iteration 2, loss = 0.39343972\n",
      "Iteration 3, loss = 0.31877239\n",
      "Iteration 4, loss = 0.28081219\n",
      "Iteration 5, loss = 0.25778406\n",
      "Iteration 6, loss = 0.23950796\n",
      "Iteration 7, loss = 0.22597633\n",
      "Iteration 8, loss = 0.21423855\n",
      "Iteration 9, loss = 0.20529436\n",
      "Iteration 10, loss = 0.19730490\n",
      "Iteration 11, loss = 0.18962429\n",
      "Iteration 12, loss = 0.18351231\n",
      "Iteration 13, loss = 0.17793223\n",
      "Iteration 14, loss = 0.17227001\n",
      "Iteration 15, loss = 0.16799199\n",
      "Iteration 16, loss = 0.16330386\n",
      "Iteration 17, loss = 0.15934790\n",
      "Iteration 18, loss = 0.15570746\n",
      "Iteration 19, loss = 0.15197342\n",
      "Iteration 20, loss = 0.14879394\n",
      "Iteration 21, loss = 0.14569451\n",
      "Iteration 22, loss = 0.14277227\n",
      "Iteration 23, loss = 0.14011437\n",
      "Iteration 24, loss = 0.13742010\n",
      "Iteration 25, loss = 0.13520361\n",
      "Iteration 26, loss = 0.13247948\n",
      "Iteration 27, loss = 0.13033816\n",
      "Iteration 28, loss = 0.12884792\n",
      "Iteration 29, loss = 0.12616039\n",
      "Iteration 30, loss = 0.12453787\n",
      "Iteration 31, loss = 0.12229934\n",
      "Iteration 32, loss = 0.12019452\n",
      "Iteration 33, loss = 0.11837287\n",
      "Iteration 34, loss = 0.11687369\n",
      "Iteration 35, loss = 0.11539931\n",
      "Iteration 36, loss = 0.11328846\n",
      "Iteration 37, loss = 0.11219901\n",
      "Iteration 38, loss = 0.11018904\n",
      "Iteration 39, loss = 0.10892576\n",
      "Iteration 40, loss = 0.10752466\n",
      "Iteration 41, loss = 0.10600019\n",
      "Iteration 42, loss = 0.10469339\n",
      "Iteration 43, loss = 0.10332750\n",
      "Iteration 44, loss = 0.10222080\n",
      "Iteration 45, loss = 0.10089640\n",
      "Iteration 46, loss = 0.09995207\n",
      "Iteration 47, loss = 0.09858791\n",
      "Iteration 48, loss = 0.09768258\n",
      "Iteration 49, loss = 0.09631769\n",
      "Iteration 50, loss = 0.09527707\n",
      "Iteration 51, loss = 0.09411867\n",
      "Iteration 52, loss = 0.09315386\n",
      "Iteration 53, loss = 0.09193551\n",
      "Iteration 54, loss = 0.09107593\n",
      "Iteration 55, loss = 0.09029313\n",
      "Iteration 56, loss = 0.08910757\n",
      "Iteration 57, loss = 0.08824472\n",
      "Iteration 58, loss = 0.08729992\n",
      "Iteration 59, loss = 0.08663289\n",
      "Iteration 60, loss = 0.08582608\n",
      "Iteration 61, loss = 0.08487516\n",
      "Iteration 62, loss = 0.08415947\n",
      "Iteration 63, loss = 0.08328396\n",
      "Iteration 64, loss = 0.08252720\n",
      "Iteration 65, loss = 0.08158233\n",
      "Iteration 66, loss = 0.08111944\n",
      "Iteration 67, loss = 0.08008782\n",
      "Iteration 68, loss = 0.07948108\n",
      "Iteration 69, loss = 0.07877411\n",
      "Iteration 70, loss = 0.07816953\n",
      "Iteration 71, loss = 0.07732647\n",
      "Iteration 72, loss = 0.07658329\n",
      "Iteration 73, loss = 0.07640389\n",
      "Iteration 74, loss = 0.07556192\n",
      "Iteration 75, loss = 0.07504514\n",
      "Iteration 76, loss = 0.07447729\n",
      "Iteration 77, loss = 0.07369694\n",
      "Iteration 78, loss = 0.07296104\n",
      "Iteration 79, loss = 0.07247008\n",
      "Iteration 80, loss = 0.07206407\n",
      "Iteration 81, loss = 0.07135928\n",
      "Iteration 82, loss = 0.07089460\n",
      "Iteration 83, loss = 0.07047618\n",
      "Iteration 84, loss = 0.06976830\n",
      "Iteration 85, loss = 0.06900083\n",
      "Iteration 86, loss = 0.06859532\n",
      "Iteration 87, loss = 0.06809592\n",
      "Iteration 88, loss = 0.06787188\n",
      "Iteration 89, loss = 0.06708862\n",
      "Iteration 90, loss = 0.06678453\n",
      "Iteration 91, loss = 0.06628728\n",
      "Iteration 92, loss = 0.06566632\n",
      "Iteration 93, loss = 0.06537726\n",
      "Iteration 94, loss = 0.06500960\n",
      "Iteration 95, loss = 0.06469511\n",
      "Iteration 96, loss = 0.06431144\n",
      "Iteration 97, loss = 0.06358698\n",
      "Iteration 98, loss = 0.06305510\n",
      "Iteration 99, loss = 0.06252799\n",
      "Iteration 100, loss = 0.06219326\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kileymack/opt/anaconda3/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 1.87154053\n",
      "Iteration 2, loss = 0.36984641\n",
      "Iteration 3, loss = 0.30516072\n",
      "Iteration 4, loss = 0.26994873\n",
      "Iteration 5, loss = 0.24698589\n",
      "Iteration 6, loss = 0.23022427\n",
      "Iteration 7, loss = 0.21633826\n",
      "Iteration 8, loss = 0.20582139\n",
      "Iteration 9, loss = 0.19615374\n",
      "Iteration 10, loss = 0.18765500\n",
      "Iteration 11, loss = 0.18006272\n",
      "Iteration 12, loss = 0.17382802\n",
      "Iteration 13, loss = 0.16796413\n",
      "Iteration 14, loss = 0.16289001\n",
      "Iteration 15, loss = 0.15812997\n",
      "Iteration 16, loss = 0.15364505\n",
      "Iteration 17, loss = 0.14933868\n",
      "Iteration 18, loss = 0.14575272\n",
      "Iteration 19, loss = 0.14237033\n",
      "Iteration 20, loss = 0.13907103\n",
      "Iteration 21, loss = 0.13642913\n",
      "Iteration 22, loss = 0.13356152\n",
      "Iteration 23, loss = 0.13071465\n",
      "Iteration 24, loss = 0.12837162\n",
      "Iteration 25, loss = 0.12605723\n",
      "Iteration 26, loss = 0.12373836\n",
      "Iteration 27, loss = 0.12178743\n",
      "Iteration 28, loss = 0.11997184\n",
      "Iteration 29, loss = 0.11779571\n",
      "Iteration 30, loss = 0.11604227\n",
      "Iteration 31, loss = 0.11430701\n",
      "Iteration 32, loss = 0.11251499\n",
      "Iteration 33, loss = 0.11090184\n",
      "Iteration 34, loss = 0.10925848\n",
      "Iteration 35, loss = 0.10787363\n",
      "Iteration 36, loss = 0.10640827\n",
      "Iteration 37, loss = 0.10508527\n",
      "Iteration 38, loss = 0.10370952\n",
      "Iteration 39, loss = 0.10239924\n",
      "Iteration 40, loss = 0.10104234\n",
      "Iteration 41, loss = 0.09982584\n",
      "Iteration 42, loss = 0.09899792\n",
      "Iteration 43, loss = 0.09763897\n",
      "Iteration 44, loss = 0.09626285\n",
      "Iteration 45, loss = 0.09529664\n",
      "Iteration 46, loss = 0.09427948\n",
      "Iteration 47, loss = 0.09337258\n",
      "Iteration 48, loss = 0.09214950\n",
      "Iteration 49, loss = 0.09132326\n",
      "Iteration 50, loss = 0.09040691\n",
      "Iteration 51, loss = 0.08934927\n",
      "Iteration 52, loss = 0.08840254\n",
      "Iteration 53, loss = 0.08769428\n",
      "Iteration 54, loss = 0.08666737\n",
      "Iteration 55, loss = 0.08584739\n",
      "Iteration 56, loss = 0.08505522\n",
      "Iteration 57, loss = 0.08419740\n",
      "Iteration 58, loss = 0.08360420\n",
      "Iteration 59, loss = 0.08275419\n",
      "Iteration 60, loss = 0.08180130\n",
      "Iteration 61, loss = 0.08133939\n",
      "Iteration 62, loss = 0.08039931\n",
      "Iteration 63, loss = 0.07977045\n",
      "Iteration 64, loss = 0.07899364\n",
      "Iteration 65, loss = 0.07844505\n",
      "Iteration 66, loss = 0.07750442\n",
      "Iteration 67, loss = 0.07688027\n",
      "Iteration 68, loss = 0.07634798\n",
      "Iteration 69, loss = 0.07572928\n",
      "Iteration 70, loss = 0.07531053\n",
      "Iteration 71, loss = 0.07449440\n",
      "Iteration 72, loss = 0.07391494\n",
      "Iteration 73, loss = 0.07319661\n",
      "Iteration 74, loss = 0.07282381\n",
      "Iteration 75, loss = 0.07228326\n",
      "Iteration 76, loss = 0.07153390\n",
      "Iteration 77, loss = 0.07101744\n",
      "Iteration 78, loss = 0.07049496\n",
      "Iteration 79, loss = 0.07006452\n",
      "Iteration 80, loss = 0.06961019\n",
      "Iteration 81, loss = 0.06907920\n",
      "Iteration 82, loss = 0.06858898\n",
      "Iteration 83, loss = 0.06798621\n",
      "Iteration 84, loss = 0.06753785\n",
      "Iteration 85, loss = 0.06711620\n",
      "Iteration 86, loss = 0.06671475\n",
      "Iteration 87, loss = 0.06606726\n",
      "Iteration 88, loss = 0.06585792\n",
      "Iteration 89, loss = 0.06540590\n",
      "Iteration 90, loss = 0.06486181\n",
      "Iteration 91, loss = 0.06440600\n",
      "Iteration 92, loss = 0.06394931\n",
      "Iteration 93, loss = 0.06359469\n",
      "Iteration 94, loss = 0.06300005\n",
      "Iteration 95, loss = 0.06274133\n",
      "Iteration 96, loss = 0.06235245\n",
      "Iteration 97, loss = 0.06195429\n",
      "Iteration 98, loss = 0.06178340\n",
      "Iteration 99, loss = 0.06125224\n",
      "Iteration 100, loss = 0.06087035\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kileymack/opt/anaconda3/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 6.95007903\n",
      "Iteration 2, loss = 0.89860534\n",
      "Iteration 3, loss = 0.43090945\n",
      "Iteration 4, loss = 0.29672167\n",
      "Iteration 5, loss = 0.23836571\n",
      "Iteration 6, loss = 0.20469992\n",
      "Iteration 7, loss = 0.18703825\n",
      "Iteration 8, loss = 0.16745081\n",
      "Iteration 9, loss = 0.15797741\n",
      "Iteration 10, loss = 0.14938935\n",
      "Iteration 11, loss = 0.13819736\n",
      "Iteration 12, loss = 0.13012764\n",
      "Iteration 13, loss = 0.12605646\n",
      "Iteration 14, loss = 0.12030603\n",
      "Iteration 15, loss = 0.11454012\n",
      "Iteration 16, loss = 0.11011978\n",
      "Iteration 17, loss = 0.10506112\n",
      "Iteration 18, loss = 0.10146477\n",
      "Iteration 19, loss = 0.09738579\n",
      "Iteration 20, loss = 0.09428762\n",
      "Iteration 21, loss = 0.09163282\n",
      "Iteration 22, loss = 0.08808688\n",
      "Iteration 23, loss = 0.08529721\n",
      "Iteration 24, loss = 0.08556597\n",
      "Iteration 25, loss = 0.08235728\n",
      "Iteration 26, loss = 0.07957660\n",
      "Iteration 27, loss = 0.07545961\n",
      "Iteration 28, loss = 0.07523147\n",
      "Iteration 29, loss = 0.07416414\n",
      "Iteration 30, loss = 0.07205068\n",
      "Iteration 31, loss = 0.07023346\n",
      "Iteration 32, loss = 0.06965688\n",
      "Iteration 33, loss = 0.06781974\n",
      "Iteration 34, loss = 0.06574960\n",
      "Iteration 35, loss = 0.06410801\n",
      "Iteration 36, loss = 0.06377814\n",
      "Iteration 37, loss = 0.06275171\n",
      "Iteration 38, loss = 0.06040437\n",
      "Iteration 39, loss = 0.05970472\n",
      "Iteration 40, loss = 0.05974403\n",
      "Iteration 41, loss = 0.05847945\n",
      "Iteration 42, loss = 0.05616877\n",
      "Iteration 43, loss = 0.05623548\n",
      "Iteration 44, loss = 0.05534542\n",
      "Iteration 45, loss = 0.05468963\n",
      "Iteration 46, loss = 0.05404690\n",
      "Iteration 47, loss = 0.05409210\n",
      "Iteration 48, loss = 0.05374392\n",
      "Iteration 49, loss = 0.05123322\n",
      "Iteration 50, loss = 0.05210727\n",
      "Iteration 51, loss = 0.04964791\n",
      "Iteration 52, loss = 0.05004423\n",
      "Iteration 53, loss = 0.04940226\n",
      "Iteration 54, loss = 0.04817692\n",
      "Iteration 55, loss = 0.04800785\n",
      "Iteration 56, loss = 0.04759017\n",
      "Iteration 57, loss = 0.04736742\n",
      "Iteration 58, loss = 0.04652995\n",
      "Iteration 59, loss = 0.04836212\n",
      "Iteration 60, loss = 0.04561903\n",
      "Iteration 61, loss = 0.04498975\n",
      "Iteration 62, loss = 0.04527102\n",
      "Iteration 63, loss = 0.04475023\n",
      "Iteration 64, loss = 0.04292378\n",
      "Iteration 65, loss = 0.04279393\n",
      "Iteration 66, loss = 0.04399331\n",
      "Iteration 67, loss = 0.04199936\n",
      "Iteration 68, loss = 0.04238465\n",
      "Iteration 69, loss = 0.04200100\n",
      "Iteration 70, loss = 0.04105749\n",
      "Iteration 71, loss = 0.04241084\n",
      "Iteration 72, loss = 0.04074487\n",
      "Iteration 73, loss = 0.03986202\n",
      "Iteration 74, loss = 0.04161642\n",
      "Iteration 75, loss = 0.04112469\n",
      "Iteration 76, loss = 0.04169475\n",
      "Iteration 77, loss = 0.03876561\n",
      "Iteration 78, loss = 0.04001380\n",
      "Iteration 79, loss = 0.03887021\n",
      "Iteration 80, loss = 0.03839580\n",
      "Iteration 81, loss = 0.03833841\n",
      "Iteration 82, loss = 0.03779856\n",
      "Iteration 83, loss = 0.03917616\n",
      "Iteration 84, loss = 0.03734818\n",
      "Iteration 85, loss = 0.03863037\n",
      "Iteration 86, loss = 0.03816627\n",
      "Iteration 87, loss = 0.03526823\n",
      "Iteration 88, loss = 0.03708719\n",
      "Iteration 89, loss = 0.03696573\n",
      "Iteration 90, loss = 0.03801300\n",
      "Iteration 91, loss = 0.03526996\n",
      "Iteration 92, loss = 0.03592328\n",
      "Iteration 93, loss = 0.03550895\n",
      "Iteration 94, loss = 0.03635505\n",
      "Iteration 95, loss = 0.03692918\n",
      "Iteration 96, loss = 0.03491642\n",
      "Iteration 97, loss = 0.03546942\n",
      "Iteration 98, loss = 0.03511079\n",
      "Iteration 99, loss = 0.03565220\n",
      "Iteration 100, loss = 0.03586012\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kileymack/opt/anaconda3/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 11.30522517\n",
      "Iteration 2, loss = 0.64824290\n",
      "Iteration 3, loss = 0.33222301\n",
      "Iteration 4, loss = 0.25001003\n",
      "Iteration 5, loss = 0.20885924\n",
      "Iteration 6, loss = 0.18571819\n",
      "Iteration 7, loss = 0.16572888\n",
      "Iteration 8, loss = 0.15166235\n",
      "Iteration 9, loss = 0.14082520\n",
      "Iteration 10, loss = 0.13186499\n",
      "Iteration 11, loss = 0.12484348\n",
      "Iteration 12, loss = 0.11807725\n",
      "Iteration 13, loss = 0.11180171\n",
      "Iteration 14, loss = 0.10692312\n",
      "Iteration 15, loss = 0.10156887\n",
      "Iteration 16, loss = 0.09835988\n",
      "Iteration 17, loss = 0.09496421\n",
      "Iteration 18, loss = 0.08927314\n",
      "Iteration 19, loss = 0.08783164\n",
      "Iteration 20, loss = 0.08511380\n",
      "Iteration 21, loss = 0.08244346\n",
      "Iteration 22, loss = 0.08097415\n",
      "Iteration 23, loss = 0.07784105\n",
      "Iteration 24, loss = 0.07721806\n",
      "Iteration 25, loss = 0.07390274\n",
      "Iteration 26, loss = 0.07296439\n",
      "Iteration 27, loss = 0.07139478\n",
      "Iteration 28, loss = 0.06998576\n",
      "Iteration 29, loss = 0.06868677\n",
      "Iteration 30, loss = 0.06775640\n",
      "Iteration 31, loss = 0.06623529\n",
      "Iteration 32, loss = 0.06536271\n",
      "Iteration 33, loss = 0.06461181\n",
      "Iteration 34, loss = 0.06275234\n",
      "Iteration 35, loss = 0.06256400\n",
      "Iteration 36, loss = 0.06067474\n",
      "Iteration 37, loss = 0.06168964\n",
      "Iteration 38, loss = 0.05935376\n",
      "Iteration 39, loss = 0.05896557\n",
      "Iteration 40, loss = 0.05888834\n",
      "Iteration 41, loss = 0.05778169\n",
      "Iteration 42, loss = 0.05652172\n",
      "Iteration 43, loss = 0.05708312\n",
      "Iteration 44, loss = 0.05646905\n",
      "Iteration 45, loss = 0.05573602\n",
      "Iteration 46, loss = 0.05513469\n",
      "Iteration 47, loss = 0.05435184\n",
      "Iteration 48, loss = 0.05378221\n",
      "Iteration 49, loss = 0.05358814\n",
      "Iteration 50, loss = 0.05394177\n",
      "Iteration 51, loss = 0.05234197\n",
      "Iteration 52, loss = 0.05203626\n",
      "Iteration 53, loss = 0.05222042\n",
      "Iteration 54, loss = 0.05091496\n",
      "Iteration 55, loss = 0.05117624\n",
      "Iteration 56, loss = 0.05104633\n",
      "Iteration 57, loss = 0.05046523\n",
      "Iteration 58, loss = 0.04967816\n",
      "Iteration 59, loss = 0.05026631\n",
      "Iteration 60, loss = 0.04950935\n",
      "Iteration 61, loss = 0.04901356\n",
      "Iteration 62, loss = 0.04878451\n",
      "Iteration 63, loss = 0.04856701\n",
      "Iteration 64, loss = 0.04822889\n",
      "Iteration 65, loss = 0.04777637\n",
      "Iteration 66, loss = 0.04759901\n",
      "Iteration 67, loss = 0.04731421\n",
      "Iteration 68, loss = 0.04695346\n",
      "Iteration 69, loss = 0.04713318\n",
      "Iteration 70, loss = 0.04705130\n",
      "Iteration 71, loss = 0.04652174\n",
      "Iteration 72, loss = 0.04686025\n",
      "Iteration 73, loss = 0.04609435\n",
      "Iteration 74, loss = 0.04620648\n",
      "Iteration 75, loss = 0.04575359\n",
      "Iteration 76, loss = 0.04529759\n",
      "Iteration 77, loss = 0.04586443\n",
      "Iteration 78, loss = 0.04470437\n",
      "Iteration 79, loss = 0.04539546\n",
      "Iteration 80, loss = 0.04505047\n",
      "Iteration 81, loss = 0.04434726\n",
      "Iteration 82, loss = 0.04487654\n",
      "Iteration 83, loss = 0.04413282\n",
      "Iteration 84, loss = 0.04434732\n",
      "Iteration 85, loss = 0.04448294\n",
      "Iteration 86, loss = 0.04450282\n",
      "Iteration 87, loss = 0.04364103\n",
      "Iteration 88, loss = 0.04335131\n",
      "Iteration 89, loss = 0.04323819\n",
      "Iteration 90, loss = 0.04397529\n",
      "Iteration 91, loss = 0.04336111\n",
      "Iteration 92, loss = 0.04350908\n",
      "Iteration 93, loss = 0.04275669\n",
      "Iteration 94, loss = 0.04281419\n",
      "Iteration 95, loss = 0.04278931\n",
      "Iteration 96, loss = 0.04276710\n",
      "Iteration 97, loss = 0.04232463\n",
      "Iteration 98, loss = 0.04202264\n",
      "Iteration 99, loss = 0.04243852\n",
      "Iteration 100, loss = 0.04236794\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kileymack/opt/anaconda3/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 2.13630571\n",
      "Iteration 2, loss = 0.41156979\n",
      "Iteration 3, loss = 0.33075910\n",
      "Iteration 4, loss = 0.29072542\n",
      "Iteration 5, loss = 0.26597929\n",
      "Iteration 6, loss = 0.24795240\n",
      "Iteration 7, loss = 0.23403847\n",
      "Iteration 8, loss = 0.22181738\n",
      "Iteration 9, loss = 0.21304840\n",
      "Iteration 10, loss = 0.20411347\n",
      "Iteration 11, loss = 0.19674550\n",
      "Iteration 12, loss = 0.19028324\n",
      "Iteration 13, loss = 0.18404055\n",
      "Iteration 14, loss = 0.18001161\n",
      "Iteration 15, loss = 0.17429764\n",
      "Iteration 16, loss = 0.17058679\n",
      "Iteration 17, loss = 0.16616087\n",
      "Iteration 18, loss = 0.16299671\n",
      "Iteration 19, loss = 0.15917220\n",
      "Iteration 20, loss = 0.15594705\n",
      "Iteration 21, loss = 0.15317331\n",
      "Iteration 22, loss = 0.14993045\n",
      "Iteration 23, loss = 0.14694550\n",
      "Iteration 24, loss = 0.14466809\n",
      "Iteration 25, loss = 0.14182157\n",
      "Iteration 26, loss = 0.13968836\n",
      "Iteration 27, loss = 0.13708334\n",
      "Iteration 28, loss = 0.13494955\n",
      "Iteration 29, loss = 0.13316464\n",
      "Iteration 30, loss = 0.13090123\n",
      "Iteration 31, loss = 0.12884835\n",
      "Iteration 32, loss = 0.12716581\n",
      "Iteration 33, loss = 0.12512489\n",
      "Iteration 34, loss = 0.12346389\n",
      "Iteration 35, loss = 0.12158024\n",
      "Iteration 36, loss = 0.12027427\n",
      "Iteration 37, loss = 0.11861775\n",
      "Iteration 38, loss = 0.11692579\n",
      "Iteration 39, loss = 0.11594661\n",
      "Iteration 40, loss = 0.11415388\n",
      "Iteration 41, loss = 0.11287719\n",
      "Iteration 42, loss = 0.11133956\n",
      "Iteration 43, loss = 0.11029028\n",
      "Iteration 44, loss = 0.10908740\n",
      "Iteration 45, loss = 0.10797075\n",
      "Iteration 46, loss = 0.10630308\n",
      "Iteration 47, loss = 0.10542086\n",
      "Iteration 48, loss = 0.10395810\n",
      "Iteration 49, loss = 0.10337632\n",
      "Iteration 50, loss = 0.10239999\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kileymack/opt/anaconda3/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (50) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 1.89526199\n",
      "Iteration 2, loss = 0.39435285\n",
      "Iteration 3, loss = 0.32893198\n",
      "Iteration 4, loss = 0.29247830\n",
      "Iteration 5, loss = 0.26725908\n",
      "Iteration 6, loss = 0.24901219\n",
      "Iteration 7, loss = 0.23456497\n",
      "Iteration 8, loss = 0.22222329\n",
      "Iteration 9, loss = 0.21219267\n",
      "Iteration 10, loss = 0.20352886\n",
      "Iteration 11, loss = 0.19636202\n",
      "Iteration 12, loss = 0.18942859\n",
      "Iteration 13, loss = 0.18329410\n",
      "Iteration 14, loss = 0.17810742\n",
      "Iteration 15, loss = 0.17288713\n",
      "Iteration 16, loss = 0.16864115\n",
      "Iteration 17, loss = 0.16447737\n",
      "Iteration 18, loss = 0.16078274\n",
      "Iteration 19, loss = 0.15698763\n",
      "Iteration 20, loss = 0.15376658\n",
      "Iteration 21, loss = 0.15056222\n",
      "Iteration 22, loss = 0.14757861\n",
      "Iteration 23, loss = 0.14476262\n",
      "Iteration 24, loss = 0.14187262\n",
      "Iteration 25, loss = 0.13965245\n",
      "Iteration 26, loss = 0.13752207\n",
      "Iteration 27, loss = 0.13525965\n",
      "Iteration 28, loss = 0.13294208\n",
      "Iteration 29, loss = 0.13089825\n",
      "Iteration 30, loss = 0.12902012\n",
      "Iteration 31, loss = 0.12700497\n",
      "Iteration 32, loss = 0.12528995\n",
      "Iteration 33, loss = 0.12347190\n",
      "Iteration 34, loss = 0.12195631\n",
      "Iteration 35, loss = 0.12014656\n",
      "Iteration 36, loss = 0.11867509\n",
      "Iteration 37, loss = 0.11734238\n",
      "Iteration 38, loss = 0.11582675\n",
      "Iteration 39, loss = 0.11410159\n",
      "Iteration 40, loss = 0.11293415\n",
      "Iteration 41, loss = 0.11165262\n",
      "Iteration 42, loss = 0.11042602\n",
      "Iteration 43, loss = 0.10914110\n",
      "Iteration 44, loss = 0.10781827\n",
      "Iteration 45, loss = 0.10673274\n",
      "Iteration 46, loss = 0.10570576\n",
      "Iteration 47, loss = 0.10461887\n",
      "Iteration 48, loss = 0.10368361\n",
      "Iteration 49, loss = 0.10243229\n",
      "Iteration 50, loss = 0.10137271\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kileymack/opt/anaconda3/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (50) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 6.94845957\n",
      "Iteration 2, loss = 1.10447089\n",
      "Iteration 3, loss = 0.51319165\n",
      "Iteration 4, loss = 0.33995558\n",
      "Iteration 5, loss = 0.26653577\n",
      "Iteration 6, loss = 0.22692386\n",
      "Iteration 7, loss = 0.20505703\n",
      "Iteration 8, loss = 0.18052043\n",
      "Iteration 9, loss = 0.17052450\n",
      "Iteration 10, loss = 0.15569780\n",
      "Iteration 11, loss = 0.14599026\n",
      "Iteration 12, loss = 0.13678452\n",
      "Iteration 13, loss = 0.13306636\n",
      "Iteration 14, loss = 0.12921618\n",
      "Iteration 15, loss = 0.12145563\n",
      "Iteration 16, loss = 0.11716851\n",
      "Iteration 17, loss = 0.11175489\n",
      "Iteration 18, loss = 0.10729911\n",
      "Iteration 19, loss = 0.10579428\n",
      "Iteration 20, loss = 0.10294765\n",
      "Iteration 21, loss = 0.09915991\n",
      "Iteration 22, loss = 0.09503529\n",
      "Iteration 23, loss = 0.09265239\n",
      "Iteration 24, loss = 0.09243001\n",
      "Iteration 25, loss = 0.08791669\n",
      "Iteration 26, loss = 0.09030127\n",
      "Iteration 27, loss = 0.08411538\n",
      "Iteration 28, loss = 0.08239041\n",
      "Iteration 29, loss = 0.08028024\n",
      "Iteration 30, loss = 0.08206261\n",
      "Iteration 31, loss = 0.07893573\n",
      "Iteration 32, loss = 0.07717577\n",
      "Iteration 33, loss = 0.07664721\n",
      "Iteration 34, loss = 0.07542393\n",
      "Iteration 35, loss = 0.07335992\n",
      "Iteration 36, loss = 0.07238967\n",
      "Iteration 37, loss = 0.07051630\n",
      "Iteration 38, loss = 0.07103197\n",
      "Iteration 39, loss = 0.07001866\n",
      "Iteration 40, loss = 0.06698506\n",
      "Iteration 41, loss = 0.06767951\n",
      "Iteration 42, loss = 0.06701841\n",
      "Iteration 43, loss = 0.06551638\n",
      "Iteration 44, loss = 0.06438078\n",
      "Iteration 45, loss = 0.06505698\n",
      "Iteration 46, loss = 0.06393485\n",
      "Iteration 47, loss = 0.06177700\n",
      "Iteration 48, loss = 0.06233264\n",
      "Iteration 49, loss = 0.06294979\n",
      "Iteration 50, loss = 0.05861734\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kileymack/opt/anaconda3/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (50) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 11.64897593\n",
      "Iteration 2, loss = 0.81975543\n",
      "Iteration 3, loss = 0.38087477\n",
      "Iteration 4, loss = 0.27909386\n",
      "Iteration 5, loss = 0.22994017\n",
      "Iteration 6, loss = 0.20062163\n",
      "Iteration 7, loss = 0.18518249\n",
      "Iteration 8, loss = 0.16889443\n",
      "Iteration 9, loss = 0.15973776\n",
      "Iteration 10, loss = 0.15033613\n",
      "Iteration 11, loss = 0.14032831\n",
      "Iteration 12, loss = 0.13504493\n",
      "Iteration 13, loss = 0.12804213\n",
      "Iteration 14, loss = 0.12272501\n",
      "Iteration 15, loss = 0.11992817\n",
      "Iteration 16, loss = 0.11403275\n",
      "Iteration 17, loss = 0.11117440\n",
      "Iteration 18, loss = 0.10694424\n",
      "Iteration 19, loss = 0.10482551\n",
      "Iteration 20, loss = 0.10223522\n",
      "Iteration 21, loss = 0.10002135\n",
      "Iteration 22, loss = 0.09620917\n",
      "Iteration 23, loss = 0.09501419\n",
      "Iteration 24, loss = 0.09227021\n",
      "Iteration 25, loss = 0.09208538\n",
      "Iteration 26, loss = 0.08927163\n",
      "Iteration 27, loss = 0.08837318\n",
      "Iteration 28, loss = 0.08693456\n",
      "Iteration 29, loss = 0.08562405\n",
      "Iteration 30, loss = 0.08389124\n",
      "Iteration 31, loss = 0.08213392\n",
      "Iteration 32, loss = 0.08216534\n",
      "Iteration 33, loss = 0.08155407\n",
      "Iteration 34, loss = 0.07905745\n",
      "Iteration 35, loss = 0.07941896\n",
      "Iteration 36, loss = 0.07683696\n",
      "Iteration 37, loss = 0.07720381\n",
      "Iteration 38, loss = 0.07646303\n",
      "Iteration 39, loss = 0.07497041\n",
      "Iteration 40, loss = 0.07369194\n",
      "Iteration 41, loss = 0.07360791\n",
      "Iteration 42, loss = 0.07258705\n",
      "Iteration 43, loss = 0.07226169\n",
      "Iteration 44, loss = 0.07096933\n",
      "Iteration 45, loss = 0.07106527\n",
      "Iteration 46, loss = 0.07002649\n",
      "Iteration 47, loss = 0.06965836\n",
      "Iteration 48, loss = 0.06853523\n",
      "Iteration 49, loss = 0.06916452\n",
      "Iteration 50, loss = 0.06756652\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kileymack/opt/anaconda3/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (50) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 2.13630571\n",
      "Iteration 2, loss = 0.41156979\n",
      "Iteration 3, loss = 0.33075910\n",
      "Iteration 4, loss = 0.29072542\n",
      "Iteration 5, loss = 0.26597929\n",
      "Iteration 6, loss = 0.24795240\n",
      "Iteration 7, loss = 0.23403847\n",
      "Iteration 8, loss = 0.22181738\n",
      "Iteration 9, loss = 0.21304840\n",
      "Iteration 10, loss = 0.20411347\n",
      "Iteration 11, loss = 0.19674550\n",
      "Iteration 12, loss = 0.19028324\n",
      "Iteration 13, loss = 0.18404055\n",
      "Iteration 14, loss = 0.18001161\n",
      "Iteration 15, loss = 0.17429764\n",
      "Iteration 16, loss = 0.17058679\n",
      "Iteration 17, loss = 0.16616087\n",
      "Iteration 18, loss = 0.16299671\n",
      "Iteration 19, loss = 0.15917220\n",
      "Iteration 20, loss = 0.15594705\n",
      "Iteration 21, loss = 0.15317331\n",
      "Iteration 22, loss = 0.14993045\n",
      "Iteration 23, loss = 0.14694550\n",
      "Iteration 24, loss = 0.14466809\n",
      "Iteration 25, loss = 0.14182157\n",
      "Iteration 26, loss = 0.13968836\n",
      "Iteration 27, loss = 0.13708334\n",
      "Iteration 28, loss = 0.13494955\n",
      "Iteration 29, loss = 0.13316464\n",
      "Iteration 30, loss = 0.13090123\n",
      "Iteration 31, loss = 0.12884835\n",
      "Iteration 32, loss = 0.12716581\n",
      "Iteration 33, loss = 0.12512489\n",
      "Iteration 34, loss = 0.12346389\n",
      "Iteration 35, loss = 0.12158024\n",
      "Iteration 36, loss = 0.12027427\n",
      "Iteration 37, loss = 0.11861775\n",
      "Iteration 38, loss = 0.11692579\n",
      "Iteration 39, loss = 0.11594661\n",
      "Iteration 40, loss = 0.11415388\n",
      "Iteration 41, loss = 0.11287719\n",
      "Iteration 42, loss = 0.11133956\n",
      "Iteration 43, loss = 0.11029028\n",
      "Iteration 44, loss = 0.10908740\n",
      "Iteration 45, loss = 0.10797075\n",
      "Iteration 46, loss = 0.10630308\n",
      "Iteration 47, loss = 0.10542086\n",
      "Iteration 48, loss = 0.10395810\n",
      "Iteration 49, loss = 0.10337632\n",
      "Iteration 50, loss = 0.10239999\n",
      "Iteration 51, loss = 0.10115233\n",
      "Iteration 52, loss = 0.09993617\n",
      "Iteration 53, loss = 0.09911479\n",
      "Iteration 54, loss = 0.09807130\n",
      "Iteration 55, loss = 0.09717473\n",
      "Iteration 56, loss = 0.09646534\n",
      "Iteration 57, loss = 0.09554677\n",
      "Iteration 58, loss = 0.09466420\n",
      "Iteration 59, loss = 0.09352849\n",
      "Iteration 60, loss = 0.09293403\n",
      "Iteration 61, loss = 0.09219228\n",
      "Iteration 62, loss = 0.09166226\n",
      "Iteration 63, loss = 0.09071278\n",
      "Iteration 64, loss = 0.08982822\n",
      "Iteration 65, loss = 0.08913034\n",
      "Iteration 66, loss = 0.08829482\n",
      "Iteration 67, loss = 0.08745370\n",
      "Iteration 68, loss = 0.08708800\n",
      "Iteration 69, loss = 0.08657307\n",
      "Iteration 70, loss = 0.08581444\n",
      "Iteration 71, loss = 0.08502984\n",
      "Iteration 72, loss = 0.08451935\n",
      "Iteration 73, loss = 0.08408629\n",
      "Iteration 74, loss = 0.08345992\n",
      "Iteration 75, loss = 0.08245328\n",
      "Iteration 76, loss = 0.08186417\n",
      "Iteration 77, loss = 0.08125498\n",
      "Iteration 78, loss = 0.08108281\n",
      "Iteration 79, loss = 0.08020123\n",
      "Iteration 80, loss = 0.07964847\n",
      "Iteration 81, loss = 0.07921964\n",
      "Iteration 82, loss = 0.07867097\n",
      "Iteration 83, loss = 0.07814144\n",
      "Iteration 84, loss = 0.07763240\n",
      "Iteration 85, loss = 0.07731087\n",
      "Iteration 86, loss = 0.07664947\n",
      "Iteration 87, loss = 0.07595451\n",
      "Iteration 88, loss = 0.07566664\n",
      "Iteration 89, loss = 0.07530332\n",
      "Iteration 90, loss = 0.07451665\n",
      "Iteration 91, loss = 0.07445089\n",
      "Iteration 92, loss = 0.07359830\n",
      "Iteration 93, loss = 0.07353869\n",
      "Iteration 94, loss = 0.07297249\n",
      "Iteration 95, loss = 0.07210413\n",
      "Iteration 96, loss = 0.07200217\n",
      "Iteration 97, loss = 0.07183395\n",
      "Iteration 98, loss = 0.07104217\n",
      "Iteration 99, loss = 0.07063702\n",
      "Iteration 100, loss = 0.07029157\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kileymack/opt/anaconda3/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 1.89526199\n",
      "Iteration 2, loss = 0.39435285\n",
      "Iteration 3, loss = 0.32893198\n",
      "Iteration 4, loss = 0.29247830\n",
      "Iteration 5, loss = 0.26725908\n",
      "Iteration 6, loss = 0.24901219\n",
      "Iteration 7, loss = 0.23456497\n",
      "Iteration 8, loss = 0.22222329\n",
      "Iteration 9, loss = 0.21219267\n",
      "Iteration 10, loss = 0.20352886\n",
      "Iteration 11, loss = 0.19636202\n",
      "Iteration 12, loss = 0.18942859\n",
      "Iteration 13, loss = 0.18329410\n",
      "Iteration 14, loss = 0.17810742\n",
      "Iteration 15, loss = 0.17288713\n",
      "Iteration 16, loss = 0.16864115\n",
      "Iteration 17, loss = 0.16447737\n",
      "Iteration 18, loss = 0.16078274\n",
      "Iteration 19, loss = 0.15698763\n",
      "Iteration 20, loss = 0.15376658\n",
      "Iteration 21, loss = 0.15056222\n",
      "Iteration 22, loss = 0.14757861\n",
      "Iteration 23, loss = 0.14476262\n",
      "Iteration 24, loss = 0.14187262\n",
      "Iteration 25, loss = 0.13965245\n",
      "Iteration 26, loss = 0.13752207\n",
      "Iteration 27, loss = 0.13525965\n",
      "Iteration 28, loss = 0.13294208\n",
      "Iteration 29, loss = 0.13089825\n",
      "Iteration 30, loss = 0.12902012\n",
      "Iteration 31, loss = 0.12700497\n",
      "Iteration 32, loss = 0.12528995\n",
      "Iteration 33, loss = 0.12347190\n",
      "Iteration 34, loss = 0.12195631\n",
      "Iteration 35, loss = 0.12014656\n",
      "Iteration 36, loss = 0.11867509\n",
      "Iteration 37, loss = 0.11734238\n",
      "Iteration 38, loss = 0.11582675\n",
      "Iteration 39, loss = 0.11410159\n",
      "Iteration 40, loss = 0.11293415\n",
      "Iteration 41, loss = 0.11165262\n",
      "Iteration 42, loss = 0.11042602\n",
      "Iteration 43, loss = 0.10914110\n",
      "Iteration 44, loss = 0.10781827\n",
      "Iteration 45, loss = 0.10673274\n",
      "Iteration 46, loss = 0.10570576\n",
      "Iteration 47, loss = 0.10461887\n",
      "Iteration 48, loss = 0.10368361\n",
      "Iteration 49, loss = 0.10243229\n",
      "Iteration 50, loss = 0.10137271\n",
      "Iteration 51, loss = 0.10056250\n",
      "Iteration 52, loss = 0.09952156\n",
      "Iteration 53, loss = 0.09859977\n",
      "Iteration 54, loss = 0.09759321\n",
      "Iteration 55, loss = 0.09676584\n",
      "Iteration 56, loss = 0.09621073\n",
      "Iteration 57, loss = 0.09516201\n",
      "Iteration 58, loss = 0.09428851\n",
      "Iteration 59, loss = 0.09352458\n",
      "Iteration 60, loss = 0.09274089\n",
      "Iteration 61, loss = 0.09183282\n",
      "Iteration 62, loss = 0.09140756\n",
      "Iteration 63, loss = 0.09037527\n",
      "Iteration 64, loss = 0.08967908\n",
      "Iteration 65, loss = 0.08899585\n",
      "Iteration 66, loss = 0.08821431\n",
      "Iteration 67, loss = 0.08772859\n",
      "Iteration 68, loss = 0.08704016\n",
      "Iteration 69, loss = 0.08636921\n",
      "Iteration 70, loss = 0.08563291\n",
      "Iteration 71, loss = 0.08512679\n",
      "Iteration 72, loss = 0.08460241\n",
      "Iteration 73, loss = 0.08399008\n",
      "Iteration 74, loss = 0.08325671\n",
      "Iteration 75, loss = 0.08270172\n",
      "Iteration 76, loss = 0.08209737\n",
      "Iteration 77, loss = 0.08143142\n",
      "Iteration 78, loss = 0.08103059\n",
      "Iteration 79, loss = 0.08063068\n",
      "Iteration 80, loss = 0.08005237\n",
      "Iteration 81, loss = 0.07947973\n",
      "Iteration 82, loss = 0.07898445\n",
      "Iteration 83, loss = 0.07828778\n",
      "Iteration 84, loss = 0.07797880\n",
      "Iteration 85, loss = 0.07730637\n",
      "Iteration 86, loss = 0.07705752\n",
      "Iteration 87, loss = 0.07650916\n",
      "Iteration 88, loss = 0.07603509\n",
      "Iteration 89, loss = 0.07557501\n",
      "Iteration 90, loss = 0.07503735\n",
      "Iteration 91, loss = 0.07474393\n",
      "Iteration 92, loss = 0.07421474\n",
      "Iteration 93, loss = 0.07374872\n",
      "Iteration 94, loss = 0.07327456\n",
      "Iteration 95, loss = 0.07310306\n",
      "Iteration 96, loss = 0.07258096\n",
      "Iteration 97, loss = 0.07211903\n",
      "Iteration 98, loss = 0.07189192\n",
      "Iteration 99, loss = 0.07127963\n",
      "Iteration 100, loss = 0.07107207\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kileymack/opt/anaconda3/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 6.94845957\n",
      "Iteration 2, loss = 1.10447089\n",
      "Iteration 3, loss = 0.51319165\n",
      "Iteration 4, loss = 0.33995558\n",
      "Iteration 5, loss = 0.26653577\n",
      "Iteration 6, loss = 0.22692386\n",
      "Iteration 7, loss = 0.20505703\n",
      "Iteration 8, loss = 0.18052043\n",
      "Iteration 9, loss = 0.17052450\n",
      "Iteration 10, loss = 0.15569780\n",
      "Iteration 11, loss = 0.14599026\n",
      "Iteration 12, loss = 0.13678452\n",
      "Iteration 13, loss = 0.13306636\n",
      "Iteration 14, loss = 0.12921618\n",
      "Iteration 15, loss = 0.12145563\n",
      "Iteration 16, loss = 0.11716851\n",
      "Iteration 17, loss = 0.11175489\n",
      "Iteration 18, loss = 0.10729911\n",
      "Iteration 19, loss = 0.10579428\n",
      "Iteration 20, loss = 0.10294765\n",
      "Iteration 21, loss = 0.09915991\n",
      "Iteration 22, loss = 0.09503529\n",
      "Iteration 23, loss = 0.09265239\n",
      "Iteration 24, loss = 0.09243001\n",
      "Iteration 25, loss = 0.08791669\n",
      "Iteration 26, loss = 0.09030127\n",
      "Iteration 27, loss = 0.08411538\n",
      "Iteration 28, loss = 0.08239041\n",
      "Iteration 29, loss = 0.08028024\n",
      "Iteration 30, loss = 0.08206261\n",
      "Iteration 31, loss = 0.07893573\n",
      "Iteration 32, loss = 0.07717577\n",
      "Iteration 33, loss = 0.07664721\n",
      "Iteration 34, loss = 0.07542393\n",
      "Iteration 35, loss = 0.07335992\n",
      "Iteration 36, loss = 0.07238967\n",
      "Iteration 37, loss = 0.07051630\n",
      "Iteration 38, loss = 0.07103197\n",
      "Iteration 39, loss = 0.07001866\n",
      "Iteration 40, loss = 0.06698506\n",
      "Iteration 41, loss = 0.06767951\n",
      "Iteration 42, loss = 0.06701841\n",
      "Iteration 43, loss = 0.06551638\n",
      "Iteration 44, loss = 0.06438078\n",
      "Iteration 45, loss = 0.06505698\n",
      "Iteration 46, loss = 0.06393485\n",
      "Iteration 47, loss = 0.06177700\n",
      "Iteration 48, loss = 0.06233264\n",
      "Iteration 49, loss = 0.06294979\n",
      "Iteration 50, loss = 0.05861734\n",
      "Iteration 51, loss = 0.06133379\n",
      "Iteration 52, loss = 0.05796527\n",
      "Iteration 53, loss = 0.05835490\n",
      "Iteration 54, loss = 0.05854548\n",
      "Iteration 55, loss = 0.05896705\n",
      "Iteration 56, loss = 0.05686513\n",
      "Iteration 57, loss = 0.05606124\n",
      "Iteration 58, loss = 0.05649298\n",
      "Iteration 59, loss = 0.05580704\n",
      "Iteration 60, loss = 0.05393458\n",
      "Iteration 61, loss = 0.05595349\n",
      "Iteration 62, loss = 0.05375048\n",
      "Iteration 63, loss = 0.05323491\n",
      "Iteration 64, loss = 0.05477064\n",
      "Iteration 65, loss = 0.05214619\n",
      "Iteration 66, loss = 0.05333139\n",
      "Iteration 67, loss = 0.05249609\n",
      "Iteration 68, loss = 0.05226025\n",
      "Iteration 69, loss = 0.05106021\n",
      "Iteration 70, loss = 0.05127109\n",
      "Iteration 71, loss = 0.05121124\n",
      "Iteration 72, loss = 0.05059895\n",
      "Iteration 73, loss = 0.05027903\n",
      "Iteration 74, loss = 0.04987334\n",
      "Iteration 75, loss = 0.04979254\n",
      "Iteration 76, loss = 0.04835137\n",
      "Iteration 77, loss = 0.04888134\n",
      "Iteration 78, loss = 0.04854337\n",
      "Iteration 79, loss = 0.04850627\n",
      "Iteration 80, loss = 0.04776609\n",
      "Iteration 81, loss = 0.04798728\n",
      "Iteration 82, loss = 0.04707550\n",
      "Iteration 83, loss = 0.04691313\n",
      "Iteration 84, loss = 0.04594691\n",
      "Iteration 85, loss = 0.04575911\n",
      "Iteration 86, loss = 0.04653068\n",
      "Iteration 87, loss = 0.04637194\n",
      "Iteration 88, loss = 0.04562378\n",
      "Iteration 89, loss = 0.04589552\n",
      "Iteration 90, loss = 0.04478917\n",
      "Iteration 91, loss = 0.04535375\n",
      "Iteration 92, loss = 0.04511938\n",
      "Iteration 93, loss = 0.04430479\n",
      "Iteration 94, loss = 0.04555329\n",
      "Iteration 95, loss = 0.04435827\n",
      "Iteration 96, loss = 0.04364678\n",
      "Iteration 97, loss = 0.04360709\n",
      "Iteration 98, loss = 0.04362255\n",
      "Iteration 99, loss = 0.04368208\n",
      "Iteration 100, loss = 0.04320960\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kileymack/opt/anaconda3/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 11.64897593\n",
      "Iteration 2, loss = 0.81975543\n",
      "Iteration 3, loss = 0.38087477\n",
      "Iteration 4, loss = 0.27909386\n",
      "Iteration 5, loss = 0.22994017\n",
      "Iteration 6, loss = 0.20062163\n",
      "Iteration 7, loss = 0.18518249\n",
      "Iteration 8, loss = 0.16889443\n",
      "Iteration 9, loss = 0.15973776\n",
      "Iteration 10, loss = 0.15033613\n",
      "Iteration 11, loss = 0.14032831\n",
      "Iteration 12, loss = 0.13504493\n",
      "Iteration 13, loss = 0.12804213\n",
      "Iteration 14, loss = 0.12272501\n",
      "Iteration 15, loss = 0.11992817\n",
      "Iteration 16, loss = 0.11403275\n",
      "Iteration 17, loss = 0.11117440\n",
      "Iteration 18, loss = 0.10694424\n",
      "Iteration 19, loss = 0.10482551\n",
      "Iteration 20, loss = 0.10223522\n",
      "Iteration 21, loss = 0.10002135\n",
      "Iteration 22, loss = 0.09620917\n",
      "Iteration 23, loss = 0.09501419\n",
      "Iteration 24, loss = 0.09227021\n",
      "Iteration 25, loss = 0.09208538\n",
      "Iteration 26, loss = 0.08927163\n",
      "Iteration 27, loss = 0.08837318\n",
      "Iteration 28, loss = 0.08693456\n",
      "Iteration 29, loss = 0.08562405\n",
      "Iteration 30, loss = 0.08389124\n",
      "Iteration 31, loss = 0.08213392\n",
      "Iteration 32, loss = 0.08216534\n",
      "Iteration 33, loss = 0.08155407\n",
      "Iteration 34, loss = 0.07905745\n",
      "Iteration 35, loss = 0.07941896\n",
      "Iteration 36, loss = 0.07683696\n",
      "Iteration 37, loss = 0.07720381\n",
      "Iteration 38, loss = 0.07646303\n",
      "Iteration 39, loss = 0.07497041\n",
      "Iteration 40, loss = 0.07369194\n",
      "Iteration 41, loss = 0.07360791\n",
      "Iteration 42, loss = 0.07258705\n",
      "Iteration 43, loss = 0.07226169\n",
      "Iteration 44, loss = 0.07096933\n",
      "Iteration 45, loss = 0.07106527\n",
      "Iteration 46, loss = 0.07002649\n",
      "Iteration 47, loss = 0.06965836\n",
      "Iteration 48, loss = 0.06853523\n",
      "Iteration 49, loss = 0.06916452\n",
      "Iteration 50, loss = 0.06756652\n",
      "Iteration 51, loss = 0.06771443\n",
      "Iteration 52, loss = 0.06756843\n",
      "Iteration 53, loss = 0.06702429\n",
      "Iteration 54, loss = 0.06564871\n",
      "Iteration 55, loss = 0.06571685\n",
      "Iteration 56, loss = 0.06490287\n",
      "Iteration 57, loss = 0.06493541\n",
      "Iteration 58, loss = 0.06463575\n",
      "Iteration 59, loss = 0.06398117\n",
      "Iteration 60, loss = 0.06398324\n",
      "Iteration 61, loss = 0.06317435\n",
      "Iteration 62, loss = 0.06325149\n",
      "Iteration 63, loss = 0.06290125\n",
      "Iteration 64, loss = 0.06191544\n",
      "Iteration 65, loss = 0.06162727\n",
      "Iteration 66, loss = 0.06191486\n",
      "Iteration 67, loss = 0.06118495\n",
      "Iteration 68, loss = 0.06126265\n",
      "Iteration 69, loss = 0.06057063\n",
      "Iteration 70, loss = 0.06039055\n",
      "Iteration 71, loss = 0.06032236\n",
      "Iteration 72, loss = 0.05982024\n",
      "Iteration 73, loss = 0.06023420\n",
      "Iteration 74, loss = 0.05886149\n",
      "Iteration 75, loss = 0.05935031\n",
      "Iteration 76, loss = 0.05890497\n",
      "Iteration 77, loss = 0.05848685\n",
      "Iteration 78, loss = 0.05887677\n",
      "Iteration 79, loss = 0.05804514\n",
      "Iteration 80, loss = 0.05815629\n",
      "Iteration 81, loss = 0.05838694\n",
      "Iteration 82, loss = 0.05631981\n",
      "Iteration 83, loss = 0.05770404\n",
      "Iteration 84, loss = 0.05730358\n",
      "Iteration 85, loss = 0.05744220\n",
      "Iteration 86, loss = 0.05671268\n",
      "Iteration 87, loss = 0.05661542\n",
      "Iteration 88, loss = 0.05687931\n",
      "Iteration 89, loss = 0.05609705\n",
      "Iteration 90, loss = 0.05582466\n",
      "Iteration 91, loss = 0.05556953\n",
      "Iteration 92, loss = 0.05630414\n",
      "Iteration 93, loss = 0.05541134\n",
      "Iteration 94, loss = 0.05544071\n",
      "Iteration 95, loss = 0.05536456\n",
      "Iteration 96, loss = 0.05467858\n",
      "Iteration 97, loss = 0.05502075\n",
      "Iteration 98, loss = 0.05471433\n",
      "Iteration 99, loss = 0.05411578\n",
      "Iteration 100, loss = 0.05442966\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kileymack/opt/anaconda3/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 2.13630571\n",
      "Iteration 2, loss = 0.41156979\n",
      "Iteration 3, loss = 0.33075910\n",
      "Iteration 4, loss = 0.29072542\n",
      "Iteration 5, loss = 0.26597929\n",
      "Iteration 6, loss = 0.24795240\n",
      "Iteration 7, loss = 0.23403847\n",
      "Iteration 8, loss = 0.22181738\n",
      "Iteration 9, loss = 0.21304840\n",
      "Iteration 10, loss = 0.20411347\n",
      "Iteration 11, loss = 0.19674550\n",
      "Iteration 12, loss = 0.19028324\n",
      "Iteration 13, loss = 0.18404055\n",
      "Iteration 14, loss = 0.18001161\n",
      "Iteration 15, loss = 0.17429764\n",
      "Iteration 16, loss = 0.17058679\n",
      "Iteration 17, loss = 0.16616087\n",
      "Iteration 18, loss = 0.16299671\n",
      "Iteration 19, loss = 0.15917220\n",
      "Iteration 20, loss = 0.15594705\n",
      "Iteration 21, loss = 0.15317331\n",
      "Iteration 22, loss = 0.14993045\n",
      "Iteration 23, loss = 0.14694550\n",
      "Iteration 24, loss = 0.14466809\n",
      "Iteration 25, loss = 0.14182157\n",
      "Iteration 26, loss = 0.13968836\n",
      "Iteration 27, loss = 0.13708334\n",
      "Iteration 28, loss = 0.13494955\n",
      "Iteration 29, loss = 0.13316464\n",
      "Iteration 30, loss = 0.13090123\n",
      "Iteration 31, loss = 0.12884835\n",
      "Iteration 32, loss = 0.12716581\n",
      "Iteration 33, loss = 0.12512489\n",
      "Iteration 34, loss = 0.12346389\n",
      "Iteration 35, loss = 0.12158024\n",
      "Iteration 36, loss = 0.12027427\n",
      "Iteration 37, loss = 0.11861775\n",
      "Iteration 38, loss = 0.11692579\n",
      "Iteration 39, loss = 0.11594661\n",
      "Iteration 40, loss = 0.11415388\n",
      "Iteration 41, loss = 0.11287719\n",
      "Iteration 42, loss = 0.11133956\n",
      "Iteration 43, loss = 0.11029028\n",
      "Iteration 44, loss = 0.10908740\n",
      "Iteration 45, loss = 0.10797075\n",
      "Iteration 46, loss = 0.10630308\n",
      "Iteration 47, loss = 0.10542086\n",
      "Iteration 48, loss = 0.10395810\n",
      "Iteration 49, loss = 0.10337632\n",
      "Iteration 50, loss = 0.10239999\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kileymack/opt/anaconda3/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (50) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 1.89526199\n",
      "Iteration 2, loss = 0.39435285\n",
      "Iteration 3, loss = 0.32893198\n",
      "Iteration 4, loss = 0.29247830\n",
      "Iteration 5, loss = 0.26725908\n",
      "Iteration 6, loss = 0.24901219\n",
      "Iteration 7, loss = 0.23456497\n",
      "Iteration 8, loss = 0.22222329\n",
      "Iteration 9, loss = 0.21219267\n",
      "Iteration 10, loss = 0.20352886\n",
      "Iteration 11, loss = 0.19636202\n",
      "Iteration 12, loss = 0.18942859\n",
      "Iteration 13, loss = 0.18329410\n",
      "Iteration 14, loss = 0.17810742\n",
      "Iteration 15, loss = 0.17288713\n",
      "Iteration 16, loss = 0.16864115\n",
      "Iteration 17, loss = 0.16447737\n",
      "Iteration 18, loss = 0.16078274\n",
      "Iteration 19, loss = 0.15698763\n",
      "Iteration 20, loss = 0.15376658\n",
      "Iteration 21, loss = 0.15056222\n",
      "Iteration 22, loss = 0.14757861\n",
      "Iteration 23, loss = 0.14476262\n",
      "Iteration 24, loss = 0.14187262\n",
      "Iteration 25, loss = 0.13965245\n",
      "Iteration 26, loss = 0.13752207\n",
      "Iteration 27, loss = 0.13525965\n",
      "Iteration 28, loss = 0.13294208\n",
      "Iteration 29, loss = 0.13089825\n",
      "Iteration 30, loss = 0.12902012\n",
      "Iteration 31, loss = 0.12700497\n",
      "Iteration 32, loss = 0.12528995\n",
      "Iteration 33, loss = 0.12347190\n",
      "Iteration 34, loss = 0.12195631\n",
      "Iteration 35, loss = 0.12014656\n",
      "Iteration 36, loss = 0.11867509\n",
      "Iteration 37, loss = 0.11734238\n",
      "Iteration 38, loss = 0.11582675\n",
      "Iteration 39, loss = 0.11410159\n",
      "Iteration 40, loss = 0.11293415\n",
      "Iteration 41, loss = 0.11165262\n",
      "Iteration 42, loss = 0.11042602\n",
      "Iteration 43, loss = 0.10914110\n",
      "Iteration 44, loss = 0.10781827\n",
      "Iteration 45, loss = 0.10673274\n",
      "Iteration 46, loss = 0.10570576\n",
      "Iteration 47, loss = 0.10461887\n",
      "Iteration 48, loss = 0.10368361\n",
      "Iteration 49, loss = 0.10243229\n",
      "Iteration 50, loss = 0.10137271\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kileymack/opt/anaconda3/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (50) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 6.94845957\n",
      "Iteration 2, loss = 1.10447089\n",
      "Iteration 3, loss = 0.51319165\n",
      "Iteration 4, loss = 0.33995558\n",
      "Iteration 5, loss = 0.26653577\n",
      "Iteration 6, loss = 0.22692386\n",
      "Iteration 7, loss = 0.20505703\n",
      "Iteration 8, loss = 0.18052043\n",
      "Iteration 9, loss = 0.17052450\n",
      "Iteration 10, loss = 0.15569780\n",
      "Iteration 11, loss = 0.14599026\n",
      "Iteration 12, loss = 0.13678452\n",
      "Iteration 13, loss = 0.13306636\n",
      "Iteration 14, loss = 0.12921618\n",
      "Iteration 15, loss = 0.12145563\n",
      "Iteration 16, loss = 0.11716851\n",
      "Iteration 17, loss = 0.11175489\n",
      "Iteration 18, loss = 0.10729911\n",
      "Iteration 19, loss = 0.10579428\n",
      "Iteration 20, loss = 0.10294765\n",
      "Iteration 21, loss = 0.09915991\n",
      "Iteration 22, loss = 0.09503529\n",
      "Iteration 23, loss = 0.09265239\n",
      "Iteration 24, loss = 0.09243001\n",
      "Iteration 25, loss = 0.08791669\n",
      "Iteration 26, loss = 0.09030127\n",
      "Iteration 27, loss = 0.08411538\n",
      "Iteration 28, loss = 0.08239041\n",
      "Iteration 29, loss = 0.08028024\n",
      "Iteration 30, loss = 0.08206261\n",
      "Iteration 31, loss = 0.07893573\n",
      "Iteration 32, loss = 0.07717577\n",
      "Iteration 33, loss = 0.07664721\n",
      "Iteration 34, loss = 0.07542393\n",
      "Iteration 35, loss = 0.07335992\n",
      "Iteration 36, loss = 0.07238967\n",
      "Iteration 37, loss = 0.07051630\n",
      "Iteration 38, loss = 0.07103197\n",
      "Iteration 39, loss = 0.07001866\n",
      "Iteration 40, loss = 0.06698506\n",
      "Iteration 41, loss = 0.06767951\n",
      "Iteration 42, loss = 0.06701841\n",
      "Iteration 43, loss = 0.06551638\n",
      "Iteration 44, loss = 0.06438078\n",
      "Iteration 45, loss = 0.06505698\n",
      "Iteration 46, loss = 0.06393485\n",
      "Iteration 47, loss = 0.06177700\n",
      "Iteration 48, loss = 0.06233264\n",
      "Iteration 49, loss = 0.06294979\n",
      "Iteration 50, loss = 0.05861734\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kileymack/opt/anaconda3/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (50) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 11.64897593\n",
      "Iteration 2, loss = 0.81975543\n",
      "Iteration 3, loss = 0.38087477\n",
      "Iteration 4, loss = 0.27909386\n",
      "Iteration 5, loss = 0.22994017\n",
      "Iteration 6, loss = 0.20062163\n",
      "Iteration 7, loss = 0.18518249\n",
      "Iteration 8, loss = 0.16889443\n",
      "Iteration 9, loss = 0.15973776\n",
      "Iteration 10, loss = 0.15033613\n",
      "Iteration 11, loss = 0.14032831\n",
      "Iteration 12, loss = 0.13504493\n",
      "Iteration 13, loss = 0.12804213\n",
      "Iteration 14, loss = 0.12272501\n",
      "Iteration 15, loss = 0.11992817\n",
      "Iteration 16, loss = 0.11403275\n",
      "Iteration 17, loss = 0.11117440\n",
      "Iteration 18, loss = 0.10694424\n",
      "Iteration 19, loss = 0.10482551\n",
      "Iteration 20, loss = 0.10223522\n",
      "Iteration 21, loss = 0.10002135\n",
      "Iteration 22, loss = 0.09620917\n",
      "Iteration 23, loss = 0.09501419\n",
      "Iteration 24, loss = 0.09227021\n",
      "Iteration 25, loss = 0.09208538\n",
      "Iteration 26, loss = 0.08927163\n",
      "Iteration 27, loss = 0.08837318\n",
      "Iteration 28, loss = 0.08693456\n",
      "Iteration 29, loss = 0.08562405\n",
      "Iteration 30, loss = 0.08389124\n",
      "Iteration 31, loss = 0.08213392\n",
      "Iteration 32, loss = 0.08216534\n",
      "Iteration 33, loss = 0.08155407\n",
      "Iteration 34, loss = 0.07905745\n",
      "Iteration 35, loss = 0.07941896\n",
      "Iteration 36, loss = 0.07683696\n",
      "Iteration 37, loss = 0.07720381\n",
      "Iteration 38, loss = 0.07646303\n",
      "Iteration 39, loss = 0.07497041\n",
      "Iteration 40, loss = 0.07369194\n",
      "Iteration 41, loss = 0.07360791\n",
      "Iteration 42, loss = 0.07258705\n",
      "Iteration 43, loss = 0.07226169\n",
      "Iteration 44, loss = 0.07096933\n",
      "Iteration 45, loss = 0.07106527\n",
      "Iteration 46, loss = 0.07002649\n",
      "Iteration 47, loss = 0.06965836\n",
      "Iteration 48, loss = 0.06853523\n",
      "Iteration 49, loss = 0.06916452\n",
      "Iteration 50, loss = 0.06756652\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kileymack/opt/anaconda3/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (50) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 2.13630571\n",
      "Iteration 2, loss = 0.41156979\n",
      "Iteration 3, loss = 0.33075910\n",
      "Iteration 4, loss = 0.29072542\n",
      "Iteration 5, loss = 0.26597929\n",
      "Iteration 6, loss = 0.24795240\n",
      "Iteration 7, loss = 0.23403847\n",
      "Iteration 8, loss = 0.22181738\n",
      "Iteration 9, loss = 0.21304840\n",
      "Iteration 10, loss = 0.20411347\n",
      "Iteration 11, loss = 0.19674550\n",
      "Iteration 12, loss = 0.19028324\n",
      "Iteration 13, loss = 0.18404055\n",
      "Iteration 14, loss = 0.18001161\n",
      "Iteration 15, loss = 0.17429764\n",
      "Iteration 16, loss = 0.17058679\n",
      "Iteration 17, loss = 0.16616087\n",
      "Iteration 18, loss = 0.16299671\n",
      "Iteration 19, loss = 0.15917220\n",
      "Iteration 20, loss = 0.15594705\n",
      "Iteration 21, loss = 0.15317331\n",
      "Iteration 22, loss = 0.14993045\n",
      "Iteration 23, loss = 0.14694550\n",
      "Iteration 24, loss = 0.14466809\n",
      "Iteration 25, loss = 0.14182157\n",
      "Iteration 26, loss = 0.13968836\n",
      "Iteration 27, loss = 0.13708334\n",
      "Iteration 28, loss = 0.13494955\n",
      "Iteration 29, loss = 0.13316464\n",
      "Iteration 30, loss = 0.13090123\n",
      "Iteration 31, loss = 0.12884835\n",
      "Iteration 32, loss = 0.12716581\n",
      "Iteration 33, loss = 0.12512489\n",
      "Iteration 34, loss = 0.12346389\n",
      "Iteration 35, loss = 0.12158024\n",
      "Iteration 36, loss = 0.12027427\n",
      "Iteration 37, loss = 0.11861775\n",
      "Iteration 38, loss = 0.11692579\n",
      "Iteration 39, loss = 0.11594661\n",
      "Iteration 40, loss = 0.11415388\n",
      "Iteration 41, loss = 0.11287719\n",
      "Iteration 42, loss = 0.11133956\n",
      "Iteration 43, loss = 0.11029028\n",
      "Iteration 44, loss = 0.10908740\n",
      "Iteration 45, loss = 0.10797075\n",
      "Iteration 46, loss = 0.10630308\n",
      "Iteration 47, loss = 0.10542086\n",
      "Iteration 48, loss = 0.10395810\n",
      "Iteration 49, loss = 0.10337632\n",
      "Iteration 50, loss = 0.10239999\n",
      "Iteration 51, loss = 0.10115233\n",
      "Iteration 52, loss = 0.09993617\n",
      "Iteration 53, loss = 0.09911479\n",
      "Iteration 54, loss = 0.09807130\n",
      "Iteration 55, loss = 0.09717473\n",
      "Iteration 56, loss = 0.09646534\n",
      "Iteration 57, loss = 0.09554677\n",
      "Iteration 58, loss = 0.09466420\n",
      "Iteration 59, loss = 0.09352849\n",
      "Iteration 60, loss = 0.09293403\n",
      "Iteration 61, loss = 0.09219228\n",
      "Iteration 62, loss = 0.09166226\n",
      "Iteration 63, loss = 0.09071278\n",
      "Iteration 64, loss = 0.08982822\n",
      "Iteration 65, loss = 0.08913034\n",
      "Iteration 66, loss = 0.08829482\n",
      "Iteration 67, loss = 0.08745370\n",
      "Iteration 68, loss = 0.08708800\n",
      "Iteration 69, loss = 0.08657307\n",
      "Iteration 70, loss = 0.08581444\n",
      "Iteration 71, loss = 0.08502984\n",
      "Iteration 72, loss = 0.08451935\n",
      "Iteration 73, loss = 0.08408629\n",
      "Iteration 74, loss = 0.08345992\n",
      "Iteration 75, loss = 0.08245328\n",
      "Iteration 76, loss = 0.08186417\n",
      "Iteration 77, loss = 0.08125498\n",
      "Iteration 78, loss = 0.08108281\n",
      "Iteration 79, loss = 0.08020123\n",
      "Iteration 80, loss = 0.07964847\n",
      "Iteration 81, loss = 0.07921964\n",
      "Iteration 82, loss = 0.07867097\n",
      "Iteration 83, loss = 0.07814144\n",
      "Iteration 84, loss = 0.07763240\n",
      "Iteration 85, loss = 0.07731087\n",
      "Iteration 86, loss = 0.07664947\n",
      "Iteration 87, loss = 0.07595451\n",
      "Iteration 88, loss = 0.07566664\n",
      "Iteration 89, loss = 0.07530332\n",
      "Iteration 90, loss = 0.07451665\n",
      "Iteration 91, loss = 0.07445089\n",
      "Iteration 92, loss = 0.07359830\n",
      "Iteration 93, loss = 0.07353869\n",
      "Iteration 94, loss = 0.07297249\n",
      "Iteration 95, loss = 0.07210413\n",
      "Iteration 96, loss = 0.07200217\n",
      "Iteration 97, loss = 0.07183395\n",
      "Iteration 98, loss = 0.07104217\n",
      "Iteration 99, loss = 0.07063702\n",
      "Iteration 100, loss = 0.07029157\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kileymack/opt/anaconda3/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 1.89526199\n",
      "Iteration 2, loss = 0.39435285\n",
      "Iteration 3, loss = 0.32893198\n",
      "Iteration 4, loss = 0.29247830\n",
      "Iteration 5, loss = 0.26725908\n",
      "Iteration 6, loss = 0.24901219\n",
      "Iteration 7, loss = 0.23456497\n",
      "Iteration 8, loss = 0.22222329\n",
      "Iteration 9, loss = 0.21219267\n",
      "Iteration 10, loss = 0.20352886\n",
      "Iteration 11, loss = 0.19636202\n",
      "Iteration 12, loss = 0.18942859\n",
      "Iteration 13, loss = 0.18329410\n",
      "Iteration 14, loss = 0.17810742\n",
      "Iteration 15, loss = 0.17288713\n",
      "Iteration 16, loss = 0.16864115\n",
      "Iteration 17, loss = 0.16447737\n",
      "Iteration 18, loss = 0.16078274\n",
      "Iteration 19, loss = 0.15698763\n",
      "Iteration 20, loss = 0.15376658\n",
      "Iteration 21, loss = 0.15056222\n",
      "Iteration 22, loss = 0.14757861\n",
      "Iteration 23, loss = 0.14476262\n",
      "Iteration 24, loss = 0.14187262\n",
      "Iteration 25, loss = 0.13965245\n",
      "Iteration 26, loss = 0.13752207\n",
      "Iteration 27, loss = 0.13525965\n",
      "Iteration 28, loss = 0.13294208\n",
      "Iteration 29, loss = 0.13089825\n",
      "Iteration 30, loss = 0.12902012\n",
      "Iteration 31, loss = 0.12700497\n",
      "Iteration 32, loss = 0.12528995\n",
      "Iteration 33, loss = 0.12347190\n",
      "Iteration 34, loss = 0.12195631\n",
      "Iteration 35, loss = 0.12014656\n",
      "Iteration 36, loss = 0.11867509\n",
      "Iteration 37, loss = 0.11734238\n",
      "Iteration 38, loss = 0.11582675\n",
      "Iteration 39, loss = 0.11410159\n",
      "Iteration 40, loss = 0.11293415\n",
      "Iteration 41, loss = 0.11165262\n",
      "Iteration 42, loss = 0.11042602\n",
      "Iteration 43, loss = 0.10914110\n",
      "Iteration 44, loss = 0.10781827\n",
      "Iteration 45, loss = 0.10673274\n",
      "Iteration 46, loss = 0.10570576\n",
      "Iteration 47, loss = 0.10461887\n",
      "Iteration 48, loss = 0.10368361\n",
      "Iteration 49, loss = 0.10243229\n",
      "Iteration 50, loss = 0.10137271\n",
      "Iteration 51, loss = 0.10056250\n",
      "Iteration 52, loss = 0.09952156\n",
      "Iteration 53, loss = 0.09859977\n",
      "Iteration 54, loss = 0.09759321\n",
      "Iteration 55, loss = 0.09676584\n",
      "Iteration 56, loss = 0.09621073\n",
      "Iteration 57, loss = 0.09516201\n",
      "Iteration 58, loss = 0.09428851\n",
      "Iteration 59, loss = 0.09352458\n",
      "Iteration 60, loss = 0.09274089\n",
      "Iteration 61, loss = 0.09183282\n",
      "Iteration 62, loss = 0.09140756\n",
      "Iteration 63, loss = 0.09037527\n",
      "Iteration 64, loss = 0.08967908\n",
      "Iteration 65, loss = 0.08899585\n",
      "Iteration 66, loss = 0.08821431\n",
      "Iteration 67, loss = 0.08772859\n",
      "Iteration 68, loss = 0.08704016\n",
      "Iteration 69, loss = 0.08636921\n",
      "Iteration 70, loss = 0.08563291\n",
      "Iteration 71, loss = 0.08512679\n",
      "Iteration 72, loss = 0.08460241\n",
      "Iteration 73, loss = 0.08399008\n",
      "Iteration 74, loss = 0.08325671\n",
      "Iteration 75, loss = 0.08270172\n",
      "Iteration 76, loss = 0.08209737\n",
      "Iteration 77, loss = 0.08143142\n",
      "Iteration 78, loss = 0.08103059\n",
      "Iteration 79, loss = 0.08063068\n",
      "Iteration 80, loss = 0.08005237\n",
      "Iteration 81, loss = 0.07947973\n",
      "Iteration 82, loss = 0.07898445\n",
      "Iteration 83, loss = 0.07828778\n",
      "Iteration 84, loss = 0.07797880\n",
      "Iteration 85, loss = 0.07730637\n",
      "Iteration 86, loss = 0.07705752\n",
      "Iteration 87, loss = 0.07650916\n",
      "Iteration 88, loss = 0.07603509\n",
      "Iteration 89, loss = 0.07557501\n",
      "Iteration 90, loss = 0.07503735\n",
      "Iteration 91, loss = 0.07474393\n",
      "Iteration 92, loss = 0.07421474\n",
      "Iteration 93, loss = 0.07374872\n",
      "Iteration 94, loss = 0.07327456\n",
      "Iteration 95, loss = 0.07310306\n",
      "Iteration 96, loss = 0.07258096\n",
      "Iteration 97, loss = 0.07211903\n",
      "Iteration 98, loss = 0.07189192\n",
      "Iteration 99, loss = 0.07127963\n",
      "Iteration 100, loss = 0.07107207\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kileymack/opt/anaconda3/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 6.94845957\n",
      "Iteration 2, loss = 1.10447089\n",
      "Iteration 3, loss = 0.51319165\n",
      "Iteration 4, loss = 0.33995558\n",
      "Iteration 5, loss = 0.26653577\n",
      "Iteration 6, loss = 0.22692386\n",
      "Iteration 7, loss = 0.20505703\n",
      "Iteration 8, loss = 0.18052043\n",
      "Iteration 9, loss = 0.17052450\n",
      "Iteration 10, loss = 0.15569780\n",
      "Iteration 11, loss = 0.14599026\n",
      "Iteration 12, loss = 0.13678452\n",
      "Iteration 13, loss = 0.13306636\n",
      "Iteration 14, loss = 0.12921618\n",
      "Iteration 15, loss = 0.12145563\n",
      "Iteration 16, loss = 0.11716851\n",
      "Iteration 17, loss = 0.11175489\n",
      "Iteration 18, loss = 0.10729911\n",
      "Iteration 19, loss = 0.10579428\n",
      "Iteration 20, loss = 0.10294765\n",
      "Iteration 21, loss = 0.09915991\n",
      "Iteration 22, loss = 0.09503529\n",
      "Iteration 23, loss = 0.09265239\n",
      "Iteration 24, loss = 0.09243001\n",
      "Iteration 25, loss = 0.08791669\n",
      "Iteration 26, loss = 0.09030127\n",
      "Iteration 27, loss = 0.08411538\n",
      "Iteration 28, loss = 0.08239041\n",
      "Iteration 29, loss = 0.08028024\n",
      "Iteration 30, loss = 0.08206261\n",
      "Iteration 31, loss = 0.07893573\n",
      "Iteration 32, loss = 0.07717577\n",
      "Iteration 33, loss = 0.07664721\n",
      "Iteration 34, loss = 0.07542393\n",
      "Iteration 35, loss = 0.07335992\n",
      "Iteration 36, loss = 0.07238967\n",
      "Iteration 37, loss = 0.07051630\n",
      "Iteration 38, loss = 0.07103197\n",
      "Iteration 39, loss = 0.07001866\n",
      "Iteration 40, loss = 0.06698506\n",
      "Iteration 41, loss = 0.06767951\n",
      "Iteration 42, loss = 0.06701841\n",
      "Iteration 43, loss = 0.06551638\n",
      "Iteration 44, loss = 0.06438078\n",
      "Iteration 45, loss = 0.06505698\n",
      "Iteration 46, loss = 0.06393485\n",
      "Iteration 47, loss = 0.06177700\n",
      "Iteration 48, loss = 0.06233264\n",
      "Iteration 49, loss = 0.06294979\n",
      "Iteration 50, loss = 0.05861734\n",
      "Iteration 51, loss = 0.06133379\n",
      "Iteration 52, loss = 0.05796527\n",
      "Iteration 53, loss = 0.05835490\n",
      "Iteration 54, loss = 0.05854548\n",
      "Iteration 55, loss = 0.05896705\n",
      "Iteration 56, loss = 0.05686513\n",
      "Iteration 57, loss = 0.05606124\n",
      "Iteration 58, loss = 0.05649298\n",
      "Iteration 59, loss = 0.05580704\n",
      "Iteration 60, loss = 0.05393458\n",
      "Iteration 61, loss = 0.05595349\n",
      "Iteration 62, loss = 0.05375048\n",
      "Iteration 63, loss = 0.05323491\n",
      "Iteration 64, loss = 0.05477064\n",
      "Iteration 65, loss = 0.05214619\n",
      "Iteration 66, loss = 0.05333139\n",
      "Iteration 67, loss = 0.05249609\n",
      "Iteration 68, loss = 0.05226025\n",
      "Iteration 69, loss = 0.05106021\n",
      "Iteration 70, loss = 0.05127109\n",
      "Iteration 71, loss = 0.05121124\n",
      "Iteration 72, loss = 0.05059895\n",
      "Iteration 73, loss = 0.05027903\n",
      "Iteration 74, loss = 0.04987334\n",
      "Iteration 75, loss = 0.04979254\n",
      "Iteration 76, loss = 0.04835137\n",
      "Iteration 77, loss = 0.04888134\n",
      "Iteration 78, loss = 0.04854337\n",
      "Iteration 79, loss = 0.04850627\n",
      "Iteration 80, loss = 0.04776609\n",
      "Iteration 81, loss = 0.04798728\n",
      "Iteration 82, loss = 0.04707550\n",
      "Iteration 83, loss = 0.04691313\n",
      "Iteration 84, loss = 0.04594691\n",
      "Iteration 85, loss = 0.04575911\n",
      "Iteration 86, loss = 0.04653068\n",
      "Iteration 87, loss = 0.04637194\n",
      "Iteration 88, loss = 0.04562378\n",
      "Iteration 89, loss = 0.04589552\n",
      "Iteration 90, loss = 0.04478917\n",
      "Iteration 91, loss = 0.04535375\n",
      "Iteration 92, loss = 0.04511938\n",
      "Iteration 93, loss = 0.04430479\n",
      "Iteration 94, loss = 0.04555329\n",
      "Iteration 95, loss = 0.04435827\n",
      "Iteration 96, loss = 0.04364678\n",
      "Iteration 97, loss = 0.04360709\n",
      "Iteration 98, loss = 0.04362255\n",
      "Iteration 99, loss = 0.04368208\n",
      "Iteration 100, loss = 0.04320960\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kileymack/opt/anaconda3/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 11.64897593\n",
      "Iteration 2, loss = 0.81975543\n",
      "Iteration 3, loss = 0.38087477\n",
      "Iteration 4, loss = 0.27909386\n",
      "Iteration 5, loss = 0.22994017\n",
      "Iteration 6, loss = 0.20062163\n",
      "Iteration 7, loss = 0.18518249\n",
      "Iteration 8, loss = 0.16889443\n",
      "Iteration 9, loss = 0.15973776\n",
      "Iteration 10, loss = 0.15033613\n",
      "Iteration 11, loss = 0.14032831\n",
      "Iteration 12, loss = 0.13504493\n",
      "Iteration 13, loss = 0.12804213\n",
      "Iteration 14, loss = 0.12272501\n",
      "Iteration 15, loss = 0.11992817\n",
      "Iteration 16, loss = 0.11403275\n",
      "Iteration 17, loss = 0.11117440\n",
      "Iteration 18, loss = 0.10694424\n",
      "Iteration 19, loss = 0.10482551\n",
      "Iteration 20, loss = 0.10223522\n",
      "Iteration 21, loss = 0.10002135\n",
      "Iteration 22, loss = 0.09620917\n",
      "Iteration 23, loss = 0.09501419\n",
      "Iteration 24, loss = 0.09227021\n",
      "Iteration 25, loss = 0.09208538\n",
      "Iteration 26, loss = 0.08927163\n",
      "Iteration 27, loss = 0.08837318\n",
      "Iteration 28, loss = 0.08693456\n",
      "Iteration 29, loss = 0.08562405\n",
      "Iteration 30, loss = 0.08389124\n",
      "Iteration 31, loss = 0.08213392\n",
      "Iteration 32, loss = 0.08216534\n",
      "Iteration 33, loss = 0.08155407\n",
      "Iteration 34, loss = 0.07905745\n",
      "Iteration 35, loss = 0.07941896\n",
      "Iteration 36, loss = 0.07683696\n",
      "Iteration 37, loss = 0.07720381\n",
      "Iteration 38, loss = 0.07646303\n",
      "Iteration 39, loss = 0.07497041\n",
      "Iteration 40, loss = 0.07369194\n",
      "Iteration 41, loss = 0.07360791\n",
      "Iteration 42, loss = 0.07258705\n",
      "Iteration 43, loss = 0.07226169\n",
      "Iteration 44, loss = 0.07096933\n",
      "Iteration 45, loss = 0.07106527\n",
      "Iteration 46, loss = 0.07002649\n",
      "Iteration 47, loss = 0.06965836\n",
      "Iteration 48, loss = 0.06853523\n",
      "Iteration 49, loss = 0.06916452\n",
      "Iteration 50, loss = 0.06756652\n",
      "Iteration 51, loss = 0.06771443\n",
      "Iteration 52, loss = 0.06756843\n",
      "Iteration 53, loss = 0.06702429\n",
      "Iteration 54, loss = 0.06564871\n",
      "Iteration 55, loss = 0.06571685\n",
      "Iteration 56, loss = 0.06490287\n",
      "Iteration 57, loss = 0.06493541\n",
      "Iteration 58, loss = 0.06463575\n",
      "Iteration 59, loss = 0.06398117\n",
      "Iteration 60, loss = 0.06398324\n",
      "Iteration 61, loss = 0.06317435\n",
      "Iteration 62, loss = 0.06325149\n",
      "Iteration 63, loss = 0.06290125\n",
      "Iteration 64, loss = 0.06191544\n",
      "Iteration 65, loss = 0.06162727\n",
      "Iteration 66, loss = 0.06191486\n",
      "Iteration 67, loss = 0.06118495\n",
      "Iteration 68, loss = 0.06126265\n",
      "Iteration 69, loss = 0.06057063\n",
      "Iteration 70, loss = 0.06039055\n",
      "Iteration 71, loss = 0.06032236\n",
      "Iteration 72, loss = 0.05982024\n",
      "Iteration 73, loss = 0.06023420\n",
      "Iteration 74, loss = 0.05886149\n",
      "Iteration 75, loss = 0.05935031\n",
      "Iteration 76, loss = 0.05890497\n",
      "Iteration 77, loss = 0.05848685\n",
      "Iteration 78, loss = 0.05887677\n",
      "Iteration 79, loss = 0.05804514\n",
      "Iteration 80, loss = 0.05815629\n",
      "Iteration 81, loss = 0.05838694\n",
      "Iteration 82, loss = 0.05631981\n",
      "Iteration 83, loss = 0.05770404\n",
      "Iteration 84, loss = 0.05730358\n",
      "Iteration 85, loss = 0.05744220\n",
      "Iteration 86, loss = 0.05671268\n",
      "Iteration 87, loss = 0.05661542\n",
      "Iteration 88, loss = 0.05687931\n",
      "Iteration 89, loss = 0.05609705\n",
      "Iteration 90, loss = 0.05582466\n",
      "Iteration 91, loss = 0.05556953\n",
      "Iteration 92, loss = 0.05630414\n",
      "Iteration 93, loss = 0.05541134\n",
      "Iteration 94, loss = 0.05544071\n",
      "Iteration 95, loss = 0.05536456\n",
      "Iteration 96, loss = 0.05467858\n",
      "Iteration 97, loss = 0.05502075\n",
      "Iteration 98, loss = 0.05471433\n",
      "Iteration 99, loss = 0.05411578\n",
      "Iteration 100, loss = 0.05442966\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kileymack/opt/anaconda3/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 1.88797009\n",
      "Iteration 2, loss = 0.43287085\n",
      "Iteration 3, loss = 0.36058674\n",
      "Iteration 4, loss = 0.32454685\n",
      "Iteration 5, loss = 0.30169635\n",
      "Iteration 6, loss = 0.28486656\n",
      "Iteration 7, loss = 0.27193170\n",
      "Iteration 8, loss = 0.26154220\n",
      "Iteration 9, loss = 0.25241203\n",
      "Iteration 10, loss = 0.24471950\n",
      "Iteration 11, loss = 0.23730317\n",
      "Iteration 12, loss = 0.23092047\n",
      "Iteration 13, loss = 0.22560002\n",
      "Iteration 14, loss = 0.22029951\n",
      "Iteration 15, loss = 0.21568807\n",
      "Iteration 16, loss = 0.21156222\n",
      "Iteration 17, loss = 0.20737470\n",
      "Iteration 18, loss = 0.20350055\n",
      "Iteration 19, loss = 0.20053098\n",
      "Iteration 20, loss = 0.19725659\n",
      "Iteration 21, loss = 0.19402701\n",
      "Iteration 22, loss = 0.19116883\n",
      "Iteration 23, loss = 0.18809170\n",
      "Iteration 24, loss = 0.18603633\n",
      "Iteration 25, loss = 0.18385696\n",
      "Iteration 26, loss = 0.18111435\n",
      "Iteration 27, loss = 0.17839660\n",
      "Iteration 28, loss = 0.17697629\n",
      "Iteration 29, loss = 0.17484186\n",
      "Iteration 30, loss = 0.17264853\n",
      "Iteration 31, loss = 0.17057143\n",
      "Iteration 32, loss = 0.16854508\n",
      "Iteration 33, loss = 0.16731449\n",
      "Iteration 34, loss = 0.16576512\n",
      "Iteration 35, loss = 0.16367256\n",
      "Iteration 36, loss = 0.16241157\n",
      "Iteration 37, loss = 0.16067762\n",
      "Iteration 38, loss = 0.15926214\n",
      "Iteration 39, loss = 0.15791030\n",
      "Iteration 40, loss = 0.15618079\n",
      "Iteration 41, loss = 0.15522954\n",
      "Iteration 42, loss = 0.15415332\n",
      "Iteration 43, loss = 0.15296216\n",
      "Iteration 44, loss = 0.15147150\n",
      "Iteration 45, loss = 0.15074212\n",
      "Iteration 46, loss = 0.14898486\n",
      "Iteration 47, loss = 0.14828642\n",
      "Iteration 48, loss = 0.14693314\n",
      "Iteration 49, loss = 0.14552740\n",
      "Iteration 50, loss = 0.14522462\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kileymack/opt/anaconda3/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (50) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 1.70834157\n",
      "Iteration 2, loss = 0.41664995\n",
      "Iteration 3, loss = 0.35675968\n",
      "Iteration 4, loss = 0.32290167\n",
      "Iteration 5, loss = 0.30055594\n",
      "Iteration 6, loss = 0.28297419\n",
      "Iteration 7, loss = 0.26939022\n",
      "Iteration 8, loss = 0.25823469\n",
      "Iteration 9, loss = 0.24878514\n",
      "Iteration 10, loss = 0.24084426\n",
      "Iteration 11, loss = 0.23362374\n",
      "Iteration 12, loss = 0.22735069\n",
      "Iteration 13, loss = 0.22156620\n",
      "Iteration 14, loss = 0.21609837\n",
      "Iteration 15, loss = 0.21101247\n",
      "Iteration 16, loss = 0.20669202\n",
      "Iteration 17, loss = 0.20232124\n",
      "Iteration 18, loss = 0.19853092\n",
      "Iteration 19, loss = 0.19466281\n",
      "Iteration 20, loss = 0.19168414\n",
      "Iteration 21, loss = 0.18796198\n",
      "Iteration 22, loss = 0.18503314\n",
      "Iteration 23, loss = 0.18221174\n",
      "Iteration 24, loss = 0.17938611\n",
      "Iteration 25, loss = 0.17689382\n",
      "Iteration 26, loss = 0.17462097\n",
      "Iteration 27, loss = 0.17202355\n",
      "Iteration 28, loss = 0.16989190\n",
      "Iteration 29, loss = 0.16794146\n",
      "Iteration 30, loss = 0.16581192\n",
      "Iteration 31, loss = 0.16400897\n",
      "Iteration 32, loss = 0.16211747\n",
      "Iteration 33, loss = 0.16032609\n",
      "Iteration 34, loss = 0.15853343\n",
      "Iteration 35, loss = 0.15697586\n",
      "Iteration 36, loss = 0.15503797\n",
      "Iteration 37, loss = 0.15366920\n",
      "Iteration 38, loss = 0.15230721\n",
      "Iteration 39, loss = 0.15112761\n",
      "Iteration 40, loss = 0.14960099\n",
      "Iteration 41, loss = 0.14811787\n",
      "Iteration 42, loss = 0.14710477\n",
      "Iteration 43, loss = 0.14565775\n",
      "Iteration 44, loss = 0.14458675\n",
      "Iteration 45, loss = 0.14337422\n",
      "Iteration 46, loss = 0.14217568\n",
      "Iteration 47, loss = 0.14079204\n",
      "Iteration 48, loss = 0.13994068\n",
      "Iteration 49, loss = 0.13889693\n",
      "Iteration 50, loss = 0.13799314\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kileymack/opt/anaconda3/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (50) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 4.26863801\n",
      "Iteration 2, loss = 0.59056517\n",
      "Iteration 3, loss = 0.37714802\n",
      "Iteration 4, loss = 0.30672242\n",
      "Iteration 5, loss = 0.27237235\n",
      "Iteration 6, loss = 0.25235259\n",
      "Iteration 7, loss = 0.23470060\n",
      "Iteration 8, loss = 0.22280554\n",
      "Iteration 9, loss = 0.20981393\n",
      "Iteration 10, loss = 0.20437984\n",
      "Iteration 11, loss = 0.19767342\n",
      "Iteration 12, loss = 0.18804227\n",
      "Iteration 13, loss = 0.18813000\n",
      "Iteration 14, loss = 0.17524795\n",
      "Iteration 15, loss = 0.17174022\n",
      "Iteration 16, loss = 0.16937590\n",
      "Iteration 17, loss = 0.16626353\n",
      "Iteration 18, loss = 0.16347058\n",
      "Iteration 19, loss = 0.15792681\n",
      "Iteration 20, loss = 0.15838514\n",
      "Iteration 21, loss = 0.15477730\n",
      "Iteration 22, loss = 0.15033649\n",
      "Iteration 23, loss = 0.14491986\n",
      "Iteration 24, loss = 0.14792431\n",
      "Iteration 25, loss = 0.14371204\n",
      "Iteration 26, loss = 0.13975868\n",
      "Iteration 27, loss = 0.13867147\n",
      "Iteration 28, loss = 0.13544684\n",
      "Iteration 29, loss = 0.13650826\n",
      "Iteration 30, loss = 0.13512938\n",
      "Iteration 31, loss = 0.13323419\n",
      "Iteration 32, loss = 0.13318884\n",
      "Iteration 33, loss = 0.12981120\n",
      "Iteration 34, loss = 0.12767587\n",
      "Iteration 35, loss = 0.12931642\n",
      "Iteration 36, loss = 0.12344085\n",
      "Iteration 37, loss = 0.12647007\n",
      "Iteration 38, loss = 0.12467689\n",
      "Iteration 39, loss = 0.12675606\n",
      "Iteration 40, loss = 0.12105306\n",
      "Iteration 41, loss = 0.12359073\n",
      "Iteration 42, loss = 0.11999800\n",
      "Iteration 43, loss = 0.11945860\n",
      "Iteration 44, loss = 0.11770416\n",
      "Iteration 45, loss = 0.11985367\n",
      "Iteration 46, loss = 0.11818552\n",
      "Iteration 47, loss = 0.11497279\n",
      "Iteration 48, loss = 0.11587529\n",
      "Iteration 49, loss = 0.11420151\n",
      "Iteration 50, loss = 0.11579757\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kileymack/opt/anaconda3/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (50) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 6.98333308\n",
      "Iteration 2, loss = 0.51352558\n",
      "Iteration 3, loss = 0.34910836\n",
      "Iteration 4, loss = 0.29295713\n",
      "Iteration 5, loss = 0.26071124\n",
      "Iteration 6, loss = 0.24099502\n",
      "Iteration 7, loss = 0.22837668\n",
      "Iteration 8, loss = 0.21447187\n",
      "Iteration 9, loss = 0.20667003\n",
      "Iteration 10, loss = 0.19685586\n",
      "Iteration 11, loss = 0.19207279\n",
      "Iteration 12, loss = 0.18409908\n",
      "Iteration 13, loss = 0.17966659\n",
      "Iteration 14, loss = 0.17423779\n",
      "Iteration 15, loss = 0.17232710\n",
      "Iteration 16, loss = 0.16736804\n",
      "Iteration 17, loss = 0.16238996\n",
      "Iteration 18, loss = 0.16097865\n",
      "Iteration 19, loss = 0.15709287\n",
      "Iteration 20, loss = 0.15677148\n",
      "Iteration 21, loss = 0.15268156\n",
      "Iteration 22, loss = 0.14966075\n",
      "Iteration 23, loss = 0.15039261\n",
      "Iteration 24, loss = 0.14677656\n",
      "Iteration 25, loss = 0.14522288\n",
      "Iteration 26, loss = 0.14417400\n",
      "Iteration 27, loss = 0.14390213\n",
      "Iteration 28, loss = 0.14162425\n",
      "Iteration 29, loss = 0.14063132\n",
      "Iteration 30, loss = 0.13988025\n",
      "Iteration 31, loss = 0.13848355\n",
      "Iteration 32, loss = 0.13717634\n",
      "Iteration 33, loss = 0.13640866\n",
      "Iteration 34, loss = 0.13533597\n",
      "Iteration 35, loss = 0.13525121\n",
      "Iteration 36, loss = 0.13429891\n",
      "Iteration 37, loss = 0.13357625\n",
      "Iteration 38, loss = 0.13151110\n",
      "Iteration 39, loss = 0.13154310\n",
      "Iteration 40, loss = 0.13101724\n",
      "Iteration 41, loss = 0.13204253\n",
      "Iteration 42, loss = 0.13017499\n",
      "Iteration 43, loss = 0.12828244\n",
      "Iteration 44, loss = 0.12959821\n",
      "Iteration 45, loss = 0.12849962\n",
      "Iteration 46, loss = 0.12782304\n",
      "Iteration 47, loss = 0.12761791\n",
      "Iteration 48, loss = 0.12640237\n",
      "Iteration 49, loss = 0.12750071\n",
      "Iteration 50, loss = 0.12642591\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kileymack/opt/anaconda3/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (50) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 1.88797009\n",
      "Iteration 2, loss = 0.43287085\n",
      "Iteration 3, loss = 0.36058674\n",
      "Iteration 4, loss = 0.32454685\n",
      "Iteration 5, loss = 0.30169635\n",
      "Iteration 6, loss = 0.28486656\n",
      "Iteration 7, loss = 0.27193170\n",
      "Iteration 8, loss = 0.26154220\n",
      "Iteration 9, loss = 0.25241203\n",
      "Iteration 10, loss = 0.24471950\n",
      "Iteration 11, loss = 0.23730317\n",
      "Iteration 12, loss = 0.23092047\n",
      "Iteration 13, loss = 0.22560002\n",
      "Iteration 14, loss = 0.22029951\n",
      "Iteration 15, loss = 0.21568807\n",
      "Iteration 16, loss = 0.21156222\n",
      "Iteration 17, loss = 0.20737470\n",
      "Iteration 18, loss = 0.20350055\n",
      "Iteration 19, loss = 0.20053098\n",
      "Iteration 20, loss = 0.19725659\n",
      "Iteration 21, loss = 0.19402701\n",
      "Iteration 22, loss = 0.19116883\n",
      "Iteration 23, loss = 0.18809170\n",
      "Iteration 24, loss = 0.18603633\n",
      "Iteration 25, loss = 0.18385696\n",
      "Iteration 26, loss = 0.18111435\n",
      "Iteration 27, loss = 0.17839660\n",
      "Iteration 28, loss = 0.17697629\n",
      "Iteration 29, loss = 0.17484186\n",
      "Iteration 30, loss = 0.17264853\n",
      "Iteration 31, loss = 0.17057143\n",
      "Iteration 32, loss = 0.16854508\n",
      "Iteration 33, loss = 0.16731449\n",
      "Iteration 34, loss = 0.16576512\n",
      "Iteration 35, loss = 0.16367256\n",
      "Iteration 36, loss = 0.16241157\n",
      "Iteration 37, loss = 0.16067762\n",
      "Iteration 38, loss = 0.15926214\n",
      "Iteration 39, loss = 0.15791030\n",
      "Iteration 40, loss = 0.15618079\n",
      "Iteration 41, loss = 0.15522954\n",
      "Iteration 42, loss = 0.15415332\n",
      "Iteration 43, loss = 0.15296216\n",
      "Iteration 44, loss = 0.15147150\n",
      "Iteration 45, loss = 0.15074212\n",
      "Iteration 46, loss = 0.14898486\n",
      "Iteration 47, loss = 0.14828642\n",
      "Iteration 48, loss = 0.14693314\n",
      "Iteration 49, loss = 0.14552740\n",
      "Iteration 50, loss = 0.14522462\n",
      "Iteration 51, loss = 0.14389260\n",
      "Iteration 52, loss = 0.14265362\n",
      "Iteration 53, loss = 0.14155191\n",
      "Iteration 54, loss = 0.14111999\n",
      "Iteration 55, loss = 0.13973465\n",
      "Iteration 56, loss = 0.13919002\n",
      "Iteration 57, loss = 0.13811414\n",
      "Iteration 58, loss = 0.13711316\n",
      "Iteration 59, loss = 0.13648046\n",
      "Iteration 60, loss = 0.13538463\n",
      "Iteration 61, loss = 0.13498347\n",
      "Iteration 62, loss = 0.13365723\n",
      "Iteration 63, loss = 0.13322658\n",
      "Iteration 64, loss = 0.13199861\n",
      "Iteration 65, loss = 0.13190317\n",
      "Iteration 66, loss = 0.13079232\n",
      "Iteration 67, loss = 0.13012630\n",
      "Iteration 68, loss = 0.12966211\n",
      "Iteration 69, loss = 0.12832944\n",
      "Iteration 70, loss = 0.12760895\n",
      "Iteration 71, loss = 0.12721572\n",
      "Iteration 72, loss = 0.12697065\n",
      "Iteration 73, loss = 0.12574925\n",
      "Iteration 74, loss = 0.12550519\n",
      "Iteration 75, loss = 0.12495856\n",
      "Iteration 76, loss = 0.12428344\n",
      "Iteration 77, loss = 0.12378325\n",
      "Iteration 78, loss = 0.12278999\n",
      "Iteration 79, loss = 0.12222004\n",
      "Iteration 80, loss = 0.12210049\n",
      "Iteration 81, loss = 0.12095821\n",
      "Iteration 82, loss = 0.12070479\n",
      "Iteration 83, loss = 0.12022943\n",
      "Iteration 84, loss = 0.11961406\n",
      "Iteration 85, loss = 0.11881970\n",
      "Iteration 86, loss = 0.11868490\n",
      "Iteration 87, loss = 0.11836678\n",
      "Iteration 88, loss = 0.11755848\n",
      "Iteration 89, loss = 0.11694964\n",
      "Iteration 90, loss = 0.11702923\n",
      "Iteration 91, loss = 0.11610476\n",
      "Iteration 92, loss = 0.11589329\n",
      "Iteration 93, loss = 0.11473101\n",
      "Iteration 94, loss = 0.11430057\n",
      "Iteration 95, loss = 0.11416568\n",
      "Iteration 96, loss = 0.11368581\n",
      "Iteration 97, loss = 0.11366254\n",
      "Iteration 98, loss = 0.11301406\n",
      "Iteration 99, loss = 0.11283031\n",
      "Iteration 100, loss = 0.11217304\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kileymack/opt/anaconda3/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 1.70834157\n",
      "Iteration 2, loss = 0.41664995\n",
      "Iteration 3, loss = 0.35675968\n",
      "Iteration 4, loss = 0.32290167\n",
      "Iteration 5, loss = 0.30055594\n",
      "Iteration 6, loss = 0.28297419\n",
      "Iteration 7, loss = 0.26939022\n",
      "Iteration 8, loss = 0.25823469\n",
      "Iteration 9, loss = 0.24878514\n",
      "Iteration 10, loss = 0.24084426\n",
      "Iteration 11, loss = 0.23362374\n",
      "Iteration 12, loss = 0.22735069\n",
      "Iteration 13, loss = 0.22156620\n",
      "Iteration 14, loss = 0.21609837\n",
      "Iteration 15, loss = 0.21101247\n",
      "Iteration 16, loss = 0.20669202\n",
      "Iteration 17, loss = 0.20232124\n",
      "Iteration 18, loss = 0.19853092\n",
      "Iteration 19, loss = 0.19466281\n",
      "Iteration 20, loss = 0.19168414\n",
      "Iteration 21, loss = 0.18796198\n",
      "Iteration 22, loss = 0.18503314\n",
      "Iteration 23, loss = 0.18221174\n",
      "Iteration 24, loss = 0.17938611\n",
      "Iteration 25, loss = 0.17689382\n",
      "Iteration 26, loss = 0.17462097\n",
      "Iteration 27, loss = 0.17202355\n",
      "Iteration 28, loss = 0.16989190\n",
      "Iteration 29, loss = 0.16794146\n",
      "Iteration 30, loss = 0.16581192\n",
      "Iteration 31, loss = 0.16400897\n",
      "Iteration 32, loss = 0.16211747\n",
      "Iteration 33, loss = 0.16032609\n",
      "Iteration 34, loss = 0.15853343\n",
      "Iteration 35, loss = 0.15697586\n",
      "Iteration 36, loss = 0.15503797\n",
      "Iteration 37, loss = 0.15366920\n",
      "Iteration 38, loss = 0.15230721\n",
      "Iteration 39, loss = 0.15112761\n",
      "Iteration 40, loss = 0.14960099\n",
      "Iteration 41, loss = 0.14811787\n",
      "Iteration 42, loss = 0.14710477\n",
      "Iteration 43, loss = 0.14565775\n",
      "Iteration 44, loss = 0.14458675\n",
      "Iteration 45, loss = 0.14337422\n",
      "Iteration 46, loss = 0.14217568\n",
      "Iteration 47, loss = 0.14079204\n",
      "Iteration 48, loss = 0.13994068\n",
      "Iteration 49, loss = 0.13889693\n",
      "Iteration 50, loss = 0.13799314\n",
      "Iteration 51, loss = 0.13676190\n",
      "Iteration 52, loss = 0.13583445\n",
      "Iteration 53, loss = 0.13499703\n",
      "Iteration 54, loss = 0.13399130\n",
      "Iteration 55, loss = 0.13289388\n",
      "Iteration 56, loss = 0.13192216\n",
      "Iteration 57, loss = 0.13124135\n",
      "Iteration 58, loss = 0.13040945\n",
      "Iteration 59, loss = 0.12946225\n",
      "Iteration 60, loss = 0.12860959\n",
      "Iteration 61, loss = 0.12755480\n",
      "Iteration 62, loss = 0.12688224\n",
      "Iteration 63, loss = 0.12644426\n",
      "Iteration 64, loss = 0.12578549\n",
      "Iteration 65, loss = 0.12479726\n",
      "Iteration 66, loss = 0.12401675\n",
      "Iteration 67, loss = 0.12322387\n",
      "Iteration 68, loss = 0.12258537\n",
      "Iteration 69, loss = 0.12195872\n",
      "Iteration 70, loss = 0.12125151\n",
      "Iteration 71, loss = 0.12064000\n",
      "Iteration 72, loss = 0.11977844\n",
      "Iteration 73, loss = 0.11938014\n",
      "Iteration 74, loss = 0.11831559\n",
      "Iteration 75, loss = 0.11800245\n",
      "Iteration 76, loss = 0.11751668\n",
      "Iteration 77, loss = 0.11691271\n",
      "Iteration 78, loss = 0.11638578\n",
      "Iteration 79, loss = 0.11547373\n",
      "Iteration 80, loss = 0.11512910\n",
      "Iteration 81, loss = 0.11446624\n",
      "Iteration 82, loss = 0.11393622\n",
      "Iteration 83, loss = 0.11349054\n",
      "Iteration 84, loss = 0.11286204\n",
      "Iteration 85, loss = 0.11236968\n",
      "Iteration 86, loss = 0.11181904\n",
      "Iteration 87, loss = 0.11138328\n",
      "Iteration 88, loss = 0.11083219\n",
      "Iteration 89, loss = 0.11031767\n",
      "Iteration 90, loss = 0.10995526\n",
      "Iteration 91, loss = 0.10945729\n",
      "Iteration 92, loss = 0.10874106\n",
      "Iteration 93, loss = 0.10863427\n",
      "Iteration 94, loss = 0.10804986\n",
      "Iteration 95, loss = 0.10733230\n",
      "Iteration 96, loss = 0.10721880\n",
      "Iteration 97, loss = 0.10674340\n",
      "Iteration 98, loss = 0.10626984\n",
      "Iteration 99, loss = 0.10596389\n",
      "Iteration 100, loss = 0.10572157\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kileymack/opt/anaconda3/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 4.26863801\n",
      "Iteration 2, loss = 0.59056517\n",
      "Iteration 3, loss = 0.37714802\n",
      "Iteration 4, loss = 0.30672242\n",
      "Iteration 5, loss = 0.27237235\n",
      "Iteration 6, loss = 0.25235259\n",
      "Iteration 7, loss = 0.23470060\n",
      "Iteration 8, loss = 0.22280554\n",
      "Iteration 9, loss = 0.20981393\n",
      "Iteration 10, loss = 0.20437984\n",
      "Iteration 11, loss = 0.19767342\n",
      "Iteration 12, loss = 0.18804227\n",
      "Iteration 13, loss = 0.18813000\n",
      "Iteration 14, loss = 0.17524795\n",
      "Iteration 15, loss = 0.17174022\n",
      "Iteration 16, loss = 0.16937590\n",
      "Iteration 17, loss = 0.16626353\n",
      "Iteration 18, loss = 0.16347058\n",
      "Iteration 19, loss = 0.15792681\n",
      "Iteration 20, loss = 0.15838514\n",
      "Iteration 21, loss = 0.15477730\n",
      "Iteration 22, loss = 0.15033649\n",
      "Iteration 23, loss = 0.14491986\n",
      "Iteration 24, loss = 0.14792431\n",
      "Iteration 25, loss = 0.14371204\n",
      "Iteration 26, loss = 0.13975868\n",
      "Iteration 27, loss = 0.13867147\n",
      "Iteration 28, loss = 0.13544684\n",
      "Iteration 29, loss = 0.13650826\n",
      "Iteration 30, loss = 0.13512938\n",
      "Iteration 31, loss = 0.13323419\n",
      "Iteration 32, loss = 0.13318884\n",
      "Iteration 33, loss = 0.12981120\n",
      "Iteration 34, loss = 0.12767587\n",
      "Iteration 35, loss = 0.12931642\n",
      "Iteration 36, loss = 0.12344085\n",
      "Iteration 37, loss = 0.12647007\n",
      "Iteration 38, loss = 0.12467689\n",
      "Iteration 39, loss = 0.12675606\n",
      "Iteration 40, loss = 0.12105306\n",
      "Iteration 41, loss = 0.12359073\n",
      "Iteration 42, loss = 0.11999800\n",
      "Iteration 43, loss = 0.11945860\n",
      "Iteration 44, loss = 0.11770416\n",
      "Iteration 45, loss = 0.11985367\n",
      "Iteration 46, loss = 0.11818552\n",
      "Iteration 47, loss = 0.11497279\n",
      "Iteration 48, loss = 0.11587529\n",
      "Iteration 49, loss = 0.11420151\n",
      "Iteration 50, loss = 0.11579757\n",
      "Iteration 51, loss = 0.11564477\n",
      "Iteration 52, loss = 0.11592953\n",
      "Iteration 53, loss = 0.11104524\n",
      "Iteration 54, loss = 0.11407286\n",
      "Iteration 55, loss = 0.11097654\n",
      "Iteration 56, loss = 0.11298062\n",
      "Iteration 57, loss = 0.11220704\n",
      "Iteration 58, loss = 0.11039849\n",
      "Iteration 59, loss = 0.11070897\n",
      "Iteration 60, loss = 0.10945171\n",
      "Iteration 61, loss = 0.10992844\n",
      "Iteration 62, loss = 0.10870912\n",
      "Iteration 63, loss = 0.10851859\n",
      "Iteration 64, loss = 0.10695629\n",
      "Iteration 65, loss = 0.10750967\n",
      "Iteration 66, loss = 0.10814982\n",
      "Iteration 67, loss = 0.11001525\n",
      "Iteration 68, loss = 0.10513251\n",
      "Iteration 69, loss = 0.10547060\n",
      "Iteration 70, loss = 0.10555004\n",
      "Iteration 71, loss = 0.10485228\n",
      "Iteration 72, loss = 0.10798752\n",
      "Iteration 73, loss = 0.10499459\n",
      "Iteration 74, loss = 0.10592981\n",
      "Iteration 75, loss = 0.10447341\n",
      "Iteration 76, loss = 0.10709734\n",
      "Iteration 77, loss = 0.10240722\n",
      "Iteration 78, loss = 0.10495215\n",
      "Iteration 79, loss = 0.10255960\n",
      "Iteration 80, loss = 0.10432687\n",
      "Iteration 81, loss = 0.10355702\n",
      "Iteration 82, loss = 0.10261962\n",
      "Iteration 83, loss = 0.10167392\n",
      "Iteration 84, loss = 0.10136661\n",
      "Iteration 85, loss = 0.10219884\n",
      "Iteration 86, loss = 0.10151818\n",
      "Iteration 87, loss = 0.10070566\n",
      "Iteration 88, loss = 0.10183297\n",
      "Iteration 89, loss = 0.10259296\n",
      "Iteration 90, loss = 0.09963383\n",
      "Iteration 91, loss = 0.10148052\n",
      "Iteration 92, loss = 0.10196611\n",
      "Iteration 93, loss = 0.09958430\n",
      "Iteration 94, loss = 0.10016489\n",
      "Iteration 95, loss = 0.09943811\n",
      "Iteration 96, loss = 0.09948184\n",
      "Iteration 97, loss = 0.09888652\n",
      "Iteration 98, loss = 0.09883802\n",
      "Iteration 99, loss = 0.09838283\n",
      "Iteration 100, loss = 0.09812193\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kileymack/opt/anaconda3/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 6.98333308\n",
      "Iteration 2, loss = 0.51352558\n",
      "Iteration 3, loss = 0.34910836\n",
      "Iteration 4, loss = 0.29295713\n",
      "Iteration 5, loss = 0.26071124\n",
      "Iteration 6, loss = 0.24099502\n",
      "Iteration 7, loss = 0.22837668\n",
      "Iteration 8, loss = 0.21447187\n",
      "Iteration 9, loss = 0.20667003\n",
      "Iteration 10, loss = 0.19685586\n",
      "Iteration 11, loss = 0.19207279\n",
      "Iteration 12, loss = 0.18409908\n",
      "Iteration 13, loss = 0.17966659\n",
      "Iteration 14, loss = 0.17423779\n",
      "Iteration 15, loss = 0.17232710\n",
      "Iteration 16, loss = 0.16736804\n",
      "Iteration 17, loss = 0.16238996\n",
      "Iteration 18, loss = 0.16097865\n",
      "Iteration 19, loss = 0.15709287\n",
      "Iteration 20, loss = 0.15677148\n",
      "Iteration 21, loss = 0.15268156\n",
      "Iteration 22, loss = 0.14966075\n",
      "Iteration 23, loss = 0.15039261\n",
      "Iteration 24, loss = 0.14677656\n",
      "Iteration 25, loss = 0.14522288\n",
      "Iteration 26, loss = 0.14417400\n",
      "Iteration 27, loss = 0.14390213\n",
      "Iteration 28, loss = 0.14162425\n",
      "Iteration 29, loss = 0.14063132\n",
      "Iteration 30, loss = 0.13988025\n",
      "Iteration 31, loss = 0.13848355\n",
      "Iteration 32, loss = 0.13717634\n",
      "Iteration 33, loss = 0.13640866\n",
      "Iteration 34, loss = 0.13533597\n",
      "Iteration 35, loss = 0.13525121\n",
      "Iteration 36, loss = 0.13429891\n",
      "Iteration 37, loss = 0.13357625\n",
      "Iteration 38, loss = 0.13151110\n",
      "Iteration 39, loss = 0.13154310\n",
      "Iteration 40, loss = 0.13101724\n",
      "Iteration 41, loss = 0.13204253\n",
      "Iteration 42, loss = 0.13017499\n",
      "Iteration 43, loss = 0.12828244\n",
      "Iteration 44, loss = 0.12959821\n",
      "Iteration 45, loss = 0.12849962\n",
      "Iteration 46, loss = 0.12782304\n",
      "Iteration 47, loss = 0.12761791\n",
      "Iteration 48, loss = 0.12640237\n",
      "Iteration 49, loss = 0.12750071\n",
      "Iteration 50, loss = 0.12642591\n",
      "Iteration 51, loss = 0.12547329\n",
      "Iteration 52, loss = 0.12498184\n",
      "Iteration 53, loss = 0.12640292\n",
      "Iteration 54, loss = 0.12361559\n",
      "Iteration 55, loss = 0.12539609\n",
      "Iteration 56, loss = 0.12305646\n",
      "Iteration 57, loss = 0.12516036\n",
      "Iteration 58, loss = 0.12380411\n",
      "Iteration 59, loss = 0.12301564\n",
      "Iteration 60, loss = 0.12249838\n",
      "Iteration 61, loss = 0.12176402\n",
      "Iteration 62, loss = 0.12170579\n",
      "Iteration 63, loss = 0.12128212\n",
      "Iteration 64, loss = 0.12175100\n",
      "Iteration 65, loss = 0.12136277\n",
      "Iteration 66, loss = 0.12097579\n",
      "Iteration 67, loss = 0.12092329\n",
      "Iteration 68, loss = 0.11992226\n",
      "Iteration 69, loss = 0.12098226\n",
      "Iteration 70, loss = 0.12009913\n",
      "Iteration 71, loss = 0.11899435\n",
      "Iteration 72, loss = 0.11811021\n",
      "Iteration 73, loss = 0.11957464\n",
      "Iteration 74, loss = 0.11987355\n",
      "Iteration 75, loss = 0.11875374\n",
      "Iteration 76, loss = 0.11864458\n",
      "Iteration 77, loss = 0.11930846\n",
      "Iteration 78, loss = 0.11795570\n",
      "Iteration 79, loss = 0.11778611\n",
      "Iteration 80, loss = 0.11846412\n",
      "Iteration 81, loss = 0.11717941\n",
      "Iteration 82, loss = 0.11836051\n",
      "Iteration 83, loss = 0.11945823\n",
      "Iteration 84, loss = 0.11688193\n",
      "Iteration 85, loss = 0.11658763\n",
      "Iteration 86, loss = 0.11708568\n",
      "Iteration 87, loss = 0.11637632\n",
      "Iteration 88, loss = 0.11573270\n",
      "Iteration 89, loss = 0.11631380\n",
      "Iteration 90, loss = 0.11413333\n",
      "Iteration 91, loss = 0.11620884\n",
      "Iteration 92, loss = 0.11578217\n",
      "Iteration 93, loss = 0.11600406\n",
      "Iteration 94, loss = 0.11475339\n",
      "Iteration 95, loss = 0.11452428\n",
      "Iteration 96, loss = 0.11557865\n",
      "Iteration 97, loss = 0.11433510\n",
      "Iteration 98, loss = 0.11448233\n",
      "Iteration 99, loss = 0.11502757\n",
      "Iteration 100, loss = 0.11426852\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kileymack/opt/anaconda3/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 1.88797009\n",
      "Iteration 2, loss = 0.43287085\n",
      "Iteration 3, loss = 0.36058674\n",
      "Iteration 4, loss = 0.32454685\n",
      "Iteration 5, loss = 0.30169635\n",
      "Iteration 6, loss = 0.28486656\n",
      "Iteration 7, loss = 0.27193170\n",
      "Iteration 8, loss = 0.26154220\n",
      "Iteration 9, loss = 0.25241203\n",
      "Iteration 10, loss = 0.24471950\n",
      "Iteration 11, loss = 0.23730317\n",
      "Iteration 12, loss = 0.23092047\n",
      "Iteration 13, loss = 0.22560002\n",
      "Iteration 14, loss = 0.22029951\n",
      "Iteration 15, loss = 0.21568807\n",
      "Iteration 16, loss = 0.21156222\n",
      "Iteration 17, loss = 0.20737470\n",
      "Iteration 18, loss = 0.20350055\n",
      "Iteration 19, loss = 0.20053098\n",
      "Iteration 20, loss = 0.19725659\n",
      "Iteration 21, loss = 0.19402701\n",
      "Iteration 22, loss = 0.19116883\n",
      "Iteration 23, loss = 0.18809170\n",
      "Iteration 24, loss = 0.18603633\n",
      "Iteration 25, loss = 0.18385696\n",
      "Iteration 26, loss = 0.18111435\n",
      "Iteration 27, loss = 0.17839660\n",
      "Iteration 28, loss = 0.17697629\n",
      "Iteration 29, loss = 0.17484186\n",
      "Iteration 30, loss = 0.17264853\n",
      "Iteration 31, loss = 0.17057143\n",
      "Iteration 32, loss = 0.16854508\n",
      "Iteration 33, loss = 0.16731449\n",
      "Iteration 34, loss = 0.16576512\n",
      "Iteration 35, loss = 0.16367256\n",
      "Iteration 36, loss = 0.16241157\n",
      "Iteration 37, loss = 0.16067762\n",
      "Iteration 38, loss = 0.15926214\n",
      "Iteration 39, loss = 0.15791030\n",
      "Iteration 40, loss = 0.15618079\n",
      "Iteration 41, loss = 0.15522954\n",
      "Iteration 42, loss = 0.15415332\n",
      "Iteration 43, loss = 0.15296216\n",
      "Iteration 44, loss = 0.15147150\n",
      "Iteration 45, loss = 0.15074212\n",
      "Iteration 46, loss = 0.14898486\n",
      "Iteration 47, loss = 0.14828642\n",
      "Iteration 48, loss = 0.14693314\n",
      "Iteration 49, loss = 0.14552740\n",
      "Iteration 50, loss = 0.14522462\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kileymack/opt/anaconda3/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (50) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 1.70834157\n",
      "Iteration 2, loss = 0.41664995\n",
      "Iteration 3, loss = 0.35675968\n",
      "Iteration 4, loss = 0.32290167\n",
      "Iteration 5, loss = 0.30055594\n",
      "Iteration 6, loss = 0.28297419\n",
      "Iteration 7, loss = 0.26939022\n",
      "Iteration 8, loss = 0.25823469\n",
      "Iteration 9, loss = 0.24878514\n",
      "Iteration 10, loss = 0.24084426\n",
      "Iteration 11, loss = 0.23362374\n",
      "Iteration 12, loss = 0.22735069\n",
      "Iteration 13, loss = 0.22156620\n",
      "Iteration 14, loss = 0.21609837\n",
      "Iteration 15, loss = 0.21101247\n",
      "Iteration 16, loss = 0.20669202\n",
      "Iteration 17, loss = 0.20232124\n",
      "Iteration 18, loss = 0.19853092\n",
      "Iteration 19, loss = 0.19466281\n",
      "Iteration 20, loss = 0.19168414\n",
      "Iteration 21, loss = 0.18796198\n",
      "Iteration 22, loss = 0.18503314\n",
      "Iteration 23, loss = 0.18221174\n",
      "Iteration 24, loss = 0.17938611\n",
      "Iteration 25, loss = 0.17689382\n",
      "Iteration 26, loss = 0.17462097\n",
      "Iteration 27, loss = 0.17202355\n",
      "Iteration 28, loss = 0.16989190\n",
      "Iteration 29, loss = 0.16794146\n",
      "Iteration 30, loss = 0.16581192\n",
      "Iteration 31, loss = 0.16400897\n",
      "Iteration 32, loss = 0.16211747\n",
      "Iteration 33, loss = 0.16032609\n",
      "Iteration 34, loss = 0.15853343\n",
      "Iteration 35, loss = 0.15697586\n",
      "Iteration 36, loss = 0.15503797\n",
      "Iteration 37, loss = 0.15366920\n",
      "Iteration 38, loss = 0.15230721\n",
      "Iteration 39, loss = 0.15112761\n",
      "Iteration 40, loss = 0.14960099\n",
      "Iteration 41, loss = 0.14811787\n",
      "Iteration 42, loss = 0.14710477\n",
      "Iteration 43, loss = 0.14565775\n",
      "Iteration 44, loss = 0.14458675\n",
      "Iteration 45, loss = 0.14337422\n",
      "Iteration 46, loss = 0.14217568\n",
      "Iteration 47, loss = 0.14079204\n",
      "Iteration 48, loss = 0.13994068\n",
      "Iteration 49, loss = 0.13889693\n",
      "Iteration 50, loss = 0.13799314\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kileymack/opt/anaconda3/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (50) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 4.26863801\n",
      "Iteration 2, loss = 0.59056517\n",
      "Iteration 3, loss = 0.37714802\n",
      "Iteration 4, loss = 0.30672242\n",
      "Iteration 5, loss = 0.27237235\n",
      "Iteration 6, loss = 0.25235259\n",
      "Iteration 7, loss = 0.23470060\n",
      "Iteration 8, loss = 0.22280554\n",
      "Iteration 9, loss = 0.20981393\n",
      "Iteration 10, loss = 0.20437984\n",
      "Iteration 11, loss = 0.19767342\n",
      "Iteration 12, loss = 0.18804227\n",
      "Iteration 13, loss = 0.18813000\n",
      "Iteration 14, loss = 0.17524795\n",
      "Iteration 15, loss = 0.17174022\n",
      "Iteration 16, loss = 0.16937590\n",
      "Iteration 17, loss = 0.16626353\n",
      "Iteration 18, loss = 0.16347058\n",
      "Iteration 19, loss = 0.15792681\n",
      "Iteration 20, loss = 0.15838514\n",
      "Iteration 21, loss = 0.15477730\n",
      "Iteration 22, loss = 0.15033649\n",
      "Iteration 23, loss = 0.14491986\n",
      "Iteration 24, loss = 0.14792431\n",
      "Iteration 25, loss = 0.14371204\n",
      "Iteration 26, loss = 0.13975868\n",
      "Iteration 27, loss = 0.13867147\n",
      "Iteration 28, loss = 0.13544684\n",
      "Iteration 29, loss = 0.13650826\n",
      "Iteration 30, loss = 0.13512938\n",
      "Iteration 31, loss = 0.13323419\n",
      "Iteration 32, loss = 0.13318884\n",
      "Iteration 33, loss = 0.12981120\n",
      "Iteration 34, loss = 0.12767587\n",
      "Iteration 35, loss = 0.12931642\n",
      "Iteration 36, loss = 0.12344085\n",
      "Iteration 37, loss = 0.12647007\n",
      "Iteration 38, loss = 0.12467689\n",
      "Iteration 39, loss = 0.12675606\n",
      "Iteration 40, loss = 0.12105306\n",
      "Iteration 41, loss = 0.12359073\n",
      "Iteration 42, loss = 0.11999800\n",
      "Iteration 43, loss = 0.11945860\n",
      "Iteration 44, loss = 0.11770416\n",
      "Iteration 45, loss = 0.11985367\n",
      "Iteration 46, loss = 0.11818552\n",
      "Iteration 47, loss = 0.11497279\n",
      "Iteration 48, loss = 0.11587529\n",
      "Iteration 49, loss = 0.11420151\n",
      "Iteration 50, loss = 0.11579757\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kileymack/opt/anaconda3/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (50) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 6.98333308\n",
      "Iteration 2, loss = 0.51352558\n",
      "Iteration 3, loss = 0.34910836\n",
      "Iteration 4, loss = 0.29295713\n",
      "Iteration 5, loss = 0.26071124\n",
      "Iteration 6, loss = 0.24099502\n",
      "Iteration 7, loss = 0.22837668\n",
      "Iteration 8, loss = 0.21447187\n",
      "Iteration 9, loss = 0.20667003\n",
      "Iteration 10, loss = 0.19685586\n",
      "Iteration 11, loss = 0.19207279\n",
      "Iteration 12, loss = 0.18409908\n",
      "Iteration 13, loss = 0.17966659\n",
      "Iteration 14, loss = 0.17423779\n",
      "Iteration 15, loss = 0.17232710\n",
      "Iteration 16, loss = 0.16736804\n",
      "Iteration 17, loss = 0.16238996\n",
      "Iteration 18, loss = 0.16097865\n",
      "Iteration 19, loss = 0.15709287\n",
      "Iteration 20, loss = 0.15677148\n",
      "Iteration 21, loss = 0.15268156\n",
      "Iteration 22, loss = 0.14966075\n",
      "Iteration 23, loss = 0.15039261\n",
      "Iteration 24, loss = 0.14677656\n",
      "Iteration 25, loss = 0.14522288\n",
      "Iteration 26, loss = 0.14417400\n",
      "Iteration 27, loss = 0.14390213\n",
      "Iteration 28, loss = 0.14162425\n",
      "Iteration 29, loss = 0.14063132\n",
      "Iteration 30, loss = 0.13988025\n",
      "Iteration 31, loss = 0.13848355\n",
      "Iteration 32, loss = 0.13717634\n",
      "Iteration 33, loss = 0.13640866\n",
      "Iteration 34, loss = 0.13533597\n",
      "Iteration 35, loss = 0.13525121\n",
      "Iteration 36, loss = 0.13429891\n",
      "Iteration 37, loss = 0.13357625\n",
      "Iteration 38, loss = 0.13151110\n",
      "Iteration 39, loss = 0.13154310\n",
      "Iteration 40, loss = 0.13101724\n",
      "Iteration 41, loss = 0.13204253\n",
      "Iteration 42, loss = 0.13017499\n",
      "Iteration 43, loss = 0.12828244\n",
      "Iteration 44, loss = 0.12959821\n",
      "Iteration 45, loss = 0.12849962\n",
      "Iteration 46, loss = 0.12782304\n",
      "Iteration 47, loss = 0.12761791\n",
      "Iteration 48, loss = 0.12640237\n",
      "Iteration 49, loss = 0.12750071\n",
      "Iteration 50, loss = 0.12642591\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kileymack/opt/anaconda3/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (50) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 1.88797009\n",
      "Iteration 2, loss = 0.43287085\n",
      "Iteration 3, loss = 0.36058674\n",
      "Iteration 4, loss = 0.32454685\n",
      "Iteration 5, loss = 0.30169635\n",
      "Iteration 6, loss = 0.28486656\n",
      "Iteration 7, loss = 0.27193170\n",
      "Iteration 8, loss = 0.26154220\n",
      "Iteration 9, loss = 0.25241203\n",
      "Iteration 10, loss = 0.24471950\n",
      "Iteration 11, loss = 0.23730317\n",
      "Iteration 12, loss = 0.23092047\n",
      "Iteration 13, loss = 0.22560002\n",
      "Iteration 14, loss = 0.22029951\n",
      "Iteration 15, loss = 0.21568807\n",
      "Iteration 16, loss = 0.21156222\n",
      "Iteration 17, loss = 0.20737470\n",
      "Iteration 18, loss = 0.20350055\n",
      "Iteration 19, loss = 0.20053098\n",
      "Iteration 20, loss = 0.19725659\n",
      "Iteration 21, loss = 0.19402701\n",
      "Iteration 22, loss = 0.19116883\n",
      "Iteration 23, loss = 0.18809170\n",
      "Iteration 24, loss = 0.18603633\n",
      "Iteration 25, loss = 0.18385696\n",
      "Iteration 26, loss = 0.18111435\n",
      "Iteration 27, loss = 0.17839660\n",
      "Iteration 28, loss = 0.17697629\n",
      "Iteration 29, loss = 0.17484186\n",
      "Iteration 30, loss = 0.17264853\n",
      "Iteration 31, loss = 0.17057143\n",
      "Iteration 32, loss = 0.16854508\n",
      "Iteration 33, loss = 0.16731449\n",
      "Iteration 34, loss = 0.16576512\n",
      "Iteration 35, loss = 0.16367256\n",
      "Iteration 36, loss = 0.16241157\n",
      "Iteration 37, loss = 0.16067762\n",
      "Iteration 38, loss = 0.15926214\n",
      "Iteration 39, loss = 0.15791030\n",
      "Iteration 40, loss = 0.15618079\n",
      "Iteration 41, loss = 0.15522954\n",
      "Iteration 42, loss = 0.15415332\n",
      "Iteration 43, loss = 0.15296216\n",
      "Iteration 44, loss = 0.15147150\n",
      "Iteration 45, loss = 0.15074212\n",
      "Iteration 46, loss = 0.14898486\n",
      "Iteration 47, loss = 0.14828642\n",
      "Iteration 48, loss = 0.14693314\n",
      "Iteration 49, loss = 0.14552740\n",
      "Iteration 50, loss = 0.14522462\n",
      "Iteration 51, loss = 0.14389260\n",
      "Iteration 52, loss = 0.14265362\n",
      "Iteration 53, loss = 0.14155191\n",
      "Iteration 54, loss = 0.14111999\n",
      "Iteration 55, loss = 0.13973465\n",
      "Iteration 56, loss = 0.13919002\n",
      "Iteration 57, loss = 0.13811414\n",
      "Iteration 58, loss = 0.13711316\n",
      "Iteration 59, loss = 0.13648046\n",
      "Iteration 60, loss = 0.13538463\n",
      "Iteration 61, loss = 0.13498347\n",
      "Iteration 62, loss = 0.13365723\n",
      "Iteration 63, loss = 0.13322658\n",
      "Iteration 64, loss = 0.13199861\n",
      "Iteration 65, loss = 0.13190317\n",
      "Iteration 66, loss = 0.13079232\n",
      "Iteration 67, loss = 0.13012630\n",
      "Iteration 68, loss = 0.12966211\n",
      "Iteration 69, loss = 0.12832944\n",
      "Iteration 70, loss = 0.12760895\n",
      "Iteration 71, loss = 0.12721572\n",
      "Iteration 72, loss = 0.12697065\n",
      "Iteration 73, loss = 0.12574925\n",
      "Iteration 74, loss = 0.12550519\n",
      "Iteration 75, loss = 0.12495856\n",
      "Iteration 76, loss = 0.12428344\n",
      "Iteration 77, loss = 0.12378325\n",
      "Iteration 78, loss = 0.12278999\n",
      "Iteration 79, loss = 0.12222004\n",
      "Iteration 80, loss = 0.12210049\n",
      "Iteration 81, loss = 0.12095821\n",
      "Iteration 82, loss = 0.12070479\n",
      "Iteration 83, loss = 0.12022943\n",
      "Iteration 84, loss = 0.11961406\n",
      "Iteration 85, loss = 0.11881970\n",
      "Iteration 86, loss = 0.11868490\n",
      "Iteration 87, loss = 0.11836678\n",
      "Iteration 88, loss = 0.11755848\n",
      "Iteration 89, loss = 0.11694964\n",
      "Iteration 90, loss = 0.11702923\n",
      "Iteration 91, loss = 0.11610476\n",
      "Iteration 92, loss = 0.11589329\n",
      "Iteration 93, loss = 0.11473101\n",
      "Iteration 94, loss = 0.11430057\n",
      "Iteration 95, loss = 0.11416568\n",
      "Iteration 96, loss = 0.11368581\n",
      "Iteration 97, loss = 0.11366254\n",
      "Iteration 98, loss = 0.11301406\n",
      "Iteration 99, loss = 0.11283031\n",
      "Iteration 100, loss = 0.11217304\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kileymack/opt/anaconda3/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 1.70834157\n",
      "Iteration 2, loss = 0.41664995\n",
      "Iteration 3, loss = 0.35675968\n",
      "Iteration 4, loss = 0.32290167\n",
      "Iteration 5, loss = 0.30055594\n",
      "Iteration 6, loss = 0.28297419\n",
      "Iteration 7, loss = 0.26939022\n",
      "Iteration 8, loss = 0.25823469\n",
      "Iteration 9, loss = 0.24878514\n",
      "Iteration 10, loss = 0.24084426\n",
      "Iteration 11, loss = 0.23362374\n",
      "Iteration 12, loss = 0.22735069\n",
      "Iteration 13, loss = 0.22156620\n",
      "Iteration 14, loss = 0.21609837\n",
      "Iteration 15, loss = 0.21101247\n",
      "Iteration 16, loss = 0.20669202\n",
      "Iteration 17, loss = 0.20232124\n",
      "Iteration 18, loss = 0.19853092\n",
      "Iteration 19, loss = 0.19466281\n",
      "Iteration 20, loss = 0.19168414\n",
      "Iteration 21, loss = 0.18796198\n",
      "Iteration 22, loss = 0.18503314\n",
      "Iteration 23, loss = 0.18221174\n",
      "Iteration 24, loss = 0.17938611\n",
      "Iteration 25, loss = 0.17689382\n",
      "Iteration 26, loss = 0.17462097\n",
      "Iteration 27, loss = 0.17202355\n",
      "Iteration 28, loss = 0.16989190\n",
      "Iteration 29, loss = 0.16794146\n",
      "Iteration 30, loss = 0.16581192\n",
      "Iteration 31, loss = 0.16400897\n",
      "Iteration 32, loss = 0.16211747\n",
      "Iteration 33, loss = 0.16032609\n",
      "Iteration 34, loss = 0.15853343\n",
      "Iteration 35, loss = 0.15697586\n",
      "Iteration 36, loss = 0.15503797\n",
      "Iteration 37, loss = 0.15366920\n",
      "Iteration 38, loss = 0.15230721\n",
      "Iteration 39, loss = 0.15112761\n",
      "Iteration 40, loss = 0.14960099\n",
      "Iteration 41, loss = 0.14811787\n",
      "Iteration 42, loss = 0.14710477\n",
      "Iteration 43, loss = 0.14565775\n",
      "Iteration 44, loss = 0.14458675\n",
      "Iteration 45, loss = 0.14337422\n",
      "Iteration 46, loss = 0.14217568\n",
      "Iteration 47, loss = 0.14079204\n",
      "Iteration 48, loss = 0.13994068\n",
      "Iteration 49, loss = 0.13889693\n",
      "Iteration 50, loss = 0.13799314\n",
      "Iteration 51, loss = 0.13676190\n",
      "Iteration 52, loss = 0.13583445\n",
      "Iteration 53, loss = 0.13499703\n",
      "Iteration 54, loss = 0.13399130\n",
      "Iteration 55, loss = 0.13289388\n",
      "Iteration 56, loss = 0.13192216\n",
      "Iteration 57, loss = 0.13124135\n",
      "Iteration 58, loss = 0.13040945\n",
      "Iteration 59, loss = 0.12946225\n",
      "Iteration 60, loss = 0.12860959\n",
      "Iteration 61, loss = 0.12755480\n",
      "Iteration 62, loss = 0.12688224\n",
      "Iteration 63, loss = 0.12644426\n",
      "Iteration 64, loss = 0.12578549\n",
      "Iteration 65, loss = 0.12479726\n",
      "Iteration 66, loss = 0.12401675\n",
      "Iteration 67, loss = 0.12322387\n",
      "Iteration 68, loss = 0.12258537\n",
      "Iteration 69, loss = 0.12195872\n",
      "Iteration 70, loss = 0.12125151\n",
      "Iteration 71, loss = 0.12064000\n",
      "Iteration 72, loss = 0.11977844\n",
      "Iteration 73, loss = 0.11938014\n",
      "Iteration 74, loss = 0.11831559\n",
      "Iteration 75, loss = 0.11800245\n",
      "Iteration 76, loss = 0.11751668\n",
      "Iteration 77, loss = 0.11691271\n",
      "Iteration 78, loss = 0.11638578\n",
      "Iteration 79, loss = 0.11547373\n",
      "Iteration 80, loss = 0.11512910\n",
      "Iteration 81, loss = 0.11446624\n",
      "Iteration 82, loss = 0.11393622\n",
      "Iteration 83, loss = 0.11349054\n",
      "Iteration 84, loss = 0.11286204\n",
      "Iteration 85, loss = 0.11236968\n",
      "Iteration 86, loss = 0.11181904\n",
      "Iteration 87, loss = 0.11138328\n",
      "Iteration 88, loss = 0.11083219\n",
      "Iteration 89, loss = 0.11031767\n",
      "Iteration 90, loss = 0.10995526\n",
      "Iteration 91, loss = 0.10945729\n",
      "Iteration 92, loss = 0.10874106\n",
      "Iteration 93, loss = 0.10863427\n",
      "Iteration 94, loss = 0.10804986\n",
      "Iteration 95, loss = 0.10733230\n",
      "Iteration 96, loss = 0.10721880\n",
      "Iteration 97, loss = 0.10674340\n",
      "Iteration 98, loss = 0.10626984\n",
      "Iteration 99, loss = 0.10596389\n",
      "Iteration 100, loss = 0.10572157\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kileymack/opt/anaconda3/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 4.26863801\n",
      "Iteration 2, loss = 0.59056517\n",
      "Iteration 3, loss = 0.37714802\n",
      "Iteration 4, loss = 0.30672242\n",
      "Iteration 5, loss = 0.27237235\n",
      "Iteration 6, loss = 0.25235259\n",
      "Iteration 7, loss = 0.23470060\n",
      "Iteration 8, loss = 0.22280554\n",
      "Iteration 9, loss = 0.20981393\n",
      "Iteration 10, loss = 0.20437984\n",
      "Iteration 11, loss = 0.19767342\n",
      "Iteration 12, loss = 0.18804227\n",
      "Iteration 13, loss = 0.18813000\n",
      "Iteration 14, loss = 0.17524795\n",
      "Iteration 15, loss = 0.17174022\n",
      "Iteration 16, loss = 0.16937590\n",
      "Iteration 17, loss = 0.16626353\n",
      "Iteration 18, loss = 0.16347058\n",
      "Iteration 19, loss = 0.15792681\n",
      "Iteration 20, loss = 0.15838514\n",
      "Iteration 21, loss = 0.15477730\n",
      "Iteration 22, loss = 0.15033649\n",
      "Iteration 23, loss = 0.14491986\n",
      "Iteration 24, loss = 0.14792431\n",
      "Iteration 25, loss = 0.14371204\n",
      "Iteration 26, loss = 0.13975868\n",
      "Iteration 27, loss = 0.13867147\n",
      "Iteration 28, loss = 0.13544684\n",
      "Iteration 29, loss = 0.13650826\n",
      "Iteration 30, loss = 0.13512938\n",
      "Iteration 31, loss = 0.13323419\n",
      "Iteration 32, loss = 0.13318884\n",
      "Iteration 33, loss = 0.12981120\n",
      "Iteration 34, loss = 0.12767587\n",
      "Iteration 35, loss = 0.12931642\n",
      "Iteration 36, loss = 0.12344085\n",
      "Iteration 37, loss = 0.12647007\n",
      "Iteration 38, loss = 0.12467689\n",
      "Iteration 39, loss = 0.12675606\n",
      "Iteration 40, loss = 0.12105306\n",
      "Iteration 41, loss = 0.12359073\n",
      "Iteration 42, loss = 0.11999800\n",
      "Iteration 43, loss = 0.11945860\n",
      "Iteration 44, loss = 0.11770416\n",
      "Iteration 45, loss = 0.11985367\n",
      "Iteration 46, loss = 0.11818552\n",
      "Iteration 47, loss = 0.11497279\n",
      "Iteration 48, loss = 0.11587529\n",
      "Iteration 49, loss = 0.11420151\n",
      "Iteration 50, loss = 0.11579757\n",
      "Iteration 51, loss = 0.11564477\n",
      "Iteration 52, loss = 0.11592953\n",
      "Iteration 53, loss = 0.11104524\n",
      "Iteration 54, loss = 0.11407286\n",
      "Iteration 55, loss = 0.11097654\n",
      "Iteration 56, loss = 0.11298062\n",
      "Iteration 57, loss = 0.11220704\n",
      "Iteration 58, loss = 0.11039849\n",
      "Iteration 59, loss = 0.11070897\n",
      "Iteration 60, loss = 0.10945171\n",
      "Iteration 61, loss = 0.10992844\n",
      "Iteration 62, loss = 0.10870912\n",
      "Iteration 63, loss = 0.10851859\n",
      "Iteration 64, loss = 0.10695629\n",
      "Iteration 65, loss = 0.10750967\n",
      "Iteration 66, loss = 0.10814982\n",
      "Iteration 67, loss = 0.11001525\n",
      "Iteration 68, loss = 0.10513251\n",
      "Iteration 69, loss = 0.10547060\n",
      "Iteration 70, loss = 0.10555004\n",
      "Iteration 71, loss = 0.10485228\n",
      "Iteration 72, loss = 0.10798752\n",
      "Iteration 73, loss = 0.10499459\n",
      "Iteration 74, loss = 0.10592981\n",
      "Iteration 75, loss = 0.10447341\n",
      "Iteration 76, loss = 0.10709734\n",
      "Iteration 77, loss = 0.10240722\n",
      "Iteration 78, loss = 0.10495215\n",
      "Iteration 79, loss = 0.10255960\n",
      "Iteration 80, loss = 0.10432687\n",
      "Iteration 81, loss = 0.10355702\n",
      "Iteration 82, loss = 0.10261962\n",
      "Iteration 83, loss = 0.10167392\n",
      "Iteration 84, loss = 0.10136661\n",
      "Iteration 85, loss = 0.10219884\n",
      "Iteration 86, loss = 0.10151818\n",
      "Iteration 87, loss = 0.10070566\n",
      "Iteration 88, loss = 0.10183297\n",
      "Iteration 89, loss = 0.10259296\n",
      "Iteration 90, loss = 0.09963383\n",
      "Iteration 91, loss = 0.10148052\n",
      "Iteration 92, loss = 0.10196611\n",
      "Iteration 93, loss = 0.09958430\n",
      "Iteration 94, loss = 0.10016489\n",
      "Iteration 95, loss = 0.09943811\n",
      "Iteration 96, loss = 0.09948184\n",
      "Iteration 97, loss = 0.09888652\n",
      "Iteration 98, loss = 0.09883802\n",
      "Iteration 99, loss = 0.09838283\n",
      "Iteration 100, loss = 0.09812193\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kileymack/opt/anaconda3/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 6.98333308\n",
      "Iteration 2, loss = 0.51352558\n",
      "Iteration 3, loss = 0.34910836\n",
      "Iteration 4, loss = 0.29295713\n",
      "Iteration 5, loss = 0.26071124\n",
      "Iteration 6, loss = 0.24099502\n",
      "Iteration 7, loss = 0.22837668\n",
      "Iteration 8, loss = 0.21447187\n",
      "Iteration 9, loss = 0.20667003\n",
      "Iteration 10, loss = 0.19685586\n",
      "Iteration 11, loss = 0.19207279\n",
      "Iteration 12, loss = 0.18409908\n",
      "Iteration 13, loss = 0.17966659\n",
      "Iteration 14, loss = 0.17423779\n",
      "Iteration 15, loss = 0.17232710\n",
      "Iteration 16, loss = 0.16736804\n",
      "Iteration 17, loss = 0.16238996\n",
      "Iteration 18, loss = 0.16097865\n",
      "Iteration 19, loss = 0.15709287\n",
      "Iteration 20, loss = 0.15677148\n",
      "Iteration 21, loss = 0.15268156\n",
      "Iteration 22, loss = 0.14966075\n",
      "Iteration 23, loss = 0.15039261\n",
      "Iteration 24, loss = 0.14677656\n",
      "Iteration 25, loss = 0.14522288\n",
      "Iteration 26, loss = 0.14417400\n",
      "Iteration 27, loss = 0.14390213\n",
      "Iteration 28, loss = 0.14162425\n",
      "Iteration 29, loss = 0.14063132\n",
      "Iteration 30, loss = 0.13988025\n",
      "Iteration 31, loss = 0.13848355\n",
      "Iteration 32, loss = 0.13717634\n",
      "Iteration 33, loss = 0.13640866\n",
      "Iteration 34, loss = 0.13533597\n",
      "Iteration 35, loss = 0.13525121\n",
      "Iteration 36, loss = 0.13429891\n",
      "Iteration 37, loss = 0.13357625\n",
      "Iteration 38, loss = 0.13151110\n",
      "Iteration 39, loss = 0.13154310\n",
      "Iteration 40, loss = 0.13101724\n",
      "Iteration 41, loss = 0.13204253\n",
      "Iteration 42, loss = 0.13017499\n",
      "Iteration 43, loss = 0.12828244\n",
      "Iteration 44, loss = 0.12959821\n",
      "Iteration 45, loss = 0.12849962\n",
      "Iteration 46, loss = 0.12782304\n",
      "Iteration 47, loss = 0.12761791\n",
      "Iteration 48, loss = 0.12640237\n",
      "Iteration 49, loss = 0.12750071\n",
      "Iteration 50, loss = 0.12642591\n",
      "Iteration 51, loss = 0.12547329\n",
      "Iteration 52, loss = 0.12498184\n",
      "Iteration 53, loss = 0.12640292\n",
      "Iteration 54, loss = 0.12361559\n",
      "Iteration 55, loss = 0.12539609\n",
      "Iteration 56, loss = 0.12305646\n",
      "Iteration 57, loss = 0.12516036\n",
      "Iteration 58, loss = 0.12380411\n",
      "Iteration 59, loss = 0.12301564\n",
      "Iteration 60, loss = 0.12249838\n",
      "Iteration 61, loss = 0.12176402\n",
      "Iteration 62, loss = 0.12170579\n",
      "Iteration 63, loss = 0.12128212\n",
      "Iteration 64, loss = 0.12175100\n",
      "Iteration 65, loss = 0.12136277\n",
      "Iteration 66, loss = 0.12097579\n",
      "Iteration 67, loss = 0.12092329\n",
      "Iteration 68, loss = 0.11992226\n",
      "Iteration 69, loss = 0.12098226\n",
      "Iteration 70, loss = 0.12009913\n",
      "Iteration 71, loss = 0.11899435\n",
      "Iteration 72, loss = 0.11811021\n",
      "Iteration 73, loss = 0.11957464\n",
      "Iteration 74, loss = 0.11987355\n",
      "Iteration 75, loss = 0.11875374\n",
      "Iteration 76, loss = 0.11864458\n",
      "Iteration 77, loss = 0.11930846\n",
      "Iteration 78, loss = 0.11795570\n",
      "Iteration 79, loss = 0.11778611\n",
      "Iteration 80, loss = 0.11846412\n",
      "Iteration 81, loss = 0.11717941\n",
      "Iteration 82, loss = 0.11836051\n",
      "Iteration 83, loss = 0.11945823\n",
      "Iteration 84, loss = 0.11688193\n",
      "Iteration 85, loss = 0.11658763\n",
      "Iteration 86, loss = 0.11708568\n",
      "Iteration 87, loss = 0.11637632\n",
      "Iteration 88, loss = 0.11573270\n",
      "Iteration 89, loss = 0.11631380\n",
      "Iteration 90, loss = 0.11413333\n",
      "Iteration 91, loss = 0.11620884\n",
      "Iteration 92, loss = 0.11578217\n",
      "Iteration 93, loss = 0.11600406\n",
      "Iteration 94, loss = 0.11475339\n",
      "Iteration 95, loss = 0.11452428\n",
      "Iteration 96, loss = 0.11557865\n",
      "Iteration 97, loss = 0.11433510\n",
      "Iteration 98, loss = 0.11448233\n",
      "Iteration 99, loss = 0.11502757\n",
      "Iteration 100, loss = 0.11426852\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kileymack/opt/anaconda3/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 2.19752057\n",
      "Iteration 2, loss = 0.43650590\n",
      "Iteration 3, loss = 0.36216129\n",
      "Iteration 4, loss = 0.32441851\n",
      "Iteration 5, loss = 0.30158227\n",
      "Iteration 6, loss = 0.28345746\n",
      "Iteration 7, loss = 0.27004685\n",
      "Iteration 8, loss = 0.25842920\n",
      "Iteration 9, loss = 0.24958351\n",
      "Iteration 10, loss = 0.24169340\n",
      "Iteration 11, loss = 0.23408265\n",
      "Iteration 12, loss = 0.22807291\n",
      "Iteration 13, loss = 0.22256590\n",
      "Iteration 14, loss = 0.21695335\n",
      "Iteration 15, loss = 0.21276965\n",
      "Iteration 16, loss = 0.20811459\n",
      "Iteration 17, loss = 0.20422803\n",
      "Iteration 18, loss = 0.20065775\n",
      "Iteration 19, loss = 0.19697908\n",
      "Iteration 20, loss = 0.19385274\n",
      "Iteration 21, loss = 0.19081145\n",
      "Iteration 22, loss = 0.18789919\n",
      "Iteration 23, loss = 0.18531781\n",
      "Iteration 24, loss = 0.18265735\n",
      "Iteration 25, loss = 0.18046915\n",
      "Iteration 26, loss = 0.17778713\n",
      "Iteration 27, loss = 0.17570253\n",
      "Iteration 28, loss = 0.17428555\n",
      "Iteration 29, loss = 0.17160035\n",
      "Iteration 30, loss = 0.17001892\n",
      "Iteration 31, loss = 0.16780341\n",
      "Iteration 32, loss = 0.16570046\n",
      "Iteration 33, loss = 0.16395161\n",
      "Iteration 34, loss = 0.16245635\n",
      "Iteration 35, loss = 0.16097370\n",
      "Iteration 36, loss = 0.15892561\n",
      "Iteration 37, loss = 0.15785396\n",
      "Iteration 38, loss = 0.15587247\n",
      "Iteration 39, loss = 0.15458483\n",
      "Iteration 40, loss = 0.15321224\n",
      "Iteration 41, loss = 0.15176170\n",
      "Iteration 42, loss = 0.15046791\n",
      "Iteration 43, loss = 0.14903388\n",
      "Iteration 44, loss = 0.14801681\n",
      "Iteration 45, loss = 0.14668045\n",
      "Iteration 46, loss = 0.14582054\n",
      "Iteration 47, loss = 0.14442351\n",
      "Iteration 48, loss = 0.14357629\n",
      "Iteration 49, loss = 0.14220158\n",
      "Iteration 50, loss = 0.14121877\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kileymack/opt/anaconda3/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (50) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 1.91526055\n",
      "Iteration 2, loss = 0.41437999\n",
      "Iteration 3, loss = 0.35006933\n",
      "Iteration 4, loss = 0.31510125\n",
      "Iteration 5, loss = 0.29234452\n",
      "Iteration 6, loss = 0.27572617\n",
      "Iteration 7, loss = 0.26197714\n",
      "Iteration 8, loss = 0.25159081\n",
      "Iteration 9, loss = 0.24203576\n",
      "Iteration 10, loss = 0.23363136\n",
      "Iteration 11, loss = 0.22616482\n",
      "Iteration 12, loss = 0.22001827\n",
      "Iteration 13, loss = 0.21424818\n",
      "Iteration 14, loss = 0.20927762\n",
      "Iteration 15, loss = 0.20460235\n",
      "Iteration 16, loss = 0.20015659\n",
      "Iteration 17, loss = 0.19592191\n",
      "Iteration 18, loss = 0.19236500\n",
      "Iteration 19, loss = 0.18901421\n",
      "Iteration 20, loss = 0.18577055\n",
      "Iteration 21, loss = 0.18317043\n",
      "Iteration 22, loss = 0.18034158\n",
      "Iteration 23, loss = 0.17750086\n",
      "Iteration 24, loss = 0.17518052\n",
      "Iteration 25, loss = 0.17292300\n",
      "Iteration 26, loss = 0.17057943\n",
      "Iteration 27, loss = 0.16866444\n",
      "Iteration 28, loss = 0.16686346\n",
      "Iteration 29, loss = 0.16470974\n",
      "Iteration 30, loss = 0.16296940\n",
      "Iteration 31, loss = 0.16122061\n",
      "Iteration 32, loss = 0.15943727\n",
      "Iteration 33, loss = 0.15780154\n",
      "Iteration 34, loss = 0.15615979\n",
      "Iteration 35, loss = 0.15474013\n",
      "Iteration 36, loss = 0.15335778\n",
      "Iteration 37, loss = 0.15204309\n",
      "Iteration 38, loss = 0.15067066\n",
      "Iteration 39, loss = 0.14929221\n",
      "Iteration 40, loss = 0.14795063\n",
      "Iteration 41, loss = 0.14670448\n",
      "Iteration 42, loss = 0.14590366\n",
      "Iteration 43, loss = 0.14456318\n",
      "Iteration 44, loss = 0.14318918\n",
      "Iteration 45, loss = 0.14224109\n",
      "Iteration 46, loss = 0.14119096\n",
      "Iteration 47, loss = 0.14031432\n",
      "Iteration 48, loss = 0.13906097\n",
      "Iteration 49, loss = 0.13822514\n",
      "Iteration 50, loss = 0.13738344\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kileymack/opt/anaconda3/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (50) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 6.99273613\n",
      "Iteration 2, loss = 0.94416238\n",
      "Iteration 3, loss = 0.47832817\n",
      "Iteration 4, loss = 0.34526336\n",
      "Iteration 5, loss = 0.28871621\n",
      "Iteration 6, loss = 0.25567405\n",
      "Iteration 7, loss = 0.23948376\n",
      "Iteration 8, loss = 0.21952559\n",
      "Iteration 9, loss = 0.21198977\n",
      "Iteration 10, loss = 0.20438833\n",
      "Iteration 11, loss = 0.19307995\n",
      "Iteration 12, loss = 0.18672140\n",
      "Iteration 13, loss = 0.18387976\n",
      "Iteration 14, loss = 0.17729944\n",
      "Iteration 15, loss = 0.17241918\n",
      "Iteration 16, loss = 0.16883258\n",
      "Iteration 17, loss = 0.16357934\n",
      "Iteration 18, loss = 0.16093528\n",
      "Iteration 19, loss = 0.15723741\n",
      "Iteration 20, loss = 0.15434191\n",
      "Iteration 21, loss = 0.15328880\n",
      "Iteration 22, loss = 0.15014220\n",
      "Iteration 23, loss = 0.14739552\n",
      "Iteration 24, loss = 0.14467506\n",
      "Iteration 25, loss = 0.14468542\n",
      "Iteration 26, loss = 0.14035390\n",
      "Iteration 27, loss = 0.13958968\n",
      "Iteration 28, loss = 0.13763586\n",
      "Iteration 29, loss = 0.13771028\n",
      "Iteration 30, loss = 0.13611173\n",
      "Iteration 31, loss = 0.13226690\n",
      "Iteration 32, loss = 0.13161740\n",
      "Iteration 33, loss = 0.13357221\n",
      "Iteration 34, loss = 0.13100903\n",
      "Iteration 35, loss = 0.12650204\n",
      "Iteration 36, loss = 0.12809260\n",
      "Iteration 37, loss = 0.12630657\n",
      "Iteration 38, loss = 0.12711144\n",
      "Iteration 39, loss = 0.12470971\n",
      "Iteration 40, loss = 0.12468861\n",
      "Iteration 41, loss = 0.12218826\n",
      "Iteration 42, loss = 0.12102687\n",
      "Iteration 43, loss = 0.11970529\n",
      "Iteration 44, loss = 0.12162161\n",
      "Iteration 45, loss = 0.12085355\n",
      "Iteration 46, loss = 0.11860018\n",
      "Iteration 47, loss = 0.11942022\n",
      "Iteration 48, loss = 0.11838877\n",
      "Iteration 49, loss = 0.11622789\n",
      "Iteration 50, loss = 0.11793442\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kileymack/opt/anaconda3/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (50) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 11.34940845\n",
      "Iteration 2, loss = 0.69678206\n",
      "Iteration 3, loss = 0.38337223\n",
      "Iteration 4, loss = 0.30364027\n",
      "Iteration 5, loss = 0.26357662\n",
      "Iteration 6, loss = 0.24186078\n",
      "Iteration 7, loss = 0.22259698\n",
      "Iteration 8, loss = 0.21121740\n",
      "Iteration 9, loss = 0.20028035\n",
      "Iteration 10, loss = 0.19397829\n",
      "Iteration 11, loss = 0.18639175\n",
      "Iteration 12, loss = 0.18162342\n",
      "Iteration 13, loss = 0.17571535\n",
      "Iteration 14, loss = 0.17107988\n",
      "Iteration 15, loss = 0.16716617\n",
      "Iteration 16, loss = 0.16505397\n",
      "Iteration 17, loss = 0.16000325\n",
      "Iteration 18, loss = 0.15742066\n",
      "Iteration 19, loss = 0.15533558\n",
      "Iteration 20, loss = 0.15281146\n",
      "Iteration 21, loss = 0.15128399\n",
      "Iteration 22, loss = 0.14959661\n",
      "Iteration 23, loss = 0.14612884\n",
      "Iteration 24, loss = 0.14706715\n",
      "Iteration 25, loss = 0.14313202\n",
      "Iteration 26, loss = 0.14258841\n",
      "Iteration 27, loss = 0.14141872\n",
      "Iteration 28, loss = 0.13943555\n",
      "Iteration 29, loss = 0.13897427\n",
      "Iteration 30, loss = 0.13887109\n",
      "Iteration 31, loss = 0.13688722\n",
      "Iteration 32, loss = 0.13551191\n",
      "Iteration 33, loss = 0.13626682\n",
      "Iteration 34, loss = 0.13314273\n",
      "Iteration 35, loss = 0.13393123\n",
      "Iteration 36, loss = 0.13291347\n",
      "Iteration 37, loss = 0.13183118\n",
      "Iteration 38, loss = 0.13034703\n",
      "Iteration 39, loss = 0.13102813\n",
      "Iteration 40, loss = 0.13093080\n",
      "Iteration 41, loss = 0.12981843\n",
      "Iteration 42, loss = 0.13000415\n",
      "Iteration 43, loss = 0.13008236\n",
      "Iteration 44, loss = 0.12937281\n",
      "Iteration 45, loss = 0.12721784\n",
      "Iteration 46, loss = 0.12735465\n",
      "Iteration 47, loss = 0.12584254\n",
      "Iteration 48, loss = 0.12620973\n",
      "Iteration 49, loss = 0.12656476\n",
      "Iteration 50, loss = 0.12665943\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kileymack/opt/anaconda3/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (50) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 2.19752057\n",
      "Iteration 2, loss = 0.43650590\n",
      "Iteration 3, loss = 0.36216129\n",
      "Iteration 4, loss = 0.32441851\n",
      "Iteration 5, loss = 0.30158227\n",
      "Iteration 6, loss = 0.28345746\n",
      "Iteration 7, loss = 0.27004685\n",
      "Iteration 8, loss = 0.25842920\n",
      "Iteration 9, loss = 0.24958351\n",
      "Iteration 10, loss = 0.24169340\n",
      "Iteration 11, loss = 0.23408265\n",
      "Iteration 12, loss = 0.22807291\n",
      "Iteration 13, loss = 0.22256590\n",
      "Iteration 14, loss = 0.21695335\n",
      "Iteration 15, loss = 0.21276965\n",
      "Iteration 16, loss = 0.20811459\n",
      "Iteration 17, loss = 0.20422803\n",
      "Iteration 18, loss = 0.20065775\n",
      "Iteration 19, loss = 0.19697908\n",
      "Iteration 20, loss = 0.19385274\n",
      "Iteration 21, loss = 0.19081145\n",
      "Iteration 22, loss = 0.18789919\n",
      "Iteration 23, loss = 0.18531781\n",
      "Iteration 24, loss = 0.18265735\n",
      "Iteration 25, loss = 0.18046915\n",
      "Iteration 26, loss = 0.17778713\n",
      "Iteration 27, loss = 0.17570253\n",
      "Iteration 28, loss = 0.17428555\n",
      "Iteration 29, loss = 0.17160035\n",
      "Iteration 30, loss = 0.17001892\n",
      "Iteration 31, loss = 0.16780341\n",
      "Iteration 32, loss = 0.16570046\n",
      "Iteration 33, loss = 0.16395161\n",
      "Iteration 34, loss = 0.16245635\n",
      "Iteration 35, loss = 0.16097370\n",
      "Iteration 36, loss = 0.15892561\n",
      "Iteration 37, loss = 0.15785396\n",
      "Iteration 38, loss = 0.15587247\n",
      "Iteration 39, loss = 0.15458483\n",
      "Iteration 40, loss = 0.15321224\n",
      "Iteration 41, loss = 0.15176170\n",
      "Iteration 42, loss = 0.15046791\n",
      "Iteration 43, loss = 0.14903388\n",
      "Iteration 44, loss = 0.14801681\n",
      "Iteration 45, loss = 0.14668045\n",
      "Iteration 46, loss = 0.14582054\n",
      "Iteration 47, loss = 0.14442351\n",
      "Iteration 48, loss = 0.14357629\n",
      "Iteration 49, loss = 0.14220158\n",
      "Iteration 50, loss = 0.14121877\n",
      "Iteration 51, loss = 0.14002149\n",
      "Iteration 52, loss = 0.13902780\n",
      "Iteration 53, loss = 0.13783768\n",
      "Iteration 54, loss = 0.13703490\n",
      "Iteration 55, loss = 0.13627452\n",
      "Iteration 56, loss = 0.13505738\n",
      "Iteration 57, loss = 0.13426273\n",
      "Iteration 58, loss = 0.13325061\n",
      "Iteration 59, loss = 0.13261218\n",
      "Iteration 60, loss = 0.13178356\n",
      "Iteration 61, loss = 0.13087704\n",
      "Iteration 62, loss = 0.13025348\n",
      "Iteration 63, loss = 0.12929000\n",
      "Iteration 64, loss = 0.12856435\n",
      "Iteration 65, loss = 0.12757243\n",
      "Iteration 66, loss = 0.12717817\n",
      "Iteration 67, loss = 0.12613512\n",
      "Iteration 68, loss = 0.12548217\n",
      "Iteration 69, loss = 0.12472787\n",
      "Iteration 70, loss = 0.12422749\n",
      "Iteration 71, loss = 0.12335648\n",
      "Iteration 72, loss = 0.12265726\n",
      "Iteration 73, loss = 0.12244063\n",
      "Iteration 74, loss = 0.12156670\n",
      "Iteration 75, loss = 0.12108456\n",
      "Iteration 76, loss = 0.12057083\n",
      "Iteration 77, loss = 0.11968788\n",
      "Iteration 78, loss = 0.11891563\n",
      "Iteration 79, loss = 0.11857500\n",
      "Iteration 80, loss = 0.11809162\n",
      "Iteration 81, loss = 0.11739889\n",
      "Iteration 82, loss = 0.11700783\n",
      "Iteration 83, loss = 0.11659478\n",
      "Iteration 84, loss = 0.11581971\n",
      "Iteration 85, loss = 0.11500474\n",
      "Iteration 86, loss = 0.11462671\n",
      "Iteration 87, loss = 0.11411776\n",
      "Iteration 88, loss = 0.11398683\n",
      "Iteration 89, loss = 0.11308006\n",
      "Iteration 90, loss = 0.11281485\n",
      "Iteration 91, loss = 0.11238969\n",
      "Iteration 92, loss = 0.11172284\n",
      "Iteration 93, loss = 0.11142361\n",
      "Iteration 94, loss = 0.11099765\n",
      "Iteration 95, loss = 0.11079498\n",
      "Iteration 96, loss = 0.11032575\n",
      "Iteration 97, loss = 0.10968040\n",
      "Iteration 98, loss = 0.10905570\n",
      "Iteration 99, loss = 0.10846479\n",
      "Iteration 100, loss = 0.10817695\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kileymack/opt/anaconda3/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 1.91526055\n",
      "Iteration 2, loss = 0.41437999\n",
      "Iteration 3, loss = 0.35006933\n",
      "Iteration 4, loss = 0.31510125\n",
      "Iteration 5, loss = 0.29234452\n",
      "Iteration 6, loss = 0.27572617\n",
      "Iteration 7, loss = 0.26197714\n",
      "Iteration 8, loss = 0.25159081\n",
      "Iteration 9, loss = 0.24203576\n",
      "Iteration 10, loss = 0.23363136\n",
      "Iteration 11, loss = 0.22616482\n",
      "Iteration 12, loss = 0.22001827\n",
      "Iteration 13, loss = 0.21424818\n",
      "Iteration 14, loss = 0.20927762\n",
      "Iteration 15, loss = 0.20460235\n",
      "Iteration 16, loss = 0.20015659\n",
      "Iteration 17, loss = 0.19592191\n",
      "Iteration 18, loss = 0.19236500\n",
      "Iteration 19, loss = 0.18901421\n",
      "Iteration 20, loss = 0.18577055\n",
      "Iteration 21, loss = 0.18317043\n",
      "Iteration 22, loss = 0.18034158\n",
      "Iteration 23, loss = 0.17750086\n",
      "Iteration 24, loss = 0.17518052\n",
      "Iteration 25, loss = 0.17292300\n",
      "Iteration 26, loss = 0.17057943\n",
      "Iteration 27, loss = 0.16866444\n",
      "Iteration 28, loss = 0.16686346\n",
      "Iteration 29, loss = 0.16470974\n",
      "Iteration 30, loss = 0.16296940\n",
      "Iteration 31, loss = 0.16122061\n",
      "Iteration 32, loss = 0.15943727\n",
      "Iteration 33, loss = 0.15780154\n",
      "Iteration 34, loss = 0.15615979\n",
      "Iteration 35, loss = 0.15474013\n",
      "Iteration 36, loss = 0.15335778\n",
      "Iteration 37, loss = 0.15204309\n",
      "Iteration 38, loss = 0.15067066\n",
      "Iteration 39, loss = 0.14929221\n",
      "Iteration 40, loss = 0.14795063\n",
      "Iteration 41, loss = 0.14670448\n",
      "Iteration 42, loss = 0.14590366\n",
      "Iteration 43, loss = 0.14456318\n",
      "Iteration 44, loss = 0.14318918\n",
      "Iteration 45, loss = 0.14224109\n",
      "Iteration 46, loss = 0.14119096\n",
      "Iteration 47, loss = 0.14031432\n",
      "Iteration 48, loss = 0.13906097\n",
      "Iteration 49, loss = 0.13822514\n",
      "Iteration 50, loss = 0.13738344\n",
      "Iteration 51, loss = 0.13627948\n",
      "Iteration 52, loss = 0.13528798\n",
      "Iteration 53, loss = 0.13462751\n",
      "Iteration 54, loss = 0.13356953\n",
      "Iteration 55, loss = 0.13273444\n",
      "Iteration 56, loss = 0.13200063\n",
      "Iteration 57, loss = 0.13116002\n",
      "Iteration 58, loss = 0.13054087\n",
      "Iteration 59, loss = 0.12962506\n",
      "Iteration 60, loss = 0.12861281\n",
      "Iteration 61, loss = 0.12824842\n",
      "Iteration 62, loss = 0.12729587\n",
      "Iteration 63, loss = 0.12658787\n",
      "Iteration 64, loss = 0.12580429\n",
      "Iteration 65, loss = 0.12524593\n",
      "Iteration 66, loss = 0.12430554\n",
      "Iteration 67, loss = 0.12365528\n",
      "Iteration 68, loss = 0.12313499\n",
      "Iteration 69, loss = 0.12245516\n",
      "Iteration 70, loss = 0.12205615\n",
      "Iteration 71, loss = 0.12120436\n",
      "Iteration 72, loss = 0.12066392\n",
      "Iteration 73, loss = 0.11995968\n",
      "Iteration 74, loss = 0.11951455\n",
      "Iteration 75, loss = 0.11902257\n",
      "Iteration 76, loss = 0.11820548\n",
      "Iteration 77, loss = 0.11766906\n",
      "Iteration 78, loss = 0.11716562\n",
      "Iteration 79, loss = 0.11663798\n",
      "Iteration 80, loss = 0.11625556\n",
      "Iteration 81, loss = 0.11574332\n",
      "Iteration 82, loss = 0.11520823\n",
      "Iteration 83, loss = 0.11459792\n",
      "Iteration 84, loss = 0.11416214\n",
      "Iteration 85, loss = 0.11370953\n",
      "Iteration 86, loss = 0.11331081\n",
      "Iteration 87, loss = 0.11253578\n",
      "Iteration 88, loss = 0.11247207\n",
      "Iteration 89, loss = 0.11198747\n",
      "Iteration 90, loss = 0.11139502\n",
      "Iteration 91, loss = 0.11086285\n",
      "Iteration 92, loss = 0.11042379\n",
      "Iteration 93, loss = 0.11003364\n",
      "Iteration 94, loss = 0.10945472\n",
      "Iteration 95, loss = 0.10915873\n",
      "Iteration 96, loss = 0.10883390\n",
      "Iteration 97, loss = 0.10828028\n",
      "Iteration 98, loss = 0.10827989\n",
      "Iteration 99, loss = 0.10753939\n",
      "Iteration 100, loss = 0.10723173\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kileymack/opt/anaconda3/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 6.99273613\n",
      "Iteration 2, loss = 0.94416238\n",
      "Iteration 3, loss = 0.47832817\n",
      "Iteration 4, loss = 0.34526336\n",
      "Iteration 5, loss = 0.28871621\n",
      "Iteration 6, loss = 0.25567405\n",
      "Iteration 7, loss = 0.23948376\n",
      "Iteration 8, loss = 0.21952559\n",
      "Iteration 9, loss = 0.21198977\n",
      "Iteration 10, loss = 0.20438833\n",
      "Iteration 11, loss = 0.19307995\n",
      "Iteration 12, loss = 0.18672140\n",
      "Iteration 13, loss = 0.18387976\n",
      "Iteration 14, loss = 0.17729944\n",
      "Iteration 15, loss = 0.17241918\n",
      "Iteration 16, loss = 0.16883258\n",
      "Iteration 17, loss = 0.16357934\n",
      "Iteration 18, loss = 0.16093528\n",
      "Iteration 19, loss = 0.15723741\n",
      "Iteration 20, loss = 0.15434191\n",
      "Iteration 21, loss = 0.15328880\n",
      "Iteration 22, loss = 0.15014220\n",
      "Iteration 23, loss = 0.14739552\n",
      "Iteration 24, loss = 0.14467506\n",
      "Iteration 25, loss = 0.14468542\n",
      "Iteration 26, loss = 0.14035390\n",
      "Iteration 27, loss = 0.13958968\n",
      "Iteration 28, loss = 0.13763586\n",
      "Iteration 29, loss = 0.13771028\n",
      "Iteration 30, loss = 0.13611173\n",
      "Iteration 31, loss = 0.13226690\n",
      "Iteration 32, loss = 0.13161740\n",
      "Iteration 33, loss = 0.13357221\n",
      "Iteration 34, loss = 0.13100903\n",
      "Iteration 35, loss = 0.12650204\n",
      "Iteration 36, loss = 0.12809260\n",
      "Iteration 37, loss = 0.12630657\n",
      "Iteration 38, loss = 0.12711144\n",
      "Iteration 39, loss = 0.12470971\n",
      "Iteration 40, loss = 0.12468861\n",
      "Iteration 41, loss = 0.12218826\n",
      "Iteration 42, loss = 0.12102687\n",
      "Iteration 43, loss = 0.11970529\n",
      "Iteration 44, loss = 0.12162161\n",
      "Iteration 45, loss = 0.12085355\n",
      "Iteration 46, loss = 0.11860018\n",
      "Iteration 47, loss = 0.11942022\n",
      "Iteration 48, loss = 0.11838877\n",
      "Iteration 49, loss = 0.11622789\n",
      "Iteration 50, loss = 0.11793442\n",
      "Iteration 51, loss = 0.11475194\n",
      "Iteration 52, loss = 0.11620370\n",
      "Iteration 53, loss = 0.11498461\n",
      "Iteration 54, loss = 0.11367444\n",
      "Iteration 55, loss = 0.11471964\n",
      "Iteration 56, loss = 0.11252666\n",
      "Iteration 57, loss = 0.11200452\n",
      "Iteration 58, loss = 0.11210611\n",
      "Iteration 59, loss = 0.11206405\n",
      "Iteration 60, loss = 0.11349318\n",
      "Iteration 61, loss = 0.11014557\n",
      "Iteration 62, loss = 0.11144671\n",
      "Iteration 63, loss = 0.10899291\n",
      "Iteration 64, loss = 0.10812678\n",
      "Iteration 65, loss = 0.10924955\n",
      "Iteration 66, loss = 0.10887365\n",
      "Iteration 67, loss = 0.10865296\n",
      "Iteration 68, loss = 0.10862152\n",
      "Iteration 69, loss = 0.10801712\n",
      "Iteration 70, loss = 0.10790558\n",
      "Iteration 71, loss = 0.10907907\n",
      "Iteration 72, loss = 0.10600409\n",
      "Iteration 73, loss = 0.10888163\n",
      "Iteration 74, loss = 0.10571192\n",
      "Iteration 75, loss = 0.10706909\n",
      "Iteration 76, loss = 0.10546160\n",
      "Iteration 77, loss = 0.10619331\n",
      "Iteration 78, loss = 0.10413387\n",
      "Iteration 79, loss = 0.10618903\n",
      "Iteration 80, loss = 0.10403017\n",
      "Iteration 81, loss = 0.10371451\n",
      "Iteration 82, loss = 0.10505001\n",
      "Iteration 83, loss = 0.10350431\n",
      "Iteration 84, loss = 0.10523942\n",
      "Iteration 85, loss = 0.10363855\n",
      "Iteration 86, loss = 0.10531750\n",
      "Iteration 87, loss = 0.10198683\n",
      "Iteration 88, loss = 0.10323193\n",
      "Iteration 89, loss = 0.10370973\n",
      "Iteration 90, loss = 0.10367910\n",
      "Iteration 91, loss = 0.10129997\n",
      "Iteration 92, loss = 0.10317309\n",
      "Iteration 93, loss = 0.10265892\n",
      "Iteration 94, loss = 0.10249940\n",
      "Iteration 95, loss = 0.10298478\n",
      "Iteration 96, loss = 0.10238679\n",
      "Iteration 97, loss = 0.10017768\n",
      "Iteration 98, loss = 0.10086845\n",
      "Iteration 99, loss = 0.10369971\n",
      "Iteration 100, loss = 0.10080547\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kileymack/opt/anaconda3/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 11.34940845\n",
      "Iteration 2, loss = 0.69678206\n",
      "Iteration 3, loss = 0.38337223\n",
      "Iteration 4, loss = 0.30364027\n",
      "Iteration 5, loss = 0.26357662\n",
      "Iteration 6, loss = 0.24186078\n",
      "Iteration 7, loss = 0.22259698\n",
      "Iteration 8, loss = 0.21121740\n",
      "Iteration 9, loss = 0.20028035\n",
      "Iteration 10, loss = 0.19397829\n",
      "Iteration 11, loss = 0.18639175\n",
      "Iteration 12, loss = 0.18162342\n",
      "Iteration 13, loss = 0.17571535\n",
      "Iteration 14, loss = 0.17107988\n",
      "Iteration 15, loss = 0.16716617\n",
      "Iteration 16, loss = 0.16505397\n",
      "Iteration 17, loss = 0.16000325\n",
      "Iteration 18, loss = 0.15742066\n",
      "Iteration 19, loss = 0.15533558\n",
      "Iteration 20, loss = 0.15281146\n",
      "Iteration 21, loss = 0.15128399\n",
      "Iteration 22, loss = 0.14959661\n",
      "Iteration 23, loss = 0.14612884\n",
      "Iteration 24, loss = 0.14706715\n",
      "Iteration 25, loss = 0.14313202\n",
      "Iteration 26, loss = 0.14258841\n",
      "Iteration 27, loss = 0.14141872\n",
      "Iteration 28, loss = 0.13943555\n",
      "Iteration 29, loss = 0.13897427\n",
      "Iteration 30, loss = 0.13887109\n",
      "Iteration 31, loss = 0.13688722\n",
      "Iteration 32, loss = 0.13551191\n",
      "Iteration 33, loss = 0.13626682\n",
      "Iteration 34, loss = 0.13314273\n",
      "Iteration 35, loss = 0.13393123\n",
      "Iteration 36, loss = 0.13291347\n",
      "Iteration 37, loss = 0.13183118\n",
      "Iteration 38, loss = 0.13034703\n",
      "Iteration 39, loss = 0.13102813\n",
      "Iteration 40, loss = 0.13093080\n",
      "Iteration 41, loss = 0.12981843\n",
      "Iteration 42, loss = 0.13000415\n",
      "Iteration 43, loss = 0.13008236\n",
      "Iteration 44, loss = 0.12937281\n",
      "Iteration 45, loss = 0.12721784\n",
      "Iteration 46, loss = 0.12735465\n",
      "Iteration 47, loss = 0.12584254\n",
      "Iteration 48, loss = 0.12620973\n",
      "Iteration 49, loss = 0.12656476\n",
      "Iteration 50, loss = 0.12665943\n",
      "Iteration 51, loss = 0.12508486\n",
      "Iteration 52, loss = 0.12371991\n",
      "Iteration 53, loss = 0.12524351\n",
      "Iteration 54, loss = 0.12293627\n",
      "Iteration 55, loss = 0.12368177\n",
      "Iteration 56, loss = 0.12306116\n",
      "Iteration 57, loss = 0.12200684\n",
      "Iteration 58, loss = 0.12395582\n",
      "Iteration 59, loss = 0.12447696\n",
      "Iteration 60, loss = 0.12123020\n",
      "Iteration 61, loss = 0.12146108\n",
      "Iteration 62, loss = 0.12090057\n",
      "Iteration 63, loss = 0.12070081\n",
      "Iteration 64, loss = 0.12056835\n",
      "Iteration 65, loss = 0.11981051\n",
      "Iteration 66, loss = 0.12090949\n",
      "Iteration 67, loss = 0.11864294\n",
      "Iteration 68, loss = 0.11988381\n",
      "Iteration 69, loss = 0.12032687\n",
      "Iteration 70, loss = 0.11990392\n",
      "Iteration 71, loss = 0.11827205\n",
      "Iteration 72, loss = 0.11903976\n",
      "Iteration 73, loss = 0.11848323\n",
      "Iteration 74, loss = 0.11771380\n",
      "Iteration 75, loss = 0.11717762\n",
      "Iteration 76, loss = 0.11867096\n",
      "Iteration 77, loss = 0.11806373\n",
      "Iteration 78, loss = 0.11743061\n",
      "Iteration 79, loss = 0.11783092\n",
      "Iteration 80, loss = 0.11563993\n",
      "Iteration 81, loss = 0.11823012\n",
      "Iteration 82, loss = 0.11497765\n",
      "Iteration 83, loss = 0.11793621\n",
      "Iteration 84, loss = 0.11639057\n",
      "Iteration 85, loss = 0.11728108\n",
      "Iteration 86, loss = 0.11600320\n",
      "Iteration 87, loss = 0.11552909\n",
      "Iteration 88, loss = 0.11566141\n",
      "Iteration 89, loss = 0.11569000\n",
      "Iteration 90, loss = 0.11600499\n",
      "Iteration 91, loss = 0.11594604\n",
      "Iteration 92, loss = 0.11398283\n",
      "Iteration 93, loss = 0.11463311\n",
      "Iteration 94, loss = 0.11465790\n",
      "Iteration 95, loss = 0.11448524\n",
      "Iteration 96, loss = 0.11357303\n",
      "Iteration 97, loss = 0.11433403\n",
      "Iteration 98, loss = 0.11456559\n",
      "Iteration 99, loss = 0.11446041\n",
      "Iteration 100, loss = 0.11321674\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kileymack/opt/anaconda3/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 2.19752057\n",
      "Iteration 2, loss = 0.43650590\n",
      "Iteration 3, loss = 0.36216129\n",
      "Iteration 4, loss = 0.32441851\n",
      "Iteration 5, loss = 0.30158227\n",
      "Iteration 6, loss = 0.28345746\n",
      "Iteration 7, loss = 0.27004685\n",
      "Iteration 8, loss = 0.25842920\n",
      "Iteration 9, loss = 0.24958351\n",
      "Iteration 10, loss = 0.24169340\n",
      "Iteration 11, loss = 0.23408265\n",
      "Iteration 12, loss = 0.22807291\n",
      "Iteration 13, loss = 0.22256590\n",
      "Iteration 14, loss = 0.21695335\n",
      "Iteration 15, loss = 0.21276965\n",
      "Iteration 16, loss = 0.20811459\n",
      "Iteration 17, loss = 0.20422803\n",
      "Iteration 18, loss = 0.20065775\n",
      "Iteration 19, loss = 0.19697908\n",
      "Iteration 20, loss = 0.19385274\n",
      "Iteration 21, loss = 0.19081145\n",
      "Iteration 22, loss = 0.18789919\n",
      "Iteration 23, loss = 0.18531781\n",
      "Iteration 24, loss = 0.18265735\n",
      "Iteration 25, loss = 0.18046915\n",
      "Iteration 26, loss = 0.17778713\n",
      "Iteration 27, loss = 0.17570253\n",
      "Iteration 28, loss = 0.17428555\n",
      "Iteration 29, loss = 0.17160035\n",
      "Iteration 30, loss = 0.17001892\n",
      "Iteration 31, loss = 0.16780341\n",
      "Iteration 32, loss = 0.16570046\n",
      "Iteration 33, loss = 0.16395161\n",
      "Iteration 34, loss = 0.16245635\n",
      "Iteration 35, loss = 0.16097370\n",
      "Iteration 36, loss = 0.15892561\n",
      "Iteration 37, loss = 0.15785396\n",
      "Iteration 38, loss = 0.15587247\n",
      "Iteration 39, loss = 0.15458483\n",
      "Iteration 40, loss = 0.15321224\n",
      "Iteration 41, loss = 0.15176170\n",
      "Iteration 42, loss = 0.15046791\n",
      "Iteration 43, loss = 0.14903388\n",
      "Iteration 44, loss = 0.14801681\n",
      "Iteration 45, loss = 0.14668045\n",
      "Iteration 46, loss = 0.14582054\n",
      "Iteration 47, loss = 0.14442351\n",
      "Iteration 48, loss = 0.14357629\n",
      "Iteration 49, loss = 0.14220158\n",
      "Iteration 50, loss = 0.14121877\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kileymack/opt/anaconda3/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (50) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 1.91526055\n",
      "Iteration 2, loss = 0.41437999\n",
      "Iteration 3, loss = 0.35006933\n",
      "Iteration 4, loss = 0.31510125\n",
      "Iteration 5, loss = 0.29234452\n",
      "Iteration 6, loss = 0.27572617\n",
      "Iteration 7, loss = 0.26197714\n",
      "Iteration 8, loss = 0.25159081\n",
      "Iteration 9, loss = 0.24203576\n",
      "Iteration 10, loss = 0.23363136\n",
      "Iteration 11, loss = 0.22616482\n",
      "Iteration 12, loss = 0.22001827\n",
      "Iteration 13, loss = 0.21424818\n",
      "Iteration 14, loss = 0.20927762\n",
      "Iteration 15, loss = 0.20460235\n",
      "Iteration 16, loss = 0.20015659\n",
      "Iteration 17, loss = 0.19592191\n",
      "Iteration 18, loss = 0.19236500\n",
      "Iteration 19, loss = 0.18901421\n",
      "Iteration 20, loss = 0.18577055\n",
      "Iteration 21, loss = 0.18317043\n",
      "Iteration 22, loss = 0.18034158\n",
      "Iteration 23, loss = 0.17750086\n",
      "Iteration 24, loss = 0.17518052\n",
      "Iteration 25, loss = 0.17292300\n",
      "Iteration 26, loss = 0.17057943\n",
      "Iteration 27, loss = 0.16866444\n",
      "Iteration 28, loss = 0.16686346\n",
      "Iteration 29, loss = 0.16470974\n",
      "Iteration 30, loss = 0.16296940\n",
      "Iteration 31, loss = 0.16122061\n",
      "Iteration 32, loss = 0.15943727\n",
      "Iteration 33, loss = 0.15780154\n",
      "Iteration 34, loss = 0.15615979\n",
      "Iteration 35, loss = 0.15474013\n",
      "Iteration 36, loss = 0.15335778\n",
      "Iteration 37, loss = 0.15204309\n",
      "Iteration 38, loss = 0.15067066\n",
      "Iteration 39, loss = 0.14929221\n",
      "Iteration 40, loss = 0.14795063\n",
      "Iteration 41, loss = 0.14670448\n",
      "Iteration 42, loss = 0.14590366\n",
      "Iteration 43, loss = 0.14456318\n",
      "Iteration 44, loss = 0.14318918\n",
      "Iteration 45, loss = 0.14224109\n",
      "Iteration 46, loss = 0.14119096\n",
      "Iteration 47, loss = 0.14031432\n",
      "Iteration 48, loss = 0.13906097\n",
      "Iteration 49, loss = 0.13822514\n",
      "Iteration 50, loss = 0.13738344\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kileymack/opt/anaconda3/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (50) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 6.99273613\n",
      "Iteration 2, loss = 0.94416238\n",
      "Iteration 3, loss = 0.47832817\n",
      "Iteration 4, loss = 0.34526336\n",
      "Iteration 5, loss = 0.28871621\n",
      "Iteration 6, loss = 0.25567405\n",
      "Iteration 7, loss = 0.23948376\n",
      "Iteration 8, loss = 0.21952559\n",
      "Iteration 9, loss = 0.21198977\n",
      "Iteration 10, loss = 0.20438833\n",
      "Iteration 11, loss = 0.19307995\n",
      "Iteration 12, loss = 0.18672140\n",
      "Iteration 13, loss = 0.18387976\n",
      "Iteration 14, loss = 0.17729944\n",
      "Iteration 15, loss = 0.17241918\n",
      "Iteration 16, loss = 0.16883258\n",
      "Iteration 17, loss = 0.16357934\n",
      "Iteration 18, loss = 0.16093528\n",
      "Iteration 19, loss = 0.15723741\n",
      "Iteration 20, loss = 0.15434191\n",
      "Iteration 21, loss = 0.15328880\n",
      "Iteration 22, loss = 0.15014220\n",
      "Iteration 23, loss = 0.14739552\n",
      "Iteration 24, loss = 0.14467506\n",
      "Iteration 25, loss = 0.14468542\n",
      "Iteration 26, loss = 0.14035390\n",
      "Iteration 27, loss = 0.13958968\n",
      "Iteration 28, loss = 0.13763586\n",
      "Iteration 29, loss = 0.13771028\n",
      "Iteration 30, loss = 0.13611173\n",
      "Iteration 31, loss = 0.13226690\n",
      "Iteration 32, loss = 0.13161740\n",
      "Iteration 33, loss = 0.13357221\n",
      "Iteration 34, loss = 0.13100903\n",
      "Iteration 35, loss = 0.12650204\n",
      "Iteration 36, loss = 0.12809260\n",
      "Iteration 37, loss = 0.12630657\n",
      "Iteration 38, loss = 0.12711144\n",
      "Iteration 39, loss = 0.12470971\n",
      "Iteration 40, loss = 0.12468861\n",
      "Iteration 41, loss = 0.12218826\n",
      "Iteration 42, loss = 0.12102687\n",
      "Iteration 43, loss = 0.11970529\n",
      "Iteration 44, loss = 0.12162161\n",
      "Iteration 45, loss = 0.12085355\n",
      "Iteration 46, loss = 0.11860018\n",
      "Iteration 47, loss = 0.11942022\n",
      "Iteration 48, loss = 0.11838877\n",
      "Iteration 49, loss = 0.11622789\n",
      "Iteration 50, loss = 0.11793442\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kileymack/opt/anaconda3/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (50) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 11.34940845\n",
      "Iteration 2, loss = 0.69678206\n",
      "Iteration 3, loss = 0.38337223\n",
      "Iteration 4, loss = 0.30364027\n",
      "Iteration 5, loss = 0.26357662\n",
      "Iteration 6, loss = 0.24186078\n",
      "Iteration 7, loss = 0.22259698\n",
      "Iteration 8, loss = 0.21121740\n",
      "Iteration 9, loss = 0.20028035\n",
      "Iteration 10, loss = 0.19397829\n",
      "Iteration 11, loss = 0.18639175\n",
      "Iteration 12, loss = 0.18162342\n",
      "Iteration 13, loss = 0.17571535\n",
      "Iteration 14, loss = 0.17107988\n",
      "Iteration 15, loss = 0.16716617\n",
      "Iteration 16, loss = 0.16505397\n",
      "Iteration 17, loss = 0.16000325\n",
      "Iteration 18, loss = 0.15742066\n",
      "Iteration 19, loss = 0.15533558\n",
      "Iteration 20, loss = 0.15281146\n",
      "Iteration 21, loss = 0.15128399\n",
      "Iteration 22, loss = 0.14959661\n",
      "Iteration 23, loss = 0.14612884\n",
      "Iteration 24, loss = 0.14706715\n",
      "Iteration 25, loss = 0.14313202\n",
      "Iteration 26, loss = 0.14258841\n",
      "Iteration 27, loss = 0.14141872\n",
      "Iteration 28, loss = 0.13943555\n",
      "Iteration 29, loss = 0.13897427\n",
      "Iteration 30, loss = 0.13887109\n",
      "Iteration 31, loss = 0.13688722\n",
      "Iteration 32, loss = 0.13551191\n",
      "Iteration 33, loss = 0.13626682\n",
      "Iteration 34, loss = 0.13314273\n",
      "Iteration 35, loss = 0.13393123\n",
      "Iteration 36, loss = 0.13291347\n",
      "Iteration 37, loss = 0.13183118\n",
      "Iteration 38, loss = 0.13034703\n",
      "Iteration 39, loss = 0.13102813\n",
      "Iteration 40, loss = 0.13093080\n",
      "Iteration 41, loss = 0.12981843\n",
      "Iteration 42, loss = 0.13000415\n",
      "Iteration 43, loss = 0.13008236\n",
      "Iteration 44, loss = 0.12937281\n",
      "Iteration 45, loss = 0.12721784\n",
      "Iteration 46, loss = 0.12735465\n",
      "Iteration 47, loss = 0.12584254\n",
      "Iteration 48, loss = 0.12620973\n",
      "Iteration 49, loss = 0.12656476\n",
      "Iteration 50, loss = 0.12665943\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kileymack/opt/anaconda3/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (50) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 2.19752057\n",
      "Iteration 2, loss = 0.43650590\n",
      "Iteration 3, loss = 0.36216129\n",
      "Iteration 4, loss = 0.32441851\n",
      "Iteration 5, loss = 0.30158227\n",
      "Iteration 6, loss = 0.28345746\n",
      "Iteration 7, loss = 0.27004685\n",
      "Iteration 8, loss = 0.25842920\n",
      "Iteration 9, loss = 0.24958351\n",
      "Iteration 10, loss = 0.24169340\n",
      "Iteration 11, loss = 0.23408265\n",
      "Iteration 12, loss = 0.22807291\n",
      "Iteration 13, loss = 0.22256590\n",
      "Iteration 14, loss = 0.21695335\n",
      "Iteration 15, loss = 0.21276965\n",
      "Iteration 16, loss = 0.20811459\n",
      "Iteration 17, loss = 0.20422803\n",
      "Iteration 18, loss = 0.20065775\n",
      "Iteration 19, loss = 0.19697908\n",
      "Iteration 20, loss = 0.19385274\n",
      "Iteration 21, loss = 0.19081145\n",
      "Iteration 22, loss = 0.18789919\n",
      "Iteration 23, loss = 0.18531781\n",
      "Iteration 24, loss = 0.18265735\n",
      "Iteration 25, loss = 0.18046915\n",
      "Iteration 26, loss = 0.17778713\n",
      "Iteration 27, loss = 0.17570253\n",
      "Iteration 28, loss = 0.17428555\n",
      "Iteration 29, loss = 0.17160035\n",
      "Iteration 30, loss = 0.17001892\n",
      "Iteration 31, loss = 0.16780341\n",
      "Iteration 32, loss = 0.16570046\n",
      "Iteration 33, loss = 0.16395161\n",
      "Iteration 34, loss = 0.16245635\n",
      "Iteration 35, loss = 0.16097370\n",
      "Iteration 36, loss = 0.15892561\n",
      "Iteration 37, loss = 0.15785396\n",
      "Iteration 38, loss = 0.15587247\n",
      "Iteration 39, loss = 0.15458483\n",
      "Iteration 40, loss = 0.15321224\n",
      "Iteration 41, loss = 0.15176170\n",
      "Iteration 42, loss = 0.15046791\n",
      "Iteration 43, loss = 0.14903388\n",
      "Iteration 44, loss = 0.14801681\n",
      "Iteration 45, loss = 0.14668045\n",
      "Iteration 46, loss = 0.14582054\n",
      "Iteration 47, loss = 0.14442351\n",
      "Iteration 48, loss = 0.14357629\n",
      "Iteration 49, loss = 0.14220158\n",
      "Iteration 50, loss = 0.14121877\n",
      "Iteration 51, loss = 0.14002149\n",
      "Iteration 52, loss = 0.13902780\n",
      "Iteration 53, loss = 0.13783768\n",
      "Iteration 54, loss = 0.13703490\n",
      "Iteration 55, loss = 0.13627452\n",
      "Iteration 56, loss = 0.13505738\n",
      "Iteration 57, loss = 0.13426273\n",
      "Iteration 58, loss = 0.13325061\n",
      "Iteration 59, loss = 0.13261218\n",
      "Iteration 60, loss = 0.13178356\n",
      "Iteration 61, loss = 0.13087704\n",
      "Iteration 62, loss = 0.13025348\n",
      "Iteration 63, loss = 0.12929000\n",
      "Iteration 64, loss = 0.12856435\n",
      "Iteration 65, loss = 0.12757243\n",
      "Iteration 66, loss = 0.12717817\n",
      "Iteration 67, loss = 0.12613512\n",
      "Iteration 68, loss = 0.12548217\n",
      "Iteration 69, loss = 0.12472787\n",
      "Iteration 70, loss = 0.12422749\n",
      "Iteration 71, loss = 0.12335648\n",
      "Iteration 72, loss = 0.12265726\n",
      "Iteration 73, loss = 0.12244063\n",
      "Iteration 74, loss = 0.12156670\n",
      "Iteration 75, loss = 0.12108456\n",
      "Iteration 76, loss = 0.12057083\n",
      "Iteration 77, loss = 0.11968788\n",
      "Iteration 78, loss = 0.11891563\n",
      "Iteration 79, loss = 0.11857500\n",
      "Iteration 80, loss = 0.11809162\n",
      "Iteration 81, loss = 0.11739889\n",
      "Iteration 82, loss = 0.11700783\n",
      "Iteration 83, loss = 0.11659478\n",
      "Iteration 84, loss = 0.11581971\n",
      "Iteration 85, loss = 0.11500474\n",
      "Iteration 86, loss = 0.11462671\n",
      "Iteration 87, loss = 0.11411776\n",
      "Iteration 88, loss = 0.11398683\n",
      "Iteration 89, loss = 0.11308006\n",
      "Iteration 90, loss = 0.11281485\n",
      "Iteration 91, loss = 0.11238969\n",
      "Iteration 92, loss = 0.11172284\n",
      "Iteration 93, loss = 0.11142361\n",
      "Iteration 94, loss = 0.11099765\n",
      "Iteration 95, loss = 0.11079498\n",
      "Iteration 96, loss = 0.11032575\n",
      "Iteration 97, loss = 0.10968040\n",
      "Iteration 98, loss = 0.10905570\n",
      "Iteration 99, loss = 0.10846479\n",
      "Iteration 100, loss = 0.10817695\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kileymack/opt/anaconda3/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 1.91526055\n",
      "Iteration 2, loss = 0.41437999\n",
      "Iteration 3, loss = 0.35006933\n",
      "Iteration 4, loss = 0.31510125\n",
      "Iteration 5, loss = 0.29234452\n",
      "Iteration 6, loss = 0.27572617\n",
      "Iteration 7, loss = 0.26197714\n",
      "Iteration 8, loss = 0.25159081\n",
      "Iteration 9, loss = 0.24203576\n",
      "Iteration 10, loss = 0.23363136\n",
      "Iteration 11, loss = 0.22616482\n",
      "Iteration 12, loss = 0.22001827\n",
      "Iteration 13, loss = 0.21424818\n",
      "Iteration 14, loss = 0.20927762\n",
      "Iteration 15, loss = 0.20460235\n",
      "Iteration 16, loss = 0.20015659\n",
      "Iteration 17, loss = 0.19592191\n",
      "Iteration 18, loss = 0.19236500\n",
      "Iteration 19, loss = 0.18901421\n",
      "Iteration 20, loss = 0.18577055\n",
      "Iteration 21, loss = 0.18317043\n",
      "Iteration 22, loss = 0.18034158\n",
      "Iteration 23, loss = 0.17750086\n",
      "Iteration 24, loss = 0.17518052\n",
      "Iteration 25, loss = 0.17292300\n",
      "Iteration 26, loss = 0.17057943\n",
      "Iteration 27, loss = 0.16866444\n",
      "Iteration 28, loss = 0.16686346\n",
      "Iteration 29, loss = 0.16470974\n",
      "Iteration 30, loss = 0.16296940\n",
      "Iteration 31, loss = 0.16122061\n",
      "Iteration 32, loss = 0.15943727\n",
      "Iteration 33, loss = 0.15780154\n",
      "Iteration 34, loss = 0.15615979\n",
      "Iteration 35, loss = 0.15474013\n",
      "Iteration 36, loss = 0.15335778\n",
      "Iteration 37, loss = 0.15204309\n",
      "Iteration 38, loss = 0.15067066\n",
      "Iteration 39, loss = 0.14929221\n",
      "Iteration 40, loss = 0.14795063\n",
      "Iteration 41, loss = 0.14670448\n",
      "Iteration 42, loss = 0.14590366\n",
      "Iteration 43, loss = 0.14456318\n",
      "Iteration 44, loss = 0.14318918\n",
      "Iteration 45, loss = 0.14224109\n",
      "Iteration 46, loss = 0.14119096\n",
      "Iteration 47, loss = 0.14031432\n",
      "Iteration 48, loss = 0.13906097\n",
      "Iteration 49, loss = 0.13822514\n",
      "Iteration 50, loss = 0.13738344\n",
      "Iteration 51, loss = 0.13627948\n",
      "Iteration 52, loss = 0.13528798\n",
      "Iteration 53, loss = 0.13462751\n",
      "Iteration 54, loss = 0.13356953\n",
      "Iteration 55, loss = 0.13273444\n",
      "Iteration 56, loss = 0.13200063\n",
      "Iteration 57, loss = 0.13116002\n",
      "Iteration 58, loss = 0.13054087\n",
      "Iteration 59, loss = 0.12962506\n",
      "Iteration 60, loss = 0.12861281\n",
      "Iteration 61, loss = 0.12824842\n",
      "Iteration 62, loss = 0.12729587\n",
      "Iteration 63, loss = 0.12658787\n",
      "Iteration 64, loss = 0.12580429\n",
      "Iteration 65, loss = 0.12524593\n",
      "Iteration 66, loss = 0.12430554\n",
      "Iteration 67, loss = 0.12365528\n",
      "Iteration 68, loss = 0.12313499\n",
      "Iteration 69, loss = 0.12245516\n",
      "Iteration 70, loss = 0.12205615\n",
      "Iteration 71, loss = 0.12120436\n",
      "Iteration 72, loss = 0.12066392\n",
      "Iteration 73, loss = 0.11995968\n",
      "Iteration 74, loss = 0.11951455\n",
      "Iteration 75, loss = 0.11902257\n",
      "Iteration 76, loss = 0.11820548\n",
      "Iteration 77, loss = 0.11766906\n",
      "Iteration 78, loss = 0.11716562\n",
      "Iteration 79, loss = 0.11663798\n",
      "Iteration 80, loss = 0.11625556\n",
      "Iteration 81, loss = 0.11574332\n",
      "Iteration 82, loss = 0.11520823\n",
      "Iteration 83, loss = 0.11459792\n",
      "Iteration 84, loss = 0.11416214\n",
      "Iteration 85, loss = 0.11370953\n",
      "Iteration 86, loss = 0.11331081\n",
      "Iteration 87, loss = 0.11253578\n",
      "Iteration 88, loss = 0.11247207\n",
      "Iteration 89, loss = 0.11198747\n",
      "Iteration 90, loss = 0.11139502\n",
      "Iteration 91, loss = 0.11086285\n",
      "Iteration 92, loss = 0.11042379\n",
      "Iteration 93, loss = 0.11003364\n",
      "Iteration 94, loss = 0.10945472\n",
      "Iteration 95, loss = 0.10915873\n",
      "Iteration 96, loss = 0.10883390\n",
      "Iteration 97, loss = 0.10828028\n",
      "Iteration 98, loss = 0.10827989\n",
      "Iteration 99, loss = 0.10753939\n",
      "Iteration 100, loss = 0.10723173\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kileymack/opt/anaconda3/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 6.99273613\n",
      "Iteration 2, loss = 0.94416238\n",
      "Iteration 3, loss = 0.47832817\n",
      "Iteration 4, loss = 0.34526336\n",
      "Iteration 5, loss = 0.28871621\n",
      "Iteration 6, loss = 0.25567405\n",
      "Iteration 7, loss = 0.23948376\n",
      "Iteration 8, loss = 0.21952559\n",
      "Iteration 9, loss = 0.21198977\n",
      "Iteration 10, loss = 0.20438833\n",
      "Iteration 11, loss = 0.19307995\n",
      "Iteration 12, loss = 0.18672140\n",
      "Iteration 13, loss = 0.18387976\n",
      "Iteration 14, loss = 0.17729944\n",
      "Iteration 15, loss = 0.17241918\n",
      "Iteration 16, loss = 0.16883258\n",
      "Iteration 17, loss = 0.16357934\n",
      "Iteration 18, loss = 0.16093528\n",
      "Iteration 19, loss = 0.15723741\n",
      "Iteration 20, loss = 0.15434191\n",
      "Iteration 21, loss = 0.15328880\n",
      "Iteration 22, loss = 0.15014220\n",
      "Iteration 23, loss = 0.14739552\n",
      "Iteration 24, loss = 0.14467506\n",
      "Iteration 25, loss = 0.14468542\n",
      "Iteration 26, loss = 0.14035390\n",
      "Iteration 27, loss = 0.13958968\n",
      "Iteration 28, loss = 0.13763586\n",
      "Iteration 29, loss = 0.13771028\n",
      "Iteration 30, loss = 0.13611173\n",
      "Iteration 31, loss = 0.13226690\n",
      "Iteration 32, loss = 0.13161740\n",
      "Iteration 33, loss = 0.13357221\n",
      "Iteration 34, loss = 0.13100903\n",
      "Iteration 35, loss = 0.12650204\n",
      "Iteration 36, loss = 0.12809260\n",
      "Iteration 37, loss = 0.12630657\n",
      "Iteration 38, loss = 0.12711144\n",
      "Iteration 39, loss = 0.12470971\n",
      "Iteration 40, loss = 0.12468861\n",
      "Iteration 41, loss = 0.12218826\n",
      "Iteration 42, loss = 0.12102687\n",
      "Iteration 43, loss = 0.11970529\n",
      "Iteration 44, loss = 0.12162161\n",
      "Iteration 45, loss = 0.12085355\n",
      "Iteration 46, loss = 0.11860018\n",
      "Iteration 47, loss = 0.11942022\n",
      "Iteration 48, loss = 0.11838877\n",
      "Iteration 49, loss = 0.11622789\n",
      "Iteration 50, loss = 0.11793442\n",
      "Iteration 51, loss = 0.11475194\n",
      "Iteration 52, loss = 0.11620370\n",
      "Iteration 53, loss = 0.11498461\n",
      "Iteration 54, loss = 0.11367444\n",
      "Iteration 55, loss = 0.11471964\n",
      "Iteration 56, loss = 0.11252666\n",
      "Iteration 57, loss = 0.11200452\n",
      "Iteration 58, loss = 0.11210611\n",
      "Iteration 59, loss = 0.11206405\n",
      "Iteration 60, loss = 0.11349318\n",
      "Iteration 61, loss = 0.11014557\n",
      "Iteration 62, loss = 0.11144671\n",
      "Iteration 63, loss = 0.10899291\n",
      "Iteration 64, loss = 0.10812678\n",
      "Iteration 65, loss = 0.10924955\n",
      "Iteration 66, loss = 0.10887365\n",
      "Iteration 67, loss = 0.10865296\n",
      "Iteration 68, loss = 0.10862152\n",
      "Iteration 69, loss = 0.10801712\n",
      "Iteration 70, loss = 0.10790558\n",
      "Iteration 71, loss = 0.10907907\n",
      "Iteration 72, loss = 0.10600409\n",
      "Iteration 73, loss = 0.10888163\n",
      "Iteration 74, loss = 0.10571192\n",
      "Iteration 75, loss = 0.10706909\n",
      "Iteration 76, loss = 0.10546160\n",
      "Iteration 77, loss = 0.10619331\n",
      "Iteration 78, loss = 0.10413387\n",
      "Iteration 79, loss = 0.10618903\n",
      "Iteration 80, loss = 0.10403017\n",
      "Iteration 81, loss = 0.10371451\n",
      "Iteration 82, loss = 0.10505001\n",
      "Iteration 83, loss = 0.10350431\n",
      "Iteration 84, loss = 0.10523942\n",
      "Iteration 85, loss = 0.10363855\n",
      "Iteration 86, loss = 0.10531750\n",
      "Iteration 87, loss = 0.10198683\n",
      "Iteration 88, loss = 0.10323193\n",
      "Iteration 89, loss = 0.10370973\n",
      "Iteration 90, loss = 0.10367910\n",
      "Iteration 91, loss = 0.10129997\n",
      "Iteration 92, loss = 0.10317309\n",
      "Iteration 93, loss = 0.10265892\n",
      "Iteration 94, loss = 0.10249940\n",
      "Iteration 95, loss = 0.10298478\n",
      "Iteration 96, loss = 0.10238679\n",
      "Iteration 97, loss = 0.10017768\n",
      "Iteration 98, loss = 0.10086845\n",
      "Iteration 99, loss = 0.10369971\n",
      "Iteration 100, loss = 0.10080547\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kileymack/opt/anaconda3/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 11.34940845\n",
      "Iteration 2, loss = 0.69678206\n",
      "Iteration 3, loss = 0.38337223\n",
      "Iteration 4, loss = 0.30364027\n",
      "Iteration 5, loss = 0.26357662\n",
      "Iteration 6, loss = 0.24186078\n",
      "Iteration 7, loss = 0.22259698\n",
      "Iteration 8, loss = 0.21121740\n",
      "Iteration 9, loss = 0.20028035\n",
      "Iteration 10, loss = 0.19397829\n",
      "Iteration 11, loss = 0.18639175\n",
      "Iteration 12, loss = 0.18162342\n",
      "Iteration 13, loss = 0.17571535\n",
      "Iteration 14, loss = 0.17107988\n",
      "Iteration 15, loss = 0.16716617\n",
      "Iteration 16, loss = 0.16505397\n",
      "Iteration 17, loss = 0.16000325\n",
      "Iteration 18, loss = 0.15742066\n",
      "Iteration 19, loss = 0.15533558\n",
      "Iteration 20, loss = 0.15281146\n",
      "Iteration 21, loss = 0.15128399\n",
      "Iteration 22, loss = 0.14959661\n",
      "Iteration 23, loss = 0.14612884\n",
      "Iteration 24, loss = 0.14706715\n",
      "Iteration 25, loss = 0.14313202\n",
      "Iteration 26, loss = 0.14258841\n",
      "Iteration 27, loss = 0.14141872\n",
      "Iteration 28, loss = 0.13943555\n",
      "Iteration 29, loss = 0.13897427\n",
      "Iteration 30, loss = 0.13887109\n",
      "Iteration 31, loss = 0.13688722\n",
      "Iteration 32, loss = 0.13551191\n",
      "Iteration 33, loss = 0.13626682\n",
      "Iteration 34, loss = 0.13314273\n",
      "Iteration 35, loss = 0.13393123\n",
      "Iteration 36, loss = 0.13291347\n",
      "Iteration 37, loss = 0.13183118\n",
      "Iteration 38, loss = 0.13034703\n",
      "Iteration 39, loss = 0.13102813\n",
      "Iteration 40, loss = 0.13093080\n",
      "Iteration 41, loss = 0.12981843\n",
      "Iteration 42, loss = 0.13000415\n",
      "Iteration 43, loss = 0.13008236\n",
      "Iteration 44, loss = 0.12937281\n",
      "Iteration 45, loss = 0.12721784\n",
      "Iteration 46, loss = 0.12735465\n",
      "Iteration 47, loss = 0.12584254\n",
      "Iteration 48, loss = 0.12620973\n",
      "Iteration 49, loss = 0.12656476\n",
      "Iteration 50, loss = 0.12665943\n",
      "Iteration 51, loss = 0.12508486\n",
      "Iteration 52, loss = 0.12371991\n",
      "Iteration 53, loss = 0.12524351\n",
      "Iteration 54, loss = 0.12293627\n",
      "Iteration 55, loss = 0.12368177\n",
      "Iteration 56, loss = 0.12306116\n",
      "Iteration 57, loss = 0.12200684\n",
      "Iteration 58, loss = 0.12395582\n",
      "Iteration 59, loss = 0.12447696\n",
      "Iteration 60, loss = 0.12123020\n",
      "Iteration 61, loss = 0.12146108\n",
      "Iteration 62, loss = 0.12090057\n",
      "Iteration 63, loss = 0.12070081\n",
      "Iteration 64, loss = 0.12056835\n",
      "Iteration 65, loss = 0.11981051\n",
      "Iteration 66, loss = 0.12090949\n",
      "Iteration 67, loss = 0.11864294\n",
      "Iteration 68, loss = 0.11988381\n",
      "Iteration 69, loss = 0.12032687\n",
      "Iteration 70, loss = 0.11990392\n",
      "Iteration 71, loss = 0.11827205\n",
      "Iteration 72, loss = 0.11903976\n",
      "Iteration 73, loss = 0.11848323\n",
      "Iteration 74, loss = 0.11771380\n",
      "Iteration 75, loss = 0.11717762\n",
      "Iteration 76, loss = 0.11867096\n",
      "Iteration 77, loss = 0.11806373\n",
      "Iteration 78, loss = 0.11743061\n",
      "Iteration 79, loss = 0.11783092\n",
      "Iteration 80, loss = 0.11563993\n",
      "Iteration 81, loss = 0.11823012\n",
      "Iteration 82, loss = 0.11497765\n",
      "Iteration 83, loss = 0.11793621\n",
      "Iteration 84, loss = 0.11639057\n",
      "Iteration 85, loss = 0.11728108\n",
      "Iteration 86, loss = 0.11600320\n",
      "Iteration 87, loss = 0.11552909\n",
      "Iteration 88, loss = 0.11566141\n",
      "Iteration 89, loss = 0.11569000\n",
      "Iteration 90, loss = 0.11600499\n",
      "Iteration 91, loss = 0.11594604\n",
      "Iteration 92, loss = 0.11398283\n",
      "Iteration 93, loss = 0.11463311\n",
      "Iteration 94, loss = 0.11465790\n",
      "Iteration 95, loss = 0.11448524\n",
      "Iteration 96, loss = 0.11357303\n",
      "Iteration 97, loss = 0.11433403\n",
      "Iteration 98, loss = 0.11456559\n",
      "Iteration 99, loss = 0.11446041\n",
      "Iteration 100, loss = 0.11321674\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kileymack/opt/anaconda3/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 2.17091317\n",
      "Iteration 2, loss = 0.44715546\n",
      "Iteration 3, loss = 0.36665842\n",
      "Iteration 4, loss = 0.32682710\n",
      "Iteration 5, loss = 0.30225080\n",
      "Iteration 6, loss = 0.28434705\n",
      "Iteration 7, loss = 0.27057564\n",
      "Iteration 8, loss = 0.25844814\n",
      "Iteration 9, loss = 0.24978782\n",
      "Iteration 10, loss = 0.24092335\n",
      "Iteration 11, loss = 0.23363347\n",
      "Iteration 12, loss = 0.22723987\n",
      "Iteration 13, loss = 0.22103620\n",
      "Iteration 14, loss = 0.21710365\n",
      "Iteration 15, loss = 0.21142134\n",
      "Iteration 16, loss = 0.20779956\n",
      "Iteration 17, loss = 0.20341972\n",
      "Iteration 18, loss = 0.20031112\n",
      "Iteration 19, loss = 0.19652311\n",
      "Iteration 20, loss = 0.19332581\n",
      "Iteration 21, loss = 0.19065675\n",
      "Iteration 22, loss = 0.18742792\n",
      "Iteration 23, loss = 0.18449249\n",
      "Iteration 24, loss = 0.18229404\n",
      "Iteration 25, loss = 0.17949034\n",
      "Iteration 26, loss = 0.17738587\n",
      "Iteration 27, loss = 0.17482308\n",
      "Iteration 28, loss = 0.17271337\n",
      "Iteration 29, loss = 0.17098257\n",
      "Iteration 30, loss = 0.16874480\n",
      "Iteration 31, loss = 0.16674596\n",
      "Iteration 32, loss = 0.16510179\n",
      "Iteration 33, loss = 0.16304945\n",
      "Iteration 34, loss = 0.16143951\n",
      "Iteration 35, loss = 0.15954429\n",
      "Iteration 36, loss = 0.15831962\n",
      "Iteration 37, loss = 0.15666389\n",
      "Iteration 38, loss = 0.15497863\n",
      "Iteration 39, loss = 0.15411023\n",
      "Iteration 40, loss = 0.15230788\n",
      "Iteration 41, loss = 0.15107513\n",
      "Iteration 42, loss = 0.14953404\n",
      "Iteration 43, loss = 0.14851418\n",
      "Iteration 44, loss = 0.14739592\n",
      "Iteration 45, loss = 0.14630817\n",
      "Iteration 46, loss = 0.14457343\n",
      "Iteration 47, loss = 0.14372020\n",
      "Iteration 48, loss = 0.14228512\n",
      "Iteration 49, loss = 0.14169287\n",
      "Iteration 50, loss = 0.14081788\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kileymack/opt/anaconda3/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (50) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 1.93167199\n",
      "Iteration 2, loss = 0.43170030\n",
      "Iteration 3, loss = 0.36665183\n",
      "Iteration 4, loss = 0.33049613\n",
      "Iteration 5, loss = 0.30550774\n",
      "Iteration 6, loss = 0.28745741\n",
      "Iteration 7, loss = 0.27315135\n",
      "Iteration 8, loss = 0.26092461\n",
      "Iteration 9, loss = 0.25102035\n",
      "Iteration 10, loss = 0.24245806\n",
      "Iteration 11, loss = 0.23538834\n",
      "Iteration 12, loss = 0.22850863\n",
      "Iteration 13, loss = 0.22246222\n",
      "Iteration 14, loss = 0.21733597\n",
      "Iteration 15, loss = 0.21217596\n",
      "Iteration 16, loss = 0.20799271\n",
      "Iteration 17, loss = 0.20390671\n",
      "Iteration 18, loss = 0.20027888\n",
      "Iteration 19, loss = 0.19655087\n",
      "Iteration 20, loss = 0.19340329\n",
      "Iteration 21, loss = 0.19021605\n",
      "Iteration 22, loss = 0.18730186\n",
      "Iteration 23, loss = 0.18454325\n",
      "Iteration 24, loss = 0.18164614\n",
      "Iteration 25, loss = 0.17948260\n",
      "Iteration 26, loss = 0.17744485\n",
      "Iteration 27, loss = 0.17519554\n",
      "Iteration 28, loss = 0.17295257\n",
      "Iteration 29, loss = 0.17087773\n",
      "Iteration 30, loss = 0.16909029\n",
      "Iteration 31, loss = 0.16700837\n",
      "Iteration 32, loss = 0.16532820\n",
      "Iteration 33, loss = 0.16355630\n",
      "Iteration 34, loss = 0.16207388\n",
      "Iteration 35, loss = 0.16028048\n",
      "Iteration 36, loss = 0.15883988\n",
      "Iteration 37, loss = 0.15758577\n",
      "Iteration 38, loss = 0.15608244\n",
      "Iteration 39, loss = 0.15434781\n",
      "Iteration 40, loss = 0.15314872\n",
      "Iteration 41, loss = 0.15187258\n",
      "Iteration 42, loss = 0.15066906\n",
      "Iteration 43, loss = 0.14943159\n",
      "Iteration 44, loss = 0.14813789\n",
      "Iteration 45, loss = 0.14704906\n",
      "Iteration 46, loss = 0.14594481\n",
      "Iteration 47, loss = 0.14492434\n",
      "Iteration 48, loss = 0.14401352\n",
      "Iteration 49, loss = 0.14269918\n",
      "Iteration 50, loss = 0.14167848\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kileymack/opt/anaconda3/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (50) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 6.98253107\n",
      "Iteration 2, loss = 1.14105725\n",
      "Iteration 3, loss = 0.55126996\n",
      "Iteration 4, loss = 0.37982531\n",
      "Iteration 5, loss = 0.30739993\n",
      "Iteration 6, loss = 0.26883662\n",
      "Iteration 7, loss = 0.24769664\n",
      "Iteration 8, loss = 0.22369946\n",
      "Iteration 9, loss = 0.21574839\n",
      "Iteration 10, loss = 0.20066239\n",
      "Iteration 11, loss = 0.19158193\n",
      "Iteration 12, loss = 0.18345692\n",
      "Iteration 13, loss = 0.18010065\n",
      "Iteration 14, loss = 0.17744574\n",
      "Iteration 15, loss = 0.17046573\n",
      "Iteration 16, loss = 0.16501234\n",
      "Iteration 17, loss = 0.16124659\n",
      "Iteration 18, loss = 0.15774010\n",
      "Iteration 19, loss = 0.15656237\n",
      "Iteration 20, loss = 0.15306920\n",
      "Iteration 21, loss = 0.14748937\n",
      "Iteration 22, loss = 0.14602192\n",
      "Iteration 23, loss = 0.14475378\n",
      "Iteration 24, loss = 0.14333990\n",
      "Iteration 25, loss = 0.13892598\n",
      "Iteration 26, loss = 0.14130017\n",
      "Iteration 27, loss = 0.13640069\n",
      "Iteration 28, loss = 0.13585363\n",
      "Iteration 29, loss = 0.13515654\n",
      "Iteration 30, loss = 0.13123488\n",
      "Iteration 31, loss = 0.13142846\n",
      "Iteration 32, loss = 0.12922290\n",
      "Iteration 33, loss = 0.12908529\n",
      "Iteration 34, loss = 0.12819550\n",
      "Iteration 35, loss = 0.12677518\n",
      "Iteration 36, loss = 0.12662864\n",
      "Iteration 37, loss = 0.12210078\n",
      "Iteration 38, loss = 0.12181733\n",
      "Iteration 39, loss = 0.12328973\n",
      "Iteration 40, loss = 0.11994025\n",
      "Iteration 41, loss = 0.12150622\n",
      "Iteration 42, loss = 0.12238894\n",
      "Iteration 43, loss = 0.11827533\n",
      "Iteration 44, loss = 0.11730697\n",
      "Iteration 45, loss = 0.11785123\n",
      "Iteration 46, loss = 0.11939737\n",
      "Iteration 47, loss = 0.11684709\n",
      "Iteration 48, loss = 0.11538637\n",
      "Iteration 49, loss = 0.11693054\n",
      "Iteration 50, loss = 0.11391842\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kileymack/opt/anaconda3/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (50) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 11.68442137\n",
      "Iteration 2, loss = 0.85883739\n",
      "Iteration 3, loss = 0.42269262\n",
      "Iteration 4, loss = 0.32294083\n",
      "Iteration 5, loss = 0.27518707\n",
      "Iteration 6, loss = 0.24733385\n",
      "Iteration 7, loss = 0.23313061\n",
      "Iteration 8, loss = 0.21780595\n",
      "Iteration 9, loss = 0.21011978\n",
      "Iteration 10, loss = 0.20054928\n",
      "Iteration 11, loss = 0.19384522\n",
      "Iteration 12, loss = 0.18785827\n",
      "Iteration 13, loss = 0.18205514\n",
      "Iteration 14, loss = 0.17766492\n",
      "Iteration 15, loss = 0.17483755\n",
      "Iteration 16, loss = 0.17048636\n",
      "Iteration 17, loss = 0.16832747\n",
      "Iteration 18, loss = 0.16402842\n",
      "Iteration 19, loss = 0.16192566\n",
      "Iteration 20, loss = 0.15939669\n",
      "Iteration 21, loss = 0.15763727\n",
      "Iteration 22, loss = 0.15456262\n",
      "Iteration 23, loss = 0.15362877\n",
      "Iteration 24, loss = 0.15114165\n",
      "Iteration 25, loss = 0.15166371\n",
      "Iteration 26, loss = 0.14914778\n",
      "Iteration 27, loss = 0.14770489\n",
      "Iteration 28, loss = 0.14671632\n",
      "Iteration 29, loss = 0.14652924\n",
      "Iteration 30, loss = 0.14477481\n",
      "Iteration 31, loss = 0.14407324\n",
      "Iteration 32, loss = 0.14144224\n",
      "Iteration 33, loss = 0.14225874\n",
      "Iteration 34, loss = 0.14162645\n",
      "Iteration 35, loss = 0.13996731\n",
      "Iteration 36, loss = 0.13968942\n",
      "Iteration 37, loss = 0.13776435\n",
      "Iteration 38, loss = 0.13749950\n",
      "Iteration 39, loss = 0.13649268\n",
      "Iteration 40, loss = 0.13650406\n",
      "Iteration 41, loss = 0.13631028\n",
      "Iteration 42, loss = 0.13578265\n",
      "Iteration 43, loss = 0.13405042\n",
      "Iteration 44, loss = 0.13328874\n",
      "Iteration 45, loss = 0.13365832\n",
      "Iteration 46, loss = 0.13231020\n",
      "Iteration 47, loss = 0.13194850\n",
      "Iteration 48, loss = 0.13195776\n",
      "Iteration 49, loss = 0.13176695\n",
      "Iteration 50, loss = 0.12982282\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kileymack/opt/anaconda3/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (50) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 2.17091317\n",
      "Iteration 2, loss = 0.44715546\n",
      "Iteration 3, loss = 0.36665842\n",
      "Iteration 4, loss = 0.32682710\n",
      "Iteration 5, loss = 0.30225080\n",
      "Iteration 6, loss = 0.28434705\n",
      "Iteration 7, loss = 0.27057564\n",
      "Iteration 8, loss = 0.25844814\n",
      "Iteration 9, loss = 0.24978782\n",
      "Iteration 10, loss = 0.24092335\n",
      "Iteration 11, loss = 0.23363347\n",
      "Iteration 12, loss = 0.22723987\n",
      "Iteration 13, loss = 0.22103620\n",
      "Iteration 14, loss = 0.21710365\n",
      "Iteration 15, loss = 0.21142134\n",
      "Iteration 16, loss = 0.20779956\n",
      "Iteration 17, loss = 0.20341972\n",
      "Iteration 18, loss = 0.20031112\n",
      "Iteration 19, loss = 0.19652311\n",
      "Iteration 20, loss = 0.19332581\n",
      "Iteration 21, loss = 0.19065675\n",
      "Iteration 22, loss = 0.18742792\n",
      "Iteration 23, loss = 0.18449249\n",
      "Iteration 24, loss = 0.18229404\n",
      "Iteration 25, loss = 0.17949034\n",
      "Iteration 26, loss = 0.17738587\n",
      "Iteration 27, loss = 0.17482308\n",
      "Iteration 28, loss = 0.17271337\n",
      "Iteration 29, loss = 0.17098257\n",
      "Iteration 30, loss = 0.16874480\n",
      "Iteration 31, loss = 0.16674596\n",
      "Iteration 32, loss = 0.16510179\n",
      "Iteration 33, loss = 0.16304945\n",
      "Iteration 34, loss = 0.16143951\n",
      "Iteration 35, loss = 0.15954429\n",
      "Iteration 36, loss = 0.15831962\n",
      "Iteration 37, loss = 0.15666389\n",
      "Iteration 38, loss = 0.15497863\n",
      "Iteration 39, loss = 0.15411023\n",
      "Iteration 40, loss = 0.15230788\n",
      "Iteration 41, loss = 0.15107513\n",
      "Iteration 42, loss = 0.14953404\n",
      "Iteration 43, loss = 0.14851418\n",
      "Iteration 44, loss = 0.14739592\n",
      "Iteration 45, loss = 0.14630817\n",
      "Iteration 46, loss = 0.14457343\n",
      "Iteration 47, loss = 0.14372020\n",
      "Iteration 48, loss = 0.14228512\n",
      "Iteration 49, loss = 0.14169287\n",
      "Iteration 50, loss = 0.14081788\n",
      "Iteration 51, loss = 0.13949019\n",
      "Iteration 52, loss = 0.13831685\n",
      "Iteration 53, loss = 0.13751506\n",
      "Iteration 54, loss = 0.13645962\n",
      "Iteration 55, loss = 0.13559662\n",
      "Iteration 56, loss = 0.13495113\n",
      "Iteration 57, loss = 0.13399948\n",
      "Iteration 58, loss = 0.13312839\n",
      "Iteration 59, loss = 0.13203945\n",
      "Iteration 60, loss = 0.13140651\n",
      "Iteration 61, loss = 0.13067053\n",
      "Iteration 62, loss = 0.13014505\n",
      "Iteration 63, loss = 0.12924753\n",
      "Iteration 64, loss = 0.12833089\n",
      "Iteration 65, loss = 0.12763665\n",
      "Iteration 66, loss = 0.12679479\n",
      "Iteration 67, loss = 0.12597071\n",
      "Iteration 68, loss = 0.12564620\n",
      "Iteration 69, loss = 0.12509310\n",
      "Iteration 70, loss = 0.12431096\n",
      "Iteration 71, loss = 0.12347993\n",
      "Iteration 72, loss = 0.12299989\n",
      "Iteration 73, loss = 0.12261431\n",
      "Iteration 74, loss = 0.12200493\n",
      "Iteration 75, loss = 0.12091195\n",
      "Iteration 76, loss = 0.12032386\n",
      "Iteration 77, loss = 0.11966124\n",
      "Iteration 78, loss = 0.11952612\n",
      "Iteration 79, loss = 0.11864101\n",
      "Iteration 80, loss = 0.11806320\n",
      "Iteration 81, loss = 0.11763820\n",
      "Iteration 82, loss = 0.11710332\n",
      "Iteration 83, loss = 0.11660080\n",
      "Iteration 84, loss = 0.11610440\n",
      "Iteration 85, loss = 0.11578500\n",
      "Iteration 86, loss = 0.11505758\n",
      "Iteration 87, loss = 0.11439681\n",
      "Iteration 88, loss = 0.11407402\n",
      "Iteration 89, loss = 0.11383887\n",
      "Iteration 90, loss = 0.11298723\n",
      "Iteration 91, loss = 0.11287021\n",
      "Iteration 92, loss = 0.11201269\n",
      "Iteration 93, loss = 0.11202689\n",
      "Iteration 94, loss = 0.11141182\n",
      "Iteration 95, loss = 0.11045833\n",
      "Iteration 96, loss = 0.11041926\n",
      "Iteration 97, loss = 0.11027051\n",
      "Iteration 98, loss = 0.10953043\n",
      "Iteration 99, loss = 0.10902811\n",
      "Iteration 100, loss = 0.10870820\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kileymack/opt/anaconda3/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 1.93167199\n",
      "Iteration 2, loss = 0.43170030\n",
      "Iteration 3, loss = 0.36665183\n",
      "Iteration 4, loss = 0.33049613\n",
      "Iteration 5, loss = 0.30550774\n",
      "Iteration 6, loss = 0.28745741\n",
      "Iteration 7, loss = 0.27315135\n",
      "Iteration 8, loss = 0.26092461\n",
      "Iteration 9, loss = 0.25102035\n",
      "Iteration 10, loss = 0.24245806\n",
      "Iteration 11, loss = 0.23538834\n",
      "Iteration 12, loss = 0.22850863\n",
      "Iteration 13, loss = 0.22246222\n",
      "Iteration 14, loss = 0.21733597\n",
      "Iteration 15, loss = 0.21217596\n",
      "Iteration 16, loss = 0.20799271\n",
      "Iteration 17, loss = 0.20390671\n",
      "Iteration 18, loss = 0.20027888\n",
      "Iteration 19, loss = 0.19655087\n",
      "Iteration 20, loss = 0.19340329\n",
      "Iteration 21, loss = 0.19021605\n",
      "Iteration 22, loss = 0.18730186\n",
      "Iteration 23, loss = 0.18454325\n",
      "Iteration 24, loss = 0.18164614\n",
      "Iteration 25, loss = 0.17948260\n",
      "Iteration 26, loss = 0.17744485\n",
      "Iteration 27, loss = 0.17519554\n",
      "Iteration 28, loss = 0.17295257\n",
      "Iteration 29, loss = 0.17087773\n",
      "Iteration 30, loss = 0.16909029\n",
      "Iteration 31, loss = 0.16700837\n",
      "Iteration 32, loss = 0.16532820\n",
      "Iteration 33, loss = 0.16355630\n",
      "Iteration 34, loss = 0.16207388\n",
      "Iteration 35, loss = 0.16028048\n",
      "Iteration 36, loss = 0.15883988\n",
      "Iteration 37, loss = 0.15758577\n",
      "Iteration 38, loss = 0.15608244\n",
      "Iteration 39, loss = 0.15434781\n",
      "Iteration 40, loss = 0.15314872\n",
      "Iteration 41, loss = 0.15187258\n",
      "Iteration 42, loss = 0.15066906\n",
      "Iteration 43, loss = 0.14943159\n",
      "Iteration 44, loss = 0.14813789\n",
      "Iteration 45, loss = 0.14704906\n",
      "Iteration 46, loss = 0.14594481\n",
      "Iteration 47, loss = 0.14492434\n",
      "Iteration 48, loss = 0.14401352\n",
      "Iteration 49, loss = 0.14269918\n",
      "Iteration 50, loss = 0.14167848\n",
      "Iteration 51, loss = 0.14087515\n",
      "Iteration 52, loss = 0.13981987\n",
      "Iteration 53, loss = 0.13886121\n",
      "Iteration 54, loss = 0.13785948\n",
      "Iteration 55, loss = 0.13702506\n",
      "Iteration 56, loss = 0.13646990\n",
      "Iteration 57, loss = 0.13539027\n",
      "Iteration 58, loss = 0.13457509\n",
      "Iteration 59, loss = 0.13371068\n",
      "Iteration 60, loss = 0.13296251\n",
      "Iteration 61, loss = 0.13203791\n",
      "Iteration 62, loss = 0.13165230\n",
      "Iteration 63, loss = 0.13052908\n",
      "Iteration 64, loss = 0.12984960\n",
      "Iteration 65, loss = 0.12908596\n",
      "Iteration 66, loss = 0.12836723\n",
      "Iteration 67, loss = 0.12778307\n",
      "Iteration 68, loss = 0.12714712\n",
      "Iteration 69, loss = 0.12647435\n",
      "Iteration 70, loss = 0.12566405\n",
      "Iteration 71, loss = 0.12513290\n",
      "Iteration 72, loss = 0.12458146\n",
      "Iteration 73, loss = 0.12399246\n",
      "Iteration 74, loss = 0.12316342\n",
      "Iteration 75, loss = 0.12260041\n",
      "Iteration 76, loss = 0.12205276\n",
      "Iteration 77, loss = 0.12123316\n",
      "Iteration 78, loss = 0.12085881\n",
      "Iteration 79, loss = 0.12040770\n",
      "Iteration 80, loss = 0.11986652\n",
      "Iteration 81, loss = 0.11928286\n",
      "Iteration 82, loss = 0.11871963\n",
      "Iteration 83, loss = 0.11801830\n",
      "Iteration 84, loss = 0.11767732\n",
      "Iteration 85, loss = 0.11712119\n",
      "Iteration 86, loss = 0.11674333\n",
      "Iteration 87, loss = 0.11610753\n",
      "Iteration 88, loss = 0.11577468\n",
      "Iteration 89, loss = 0.11519391\n",
      "Iteration 90, loss = 0.11467231\n",
      "Iteration 91, loss = 0.11441656\n",
      "Iteration 92, loss = 0.11387045\n",
      "Iteration 93, loss = 0.11329732\n",
      "Iteration 94, loss = 0.11291547\n",
      "Iteration 95, loss = 0.11264047\n",
      "Iteration 96, loss = 0.11212394\n",
      "Iteration 97, loss = 0.11162961\n",
      "Iteration 98, loss = 0.11136881\n",
      "Iteration 99, loss = 0.11073191\n",
      "Iteration 100, loss = 0.11058756\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kileymack/opt/anaconda3/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 6.98253107\n",
      "Iteration 2, loss = 1.14105725\n",
      "Iteration 3, loss = 0.55126996\n",
      "Iteration 4, loss = 0.37982531\n",
      "Iteration 5, loss = 0.30739993\n",
      "Iteration 6, loss = 0.26883662\n",
      "Iteration 7, loss = 0.24769664\n",
      "Iteration 8, loss = 0.22369946\n",
      "Iteration 9, loss = 0.21574839\n",
      "Iteration 10, loss = 0.20066239\n",
      "Iteration 11, loss = 0.19158193\n",
      "Iteration 12, loss = 0.18345692\n",
      "Iteration 13, loss = 0.18010065\n",
      "Iteration 14, loss = 0.17744574\n",
      "Iteration 15, loss = 0.17046573\n",
      "Iteration 16, loss = 0.16501234\n",
      "Iteration 17, loss = 0.16124659\n",
      "Iteration 18, loss = 0.15774010\n",
      "Iteration 19, loss = 0.15656237\n",
      "Iteration 20, loss = 0.15306920\n",
      "Iteration 21, loss = 0.14748937\n",
      "Iteration 22, loss = 0.14602192\n",
      "Iteration 23, loss = 0.14475378\n",
      "Iteration 24, loss = 0.14333990\n",
      "Iteration 25, loss = 0.13892598\n",
      "Iteration 26, loss = 0.14130017\n",
      "Iteration 27, loss = 0.13640069\n",
      "Iteration 28, loss = 0.13585363\n",
      "Iteration 29, loss = 0.13515654\n",
      "Iteration 30, loss = 0.13123488\n",
      "Iteration 31, loss = 0.13142846\n",
      "Iteration 32, loss = 0.12922290\n",
      "Iteration 33, loss = 0.12908529\n",
      "Iteration 34, loss = 0.12819550\n",
      "Iteration 35, loss = 0.12677518\n",
      "Iteration 36, loss = 0.12662864\n",
      "Iteration 37, loss = 0.12210078\n",
      "Iteration 38, loss = 0.12181733\n",
      "Iteration 39, loss = 0.12328973\n",
      "Iteration 40, loss = 0.11994025\n",
      "Iteration 41, loss = 0.12150622\n",
      "Iteration 42, loss = 0.12238894\n",
      "Iteration 43, loss = 0.11827533\n",
      "Iteration 44, loss = 0.11730697\n",
      "Iteration 45, loss = 0.11785123\n",
      "Iteration 46, loss = 0.11939737\n",
      "Iteration 47, loss = 0.11684709\n",
      "Iteration 48, loss = 0.11538637\n",
      "Iteration 49, loss = 0.11693054\n",
      "Iteration 50, loss = 0.11391842\n",
      "Iteration 51, loss = 0.11555623\n",
      "Iteration 52, loss = 0.11239270\n",
      "Iteration 53, loss = 0.11268506\n",
      "Iteration 54, loss = 0.11170902\n",
      "Iteration 55, loss = 0.11445788\n",
      "Iteration 56, loss = 0.11076627\n",
      "Iteration 57, loss = 0.11145296\n",
      "Iteration 58, loss = 0.11288083\n",
      "Iteration 59, loss = 0.11041173\n",
      "Iteration 60, loss = 0.10855477\n",
      "Iteration 61, loss = 0.10986650\n",
      "Iteration 62, loss = 0.10956314\n",
      "Iteration 63, loss = 0.10775506\n",
      "Iteration 64, loss = 0.11060942\n",
      "Iteration 65, loss = 0.10743082\n",
      "Iteration 66, loss = 0.10683299\n",
      "Iteration 67, loss = 0.10856422\n",
      "Iteration 68, loss = 0.10669222\n",
      "Iteration 69, loss = 0.10667175\n",
      "Iteration 70, loss = 0.10886790\n",
      "Iteration 71, loss = 0.10662388\n",
      "Iteration 72, loss = 0.10512924\n",
      "Iteration 73, loss = 0.10742012\n",
      "Iteration 74, loss = 0.10668094\n",
      "Iteration 75, loss = 0.10514585\n",
      "Iteration 76, loss = 0.10388760\n",
      "Iteration 77, loss = 0.10476820\n",
      "Iteration 78, loss = 0.10343687\n",
      "Iteration 79, loss = 0.10657804\n",
      "Iteration 80, loss = 0.10444375\n",
      "Iteration 81, loss = 0.10443100\n",
      "Iteration 82, loss = 0.10506328\n",
      "Iteration 83, loss = 0.10447174\n",
      "Iteration 84, loss = 0.10268250\n",
      "Iteration 85, loss = 0.10417707\n",
      "Iteration 86, loss = 0.10287719\n",
      "Iteration 87, loss = 0.10130080\n",
      "Iteration 88, loss = 0.10160617\n",
      "Iteration 89, loss = 0.10207482\n",
      "Iteration 90, loss = 0.10270242\n",
      "Iteration 91, loss = 0.10066608\n",
      "Iteration 92, loss = 0.10307815\n",
      "Iteration 93, loss = 0.10084391\n",
      "Iteration 94, loss = 0.10188822\n",
      "Iteration 95, loss = 0.10100521\n",
      "Iteration 96, loss = 0.10292746\n",
      "Iteration 97, loss = 0.09892223\n",
      "Iteration 98, loss = 0.09984723\n",
      "Iteration 99, loss = 0.10031103\n",
      "Iteration 100, loss = 0.10074124\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kileymack/opt/anaconda3/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 11.68442137\n",
      "Iteration 2, loss = 0.85883739\n",
      "Iteration 3, loss = 0.42269262\n",
      "Iteration 4, loss = 0.32294083\n",
      "Iteration 5, loss = 0.27518707\n",
      "Iteration 6, loss = 0.24733385\n",
      "Iteration 7, loss = 0.23313061\n",
      "Iteration 8, loss = 0.21780595\n",
      "Iteration 9, loss = 0.21011978\n",
      "Iteration 10, loss = 0.20054928\n",
      "Iteration 11, loss = 0.19384522\n",
      "Iteration 12, loss = 0.18785827\n",
      "Iteration 13, loss = 0.18205514\n",
      "Iteration 14, loss = 0.17766492\n",
      "Iteration 15, loss = 0.17483755\n",
      "Iteration 16, loss = 0.17048636\n",
      "Iteration 17, loss = 0.16832747\n",
      "Iteration 18, loss = 0.16402842\n",
      "Iteration 19, loss = 0.16192566\n",
      "Iteration 20, loss = 0.15939669\n",
      "Iteration 21, loss = 0.15763727\n",
      "Iteration 22, loss = 0.15456262\n",
      "Iteration 23, loss = 0.15362877\n",
      "Iteration 24, loss = 0.15114165\n",
      "Iteration 25, loss = 0.15166371\n",
      "Iteration 26, loss = 0.14914778\n",
      "Iteration 27, loss = 0.14770489\n",
      "Iteration 28, loss = 0.14671632\n",
      "Iteration 29, loss = 0.14652924\n",
      "Iteration 30, loss = 0.14477481\n",
      "Iteration 31, loss = 0.14407324\n",
      "Iteration 32, loss = 0.14144224\n",
      "Iteration 33, loss = 0.14225874\n",
      "Iteration 34, loss = 0.14162645\n",
      "Iteration 35, loss = 0.13996731\n",
      "Iteration 36, loss = 0.13968942\n",
      "Iteration 37, loss = 0.13776435\n",
      "Iteration 38, loss = 0.13749950\n",
      "Iteration 39, loss = 0.13649268\n",
      "Iteration 40, loss = 0.13650406\n",
      "Iteration 41, loss = 0.13631028\n",
      "Iteration 42, loss = 0.13578265\n",
      "Iteration 43, loss = 0.13405042\n",
      "Iteration 44, loss = 0.13328874\n",
      "Iteration 45, loss = 0.13365832\n",
      "Iteration 46, loss = 0.13231020\n",
      "Iteration 47, loss = 0.13194850\n",
      "Iteration 48, loss = 0.13195776\n",
      "Iteration 49, loss = 0.13176695\n",
      "Iteration 50, loss = 0.12982282\n",
      "Iteration 51, loss = 0.13175520\n",
      "Iteration 52, loss = 0.12931917\n",
      "Iteration 53, loss = 0.13044708\n",
      "Iteration 54, loss = 0.12840693\n",
      "Iteration 55, loss = 0.12852077\n",
      "Iteration 56, loss = 0.13039083\n",
      "Iteration 57, loss = 0.12740084\n",
      "Iteration 58, loss = 0.12718870\n",
      "Iteration 59, loss = 0.12712265\n",
      "Iteration 60, loss = 0.12707250\n",
      "Iteration 61, loss = 0.12569360\n",
      "Iteration 62, loss = 0.12755444\n",
      "Iteration 63, loss = 0.12557557\n",
      "Iteration 64, loss = 0.12439675\n",
      "Iteration 65, loss = 0.12539638\n",
      "Iteration 66, loss = 0.12442049\n",
      "Iteration 67, loss = 0.12440429\n",
      "Iteration 68, loss = 0.12316941\n",
      "Iteration 69, loss = 0.12439271\n",
      "Iteration 70, loss = 0.12419698\n",
      "Iteration 71, loss = 0.12398311\n",
      "Iteration 72, loss = 0.12293541\n",
      "Iteration 73, loss = 0.12497202\n",
      "Iteration 74, loss = 0.12208329\n",
      "Iteration 75, loss = 0.12274603\n",
      "Iteration 76, loss = 0.12193599\n",
      "Iteration 77, loss = 0.12195553\n",
      "Iteration 78, loss = 0.12209867\n",
      "Iteration 79, loss = 0.12228527\n",
      "Iteration 80, loss = 0.12148632\n",
      "Iteration 81, loss = 0.12168102\n",
      "Iteration 82, loss = 0.12053949\n",
      "Iteration 83, loss = 0.12052531\n",
      "Iteration 84, loss = 0.12103296\n",
      "Iteration 85, loss = 0.11898507\n",
      "Iteration 86, loss = 0.12198556\n",
      "Iteration 87, loss = 0.11981834\n",
      "Iteration 88, loss = 0.11942500\n",
      "Iteration 89, loss = 0.11931654\n",
      "Iteration 90, loss = 0.12018737\n",
      "Iteration 91, loss = 0.12020190\n",
      "Iteration 92, loss = 0.11932074\n",
      "Iteration 93, loss = 0.11936609\n",
      "Iteration 94, loss = 0.11906018\n",
      "Iteration 95, loss = 0.11893520\n",
      "Iteration 96, loss = 0.11975986\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.17091317\n",
      "Iteration 2, loss = 0.44715546\n",
      "Iteration 3, loss = 0.36665842\n",
      "Iteration 4, loss = 0.32682710\n",
      "Iteration 5, loss = 0.30225080\n",
      "Iteration 6, loss = 0.28434705\n",
      "Iteration 7, loss = 0.27057564\n",
      "Iteration 8, loss = 0.25844814\n",
      "Iteration 9, loss = 0.24978782\n",
      "Iteration 10, loss = 0.24092335\n",
      "Iteration 11, loss = 0.23363347\n",
      "Iteration 12, loss = 0.22723987\n",
      "Iteration 13, loss = 0.22103620\n",
      "Iteration 14, loss = 0.21710365\n",
      "Iteration 15, loss = 0.21142134\n",
      "Iteration 16, loss = 0.20779956\n",
      "Iteration 17, loss = 0.20341972\n",
      "Iteration 18, loss = 0.20031112\n",
      "Iteration 19, loss = 0.19652311\n",
      "Iteration 20, loss = 0.19332581\n",
      "Iteration 21, loss = 0.19065675\n",
      "Iteration 22, loss = 0.18742792\n",
      "Iteration 23, loss = 0.18449249\n",
      "Iteration 24, loss = 0.18229404\n",
      "Iteration 25, loss = 0.17949034\n",
      "Iteration 26, loss = 0.17738587\n",
      "Iteration 27, loss = 0.17482308\n",
      "Iteration 28, loss = 0.17271337\n",
      "Iteration 29, loss = 0.17098257\n",
      "Iteration 30, loss = 0.16874480\n",
      "Iteration 31, loss = 0.16674596\n",
      "Iteration 32, loss = 0.16510179\n",
      "Iteration 33, loss = 0.16304945\n",
      "Iteration 34, loss = 0.16143951\n",
      "Iteration 35, loss = 0.15954429\n",
      "Iteration 36, loss = 0.15831962\n",
      "Iteration 37, loss = 0.15666389\n",
      "Iteration 38, loss = 0.15497863\n",
      "Iteration 39, loss = 0.15411023\n",
      "Iteration 40, loss = 0.15230788\n",
      "Iteration 41, loss = 0.15107513\n",
      "Iteration 42, loss = 0.14953404\n",
      "Iteration 43, loss = 0.14851418\n",
      "Iteration 44, loss = 0.14739592\n",
      "Iteration 45, loss = 0.14630817\n",
      "Iteration 46, loss = 0.14457343\n",
      "Iteration 47, loss = 0.14372020\n",
      "Iteration 48, loss = 0.14228512\n",
      "Iteration 49, loss = 0.14169287\n",
      "Iteration 50, loss = 0.14081788\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kileymack/opt/anaconda3/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (50) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 1.93167199\n",
      "Iteration 2, loss = 0.43170030\n",
      "Iteration 3, loss = 0.36665183\n",
      "Iteration 4, loss = 0.33049613\n",
      "Iteration 5, loss = 0.30550774\n",
      "Iteration 6, loss = 0.28745741\n",
      "Iteration 7, loss = 0.27315135\n",
      "Iteration 8, loss = 0.26092461\n",
      "Iteration 9, loss = 0.25102035\n",
      "Iteration 10, loss = 0.24245806\n",
      "Iteration 11, loss = 0.23538834\n",
      "Iteration 12, loss = 0.22850863\n",
      "Iteration 13, loss = 0.22246222\n",
      "Iteration 14, loss = 0.21733597\n",
      "Iteration 15, loss = 0.21217596\n",
      "Iteration 16, loss = 0.20799271\n",
      "Iteration 17, loss = 0.20390671\n",
      "Iteration 18, loss = 0.20027888\n",
      "Iteration 19, loss = 0.19655087\n",
      "Iteration 20, loss = 0.19340329\n",
      "Iteration 21, loss = 0.19021605\n",
      "Iteration 22, loss = 0.18730186\n",
      "Iteration 23, loss = 0.18454325\n",
      "Iteration 24, loss = 0.18164614\n",
      "Iteration 25, loss = 0.17948260\n",
      "Iteration 26, loss = 0.17744485\n",
      "Iteration 27, loss = 0.17519554\n",
      "Iteration 28, loss = 0.17295257\n",
      "Iteration 29, loss = 0.17087773\n",
      "Iteration 30, loss = 0.16909029\n",
      "Iteration 31, loss = 0.16700837\n",
      "Iteration 32, loss = 0.16532820\n",
      "Iteration 33, loss = 0.16355630\n",
      "Iteration 34, loss = 0.16207388\n",
      "Iteration 35, loss = 0.16028048\n",
      "Iteration 36, loss = 0.15883988\n",
      "Iteration 37, loss = 0.15758577\n",
      "Iteration 38, loss = 0.15608244\n",
      "Iteration 39, loss = 0.15434781\n",
      "Iteration 40, loss = 0.15314872\n",
      "Iteration 41, loss = 0.15187258\n",
      "Iteration 42, loss = 0.15066906\n",
      "Iteration 43, loss = 0.14943159\n",
      "Iteration 44, loss = 0.14813789\n",
      "Iteration 45, loss = 0.14704906\n",
      "Iteration 46, loss = 0.14594481\n",
      "Iteration 47, loss = 0.14492434\n",
      "Iteration 48, loss = 0.14401352\n",
      "Iteration 49, loss = 0.14269918\n",
      "Iteration 50, loss = 0.14167848\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kileymack/opt/anaconda3/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (50) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 6.98253107\n",
      "Iteration 2, loss = 1.14105725\n",
      "Iteration 3, loss = 0.55126996\n",
      "Iteration 4, loss = 0.37982531\n",
      "Iteration 5, loss = 0.30739993\n",
      "Iteration 6, loss = 0.26883662\n",
      "Iteration 7, loss = 0.24769664\n",
      "Iteration 8, loss = 0.22369946\n",
      "Iteration 9, loss = 0.21574839\n",
      "Iteration 10, loss = 0.20066239\n",
      "Iteration 11, loss = 0.19158193\n",
      "Iteration 12, loss = 0.18345692\n",
      "Iteration 13, loss = 0.18010065\n",
      "Iteration 14, loss = 0.17744574\n",
      "Iteration 15, loss = 0.17046573\n",
      "Iteration 16, loss = 0.16501234\n",
      "Iteration 17, loss = 0.16124659\n",
      "Iteration 18, loss = 0.15774010\n",
      "Iteration 19, loss = 0.15656237\n",
      "Iteration 20, loss = 0.15306920\n",
      "Iteration 21, loss = 0.14748937\n",
      "Iteration 22, loss = 0.14602192\n",
      "Iteration 23, loss = 0.14475378\n",
      "Iteration 24, loss = 0.14333990\n",
      "Iteration 25, loss = 0.13892598\n",
      "Iteration 26, loss = 0.14130017\n",
      "Iteration 27, loss = 0.13640069\n",
      "Iteration 28, loss = 0.13585363\n",
      "Iteration 29, loss = 0.13515654\n",
      "Iteration 30, loss = 0.13123488\n",
      "Iteration 31, loss = 0.13142846\n",
      "Iteration 32, loss = 0.12922290\n",
      "Iteration 33, loss = 0.12908529\n",
      "Iteration 34, loss = 0.12819550\n",
      "Iteration 35, loss = 0.12677518\n",
      "Iteration 36, loss = 0.12662864\n",
      "Iteration 37, loss = 0.12210078\n",
      "Iteration 38, loss = 0.12181733\n",
      "Iteration 39, loss = 0.12328973\n",
      "Iteration 40, loss = 0.11994025\n",
      "Iteration 41, loss = 0.12150622\n",
      "Iteration 42, loss = 0.12238894\n",
      "Iteration 43, loss = 0.11827533\n",
      "Iteration 44, loss = 0.11730697\n",
      "Iteration 45, loss = 0.11785123\n",
      "Iteration 46, loss = 0.11939737\n",
      "Iteration 47, loss = 0.11684709\n",
      "Iteration 48, loss = 0.11538637\n",
      "Iteration 49, loss = 0.11693054\n",
      "Iteration 50, loss = 0.11391842\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kileymack/opt/anaconda3/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (50) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 11.68442137\n",
      "Iteration 2, loss = 0.85883739\n",
      "Iteration 3, loss = 0.42269262\n",
      "Iteration 4, loss = 0.32294083\n",
      "Iteration 5, loss = 0.27518707\n",
      "Iteration 6, loss = 0.24733385\n",
      "Iteration 7, loss = 0.23313061\n",
      "Iteration 8, loss = 0.21780595\n",
      "Iteration 9, loss = 0.21011978\n",
      "Iteration 10, loss = 0.20054928\n",
      "Iteration 11, loss = 0.19384522\n",
      "Iteration 12, loss = 0.18785827\n",
      "Iteration 13, loss = 0.18205514\n",
      "Iteration 14, loss = 0.17766492\n",
      "Iteration 15, loss = 0.17483755\n",
      "Iteration 16, loss = 0.17048636\n",
      "Iteration 17, loss = 0.16832747\n",
      "Iteration 18, loss = 0.16402842\n",
      "Iteration 19, loss = 0.16192566\n",
      "Iteration 20, loss = 0.15939669\n",
      "Iteration 21, loss = 0.15763727\n",
      "Iteration 22, loss = 0.15456262\n",
      "Iteration 23, loss = 0.15362877\n",
      "Iteration 24, loss = 0.15114165\n",
      "Iteration 25, loss = 0.15166371\n",
      "Iteration 26, loss = 0.14914778\n",
      "Iteration 27, loss = 0.14770489\n",
      "Iteration 28, loss = 0.14671632\n",
      "Iteration 29, loss = 0.14652924\n",
      "Iteration 30, loss = 0.14477481\n",
      "Iteration 31, loss = 0.14407324\n",
      "Iteration 32, loss = 0.14144224\n",
      "Iteration 33, loss = 0.14225874\n",
      "Iteration 34, loss = 0.14162645\n",
      "Iteration 35, loss = 0.13996731\n",
      "Iteration 36, loss = 0.13968942\n",
      "Iteration 37, loss = 0.13776435\n",
      "Iteration 38, loss = 0.13749950\n",
      "Iteration 39, loss = 0.13649268\n",
      "Iteration 40, loss = 0.13650406\n",
      "Iteration 41, loss = 0.13631028\n",
      "Iteration 42, loss = 0.13578265\n",
      "Iteration 43, loss = 0.13405042\n",
      "Iteration 44, loss = 0.13328874\n",
      "Iteration 45, loss = 0.13365832\n",
      "Iteration 46, loss = 0.13231020\n",
      "Iteration 47, loss = 0.13194850\n",
      "Iteration 48, loss = 0.13195776\n",
      "Iteration 49, loss = 0.13176695\n",
      "Iteration 50, loss = 0.12982282\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kileymack/opt/anaconda3/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (50) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 2.17091317\n",
      "Iteration 2, loss = 0.44715546\n",
      "Iteration 3, loss = 0.36665842\n",
      "Iteration 4, loss = 0.32682710\n",
      "Iteration 5, loss = 0.30225080\n",
      "Iteration 6, loss = 0.28434705\n",
      "Iteration 7, loss = 0.27057564\n",
      "Iteration 8, loss = 0.25844814\n",
      "Iteration 9, loss = 0.24978782\n",
      "Iteration 10, loss = 0.24092335\n",
      "Iteration 11, loss = 0.23363347\n",
      "Iteration 12, loss = 0.22723987\n",
      "Iteration 13, loss = 0.22103620\n",
      "Iteration 14, loss = 0.21710365\n",
      "Iteration 15, loss = 0.21142134\n",
      "Iteration 16, loss = 0.20779956\n",
      "Iteration 17, loss = 0.20341972\n",
      "Iteration 18, loss = 0.20031112\n",
      "Iteration 19, loss = 0.19652311\n",
      "Iteration 20, loss = 0.19332581\n",
      "Iteration 21, loss = 0.19065675\n",
      "Iteration 22, loss = 0.18742792\n",
      "Iteration 23, loss = 0.18449249\n",
      "Iteration 24, loss = 0.18229404\n",
      "Iteration 25, loss = 0.17949034\n",
      "Iteration 26, loss = 0.17738587\n",
      "Iteration 27, loss = 0.17482308\n",
      "Iteration 28, loss = 0.17271337\n",
      "Iteration 29, loss = 0.17098257\n",
      "Iteration 30, loss = 0.16874480\n",
      "Iteration 31, loss = 0.16674596\n",
      "Iteration 32, loss = 0.16510179\n",
      "Iteration 33, loss = 0.16304945\n",
      "Iteration 34, loss = 0.16143951\n",
      "Iteration 35, loss = 0.15954429\n",
      "Iteration 36, loss = 0.15831962\n",
      "Iteration 37, loss = 0.15666389\n",
      "Iteration 38, loss = 0.15497863\n",
      "Iteration 39, loss = 0.15411023\n",
      "Iteration 40, loss = 0.15230788\n",
      "Iteration 41, loss = 0.15107513\n",
      "Iteration 42, loss = 0.14953404\n",
      "Iteration 43, loss = 0.14851418\n",
      "Iteration 44, loss = 0.14739592\n",
      "Iteration 45, loss = 0.14630817\n",
      "Iteration 46, loss = 0.14457343\n",
      "Iteration 47, loss = 0.14372020\n",
      "Iteration 48, loss = 0.14228512\n",
      "Iteration 49, loss = 0.14169287\n",
      "Iteration 50, loss = 0.14081788\n",
      "Iteration 51, loss = 0.13949019\n",
      "Iteration 52, loss = 0.13831685\n",
      "Iteration 53, loss = 0.13751506\n",
      "Iteration 54, loss = 0.13645962\n",
      "Iteration 55, loss = 0.13559662\n",
      "Iteration 56, loss = 0.13495113\n",
      "Iteration 57, loss = 0.13399948\n",
      "Iteration 58, loss = 0.13312839\n",
      "Iteration 59, loss = 0.13203945\n",
      "Iteration 60, loss = 0.13140651\n",
      "Iteration 61, loss = 0.13067053\n",
      "Iteration 62, loss = 0.13014505\n",
      "Iteration 63, loss = 0.12924753\n",
      "Iteration 64, loss = 0.12833089\n",
      "Iteration 65, loss = 0.12763665\n",
      "Iteration 66, loss = 0.12679479\n",
      "Iteration 67, loss = 0.12597071\n",
      "Iteration 68, loss = 0.12564620\n",
      "Iteration 69, loss = 0.12509310\n",
      "Iteration 70, loss = 0.12431096\n",
      "Iteration 71, loss = 0.12347993\n",
      "Iteration 72, loss = 0.12299989\n",
      "Iteration 73, loss = 0.12261431\n",
      "Iteration 74, loss = 0.12200493\n",
      "Iteration 75, loss = 0.12091195\n",
      "Iteration 76, loss = 0.12032386\n",
      "Iteration 77, loss = 0.11966124\n",
      "Iteration 78, loss = 0.11952612\n",
      "Iteration 79, loss = 0.11864101\n",
      "Iteration 80, loss = 0.11806320\n",
      "Iteration 81, loss = 0.11763820\n",
      "Iteration 82, loss = 0.11710332\n",
      "Iteration 83, loss = 0.11660080\n",
      "Iteration 84, loss = 0.11610440\n",
      "Iteration 85, loss = 0.11578500\n",
      "Iteration 86, loss = 0.11505758\n",
      "Iteration 87, loss = 0.11439681\n",
      "Iteration 88, loss = 0.11407402\n",
      "Iteration 89, loss = 0.11383887\n",
      "Iteration 90, loss = 0.11298723\n",
      "Iteration 91, loss = 0.11287021\n",
      "Iteration 92, loss = 0.11201269\n",
      "Iteration 93, loss = 0.11202689\n",
      "Iteration 94, loss = 0.11141182\n",
      "Iteration 95, loss = 0.11045833\n",
      "Iteration 96, loss = 0.11041926\n",
      "Iteration 97, loss = 0.11027051\n",
      "Iteration 98, loss = 0.10953043\n",
      "Iteration 99, loss = 0.10902811\n",
      "Iteration 100, loss = 0.10870820\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kileymack/opt/anaconda3/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 1.93167199\n",
      "Iteration 2, loss = 0.43170030\n",
      "Iteration 3, loss = 0.36665183\n",
      "Iteration 4, loss = 0.33049613\n",
      "Iteration 5, loss = 0.30550774\n",
      "Iteration 6, loss = 0.28745741\n",
      "Iteration 7, loss = 0.27315135\n",
      "Iteration 8, loss = 0.26092461\n",
      "Iteration 9, loss = 0.25102035\n",
      "Iteration 10, loss = 0.24245806\n",
      "Iteration 11, loss = 0.23538834\n",
      "Iteration 12, loss = 0.22850863\n",
      "Iteration 13, loss = 0.22246222\n",
      "Iteration 14, loss = 0.21733597\n",
      "Iteration 15, loss = 0.21217596\n",
      "Iteration 16, loss = 0.20799271\n",
      "Iteration 17, loss = 0.20390671\n",
      "Iteration 18, loss = 0.20027888\n",
      "Iteration 19, loss = 0.19655087\n",
      "Iteration 20, loss = 0.19340329\n",
      "Iteration 21, loss = 0.19021605\n",
      "Iteration 22, loss = 0.18730186\n",
      "Iteration 23, loss = 0.18454325\n",
      "Iteration 24, loss = 0.18164614\n",
      "Iteration 25, loss = 0.17948260\n",
      "Iteration 26, loss = 0.17744485\n",
      "Iteration 27, loss = 0.17519554\n",
      "Iteration 28, loss = 0.17295257\n",
      "Iteration 29, loss = 0.17087773\n",
      "Iteration 30, loss = 0.16909029\n",
      "Iteration 31, loss = 0.16700837\n",
      "Iteration 32, loss = 0.16532820\n",
      "Iteration 33, loss = 0.16355630\n",
      "Iteration 34, loss = 0.16207388\n",
      "Iteration 35, loss = 0.16028048\n",
      "Iteration 36, loss = 0.15883988\n",
      "Iteration 37, loss = 0.15758577\n",
      "Iteration 38, loss = 0.15608244\n",
      "Iteration 39, loss = 0.15434781\n",
      "Iteration 40, loss = 0.15314872\n",
      "Iteration 41, loss = 0.15187258\n",
      "Iteration 42, loss = 0.15066906\n",
      "Iteration 43, loss = 0.14943159\n",
      "Iteration 44, loss = 0.14813789\n",
      "Iteration 45, loss = 0.14704906\n",
      "Iteration 46, loss = 0.14594481\n",
      "Iteration 47, loss = 0.14492434\n",
      "Iteration 48, loss = 0.14401352\n",
      "Iteration 49, loss = 0.14269918\n",
      "Iteration 50, loss = 0.14167848\n",
      "Iteration 51, loss = 0.14087515\n",
      "Iteration 52, loss = 0.13981987\n",
      "Iteration 53, loss = 0.13886121\n",
      "Iteration 54, loss = 0.13785948\n",
      "Iteration 55, loss = 0.13702506\n",
      "Iteration 56, loss = 0.13646990\n",
      "Iteration 57, loss = 0.13539027\n",
      "Iteration 58, loss = 0.13457509\n",
      "Iteration 59, loss = 0.13371068\n",
      "Iteration 60, loss = 0.13296251\n",
      "Iteration 61, loss = 0.13203791\n",
      "Iteration 62, loss = 0.13165230\n",
      "Iteration 63, loss = 0.13052908\n",
      "Iteration 64, loss = 0.12984960\n",
      "Iteration 65, loss = 0.12908596\n",
      "Iteration 66, loss = 0.12836723\n",
      "Iteration 67, loss = 0.12778307\n",
      "Iteration 68, loss = 0.12714712\n",
      "Iteration 69, loss = 0.12647435\n",
      "Iteration 70, loss = 0.12566405\n",
      "Iteration 71, loss = 0.12513290\n",
      "Iteration 72, loss = 0.12458146\n",
      "Iteration 73, loss = 0.12399246\n",
      "Iteration 74, loss = 0.12316342\n",
      "Iteration 75, loss = 0.12260041\n",
      "Iteration 76, loss = 0.12205276\n",
      "Iteration 77, loss = 0.12123316\n",
      "Iteration 78, loss = 0.12085881\n",
      "Iteration 79, loss = 0.12040770\n",
      "Iteration 80, loss = 0.11986652\n",
      "Iteration 81, loss = 0.11928286\n",
      "Iteration 82, loss = 0.11871963\n",
      "Iteration 83, loss = 0.11801830\n",
      "Iteration 84, loss = 0.11767732\n",
      "Iteration 85, loss = 0.11712119\n",
      "Iteration 86, loss = 0.11674333\n",
      "Iteration 87, loss = 0.11610753\n",
      "Iteration 88, loss = 0.11577468\n",
      "Iteration 89, loss = 0.11519391\n",
      "Iteration 90, loss = 0.11467231\n",
      "Iteration 91, loss = 0.11441656\n",
      "Iteration 92, loss = 0.11387045\n",
      "Iteration 93, loss = 0.11329732\n",
      "Iteration 94, loss = 0.11291547\n",
      "Iteration 95, loss = 0.11264047\n",
      "Iteration 96, loss = 0.11212394\n",
      "Iteration 97, loss = 0.11162961\n",
      "Iteration 98, loss = 0.11136881\n",
      "Iteration 99, loss = 0.11073191\n",
      "Iteration 100, loss = 0.11058756\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kileymack/opt/anaconda3/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 6.98253107\n",
      "Iteration 2, loss = 1.14105725\n",
      "Iteration 3, loss = 0.55126996\n",
      "Iteration 4, loss = 0.37982531\n",
      "Iteration 5, loss = 0.30739993\n",
      "Iteration 6, loss = 0.26883662\n",
      "Iteration 7, loss = 0.24769664\n",
      "Iteration 8, loss = 0.22369946\n",
      "Iteration 9, loss = 0.21574839\n",
      "Iteration 10, loss = 0.20066239\n",
      "Iteration 11, loss = 0.19158193\n",
      "Iteration 12, loss = 0.18345692\n",
      "Iteration 13, loss = 0.18010065\n",
      "Iteration 14, loss = 0.17744574\n",
      "Iteration 15, loss = 0.17046573\n",
      "Iteration 16, loss = 0.16501234\n",
      "Iteration 17, loss = 0.16124659\n",
      "Iteration 18, loss = 0.15774010\n",
      "Iteration 19, loss = 0.15656237\n",
      "Iteration 20, loss = 0.15306920\n",
      "Iteration 21, loss = 0.14748937\n",
      "Iteration 22, loss = 0.14602192\n",
      "Iteration 23, loss = 0.14475378\n",
      "Iteration 24, loss = 0.14333990\n",
      "Iteration 25, loss = 0.13892598\n",
      "Iteration 26, loss = 0.14130017\n",
      "Iteration 27, loss = 0.13640069\n",
      "Iteration 28, loss = 0.13585363\n",
      "Iteration 29, loss = 0.13515654\n",
      "Iteration 30, loss = 0.13123488\n",
      "Iteration 31, loss = 0.13142846\n",
      "Iteration 32, loss = 0.12922290\n",
      "Iteration 33, loss = 0.12908529\n",
      "Iteration 34, loss = 0.12819550\n",
      "Iteration 35, loss = 0.12677518\n",
      "Iteration 36, loss = 0.12662864\n",
      "Iteration 37, loss = 0.12210078\n",
      "Iteration 38, loss = 0.12181733\n",
      "Iteration 39, loss = 0.12328973\n",
      "Iteration 40, loss = 0.11994025\n",
      "Iteration 41, loss = 0.12150622\n",
      "Iteration 42, loss = 0.12238894\n",
      "Iteration 43, loss = 0.11827533\n",
      "Iteration 44, loss = 0.11730697\n",
      "Iteration 45, loss = 0.11785123\n",
      "Iteration 46, loss = 0.11939737\n",
      "Iteration 47, loss = 0.11684709\n",
      "Iteration 48, loss = 0.11538637\n",
      "Iteration 49, loss = 0.11693054\n",
      "Iteration 50, loss = 0.11391842\n",
      "Iteration 51, loss = 0.11555623\n",
      "Iteration 52, loss = 0.11239270\n",
      "Iteration 53, loss = 0.11268506\n",
      "Iteration 54, loss = 0.11170902\n",
      "Iteration 55, loss = 0.11445788\n",
      "Iteration 56, loss = 0.11076627\n",
      "Iteration 57, loss = 0.11145296\n",
      "Iteration 58, loss = 0.11288083\n",
      "Iteration 59, loss = 0.11041173\n",
      "Iteration 60, loss = 0.10855477\n",
      "Iteration 61, loss = 0.10986650\n",
      "Iteration 62, loss = 0.10956314\n",
      "Iteration 63, loss = 0.10775506\n",
      "Iteration 64, loss = 0.11060942\n",
      "Iteration 65, loss = 0.10743082\n",
      "Iteration 66, loss = 0.10683299\n",
      "Iteration 67, loss = 0.10856422\n",
      "Iteration 68, loss = 0.10669222\n",
      "Iteration 69, loss = 0.10667175\n",
      "Iteration 70, loss = 0.10886790\n",
      "Iteration 71, loss = 0.10662388\n",
      "Iteration 72, loss = 0.10512924\n",
      "Iteration 73, loss = 0.10742012\n",
      "Iteration 74, loss = 0.10668094\n",
      "Iteration 75, loss = 0.10514585\n",
      "Iteration 76, loss = 0.10388760\n",
      "Iteration 77, loss = 0.10476820\n",
      "Iteration 78, loss = 0.10343687\n",
      "Iteration 79, loss = 0.10657804\n",
      "Iteration 80, loss = 0.10444375\n",
      "Iteration 81, loss = 0.10443100\n",
      "Iteration 82, loss = 0.10506328\n",
      "Iteration 83, loss = 0.10447174\n",
      "Iteration 84, loss = 0.10268250\n",
      "Iteration 85, loss = 0.10417707\n",
      "Iteration 86, loss = 0.10287719\n",
      "Iteration 87, loss = 0.10130080\n",
      "Iteration 88, loss = 0.10160617\n",
      "Iteration 89, loss = 0.10207482\n",
      "Iteration 90, loss = 0.10270242\n",
      "Iteration 91, loss = 0.10066608\n",
      "Iteration 92, loss = 0.10307815\n",
      "Iteration 93, loss = 0.10084391\n",
      "Iteration 94, loss = 0.10188822\n",
      "Iteration 95, loss = 0.10100521\n",
      "Iteration 96, loss = 0.10292746\n",
      "Iteration 97, loss = 0.09892223\n",
      "Iteration 98, loss = 0.09984723\n",
      "Iteration 99, loss = 0.10031103\n",
      "Iteration 100, loss = 0.10074124\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kileymack/opt/anaconda3/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 11.68442137\n",
      "Iteration 2, loss = 0.85883739\n",
      "Iteration 3, loss = 0.42269262\n",
      "Iteration 4, loss = 0.32294083\n",
      "Iteration 5, loss = 0.27518707\n",
      "Iteration 6, loss = 0.24733385\n",
      "Iteration 7, loss = 0.23313061\n",
      "Iteration 8, loss = 0.21780595\n",
      "Iteration 9, loss = 0.21011978\n",
      "Iteration 10, loss = 0.20054928\n",
      "Iteration 11, loss = 0.19384522\n",
      "Iteration 12, loss = 0.18785827\n",
      "Iteration 13, loss = 0.18205514\n",
      "Iteration 14, loss = 0.17766492\n",
      "Iteration 15, loss = 0.17483755\n",
      "Iteration 16, loss = 0.17048636\n",
      "Iteration 17, loss = 0.16832747\n",
      "Iteration 18, loss = 0.16402842\n",
      "Iteration 19, loss = 0.16192566\n",
      "Iteration 20, loss = 0.15939669\n",
      "Iteration 21, loss = 0.15763727\n",
      "Iteration 22, loss = 0.15456262\n",
      "Iteration 23, loss = 0.15362877\n",
      "Iteration 24, loss = 0.15114165\n",
      "Iteration 25, loss = 0.15166371\n",
      "Iteration 26, loss = 0.14914778\n",
      "Iteration 27, loss = 0.14770489\n",
      "Iteration 28, loss = 0.14671632\n",
      "Iteration 29, loss = 0.14652924\n",
      "Iteration 30, loss = 0.14477481\n",
      "Iteration 31, loss = 0.14407324\n",
      "Iteration 32, loss = 0.14144224\n",
      "Iteration 33, loss = 0.14225874\n",
      "Iteration 34, loss = 0.14162645\n",
      "Iteration 35, loss = 0.13996731\n",
      "Iteration 36, loss = 0.13968942\n",
      "Iteration 37, loss = 0.13776435\n",
      "Iteration 38, loss = 0.13749950\n",
      "Iteration 39, loss = 0.13649268\n",
      "Iteration 40, loss = 0.13650406\n",
      "Iteration 41, loss = 0.13631028\n",
      "Iteration 42, loss = 0.13578265\n",
      "Iteration 43, loss = 0.13405042\n",
      "Iteration 44, loss = 0.13328874\n",
      "Iteration 45, loss = 0.13365832\n",
      "Iteration 46, loss = 0.13231020\n",
      "Iteration 47, loss = 0.13194850\n",
      "Iteration 48, loss = 0.13195776\n",
      "Iteration 49, loss = 0.13176695\n",
      "Iteration 50, loss = 0.12982282\n",
      "Iteration 51, loss = 0.13175520\n",
      "Iteration 52, loss = 0.12931917\n",
      "Iteration 53, loss = 0.13044708\n",
      "Iteration 54, loss = 0.12840693\n",
      "Iteration 55, loss = 0.12852077\n",
      "Iteration 56, loss = 0.13039083\n",
      "Iteration 57, loss = 0.12740084\n",
      "Iteration 58, loss = 0.12718870\n",
      "Iteration 59, loss = 0.12712265\n",
      "Iteration 60, loss = 0.12707250\n",
      "Iteration 61, loss = 0.12569360\n",
      "Iteration 62, loss = 0.12755444\n",
      "Iteration 63, loss = 0.12557557\n",
      "Iteration 64, loss = 0.12439675\n",
      "Iteration 65, loss = 0.12539638\n",
      "Iteration 66, loss = 0.12442049\n",
      "Iteration 67, loss = 0.12440429\n",
      "Iteration 68, loss = 0.12316941\n",
      "Iteration 69, loss = 0.12439271\n",
      "Iteration 70, loss = 0.12419698\n",
      "Iteration 71, loss = 0.12398311\n",
      "Iteration 72, loss = 0.12293541\n",
      "Iteration 73, loss = 0.12497202\n",
      "Iteration 74, loss = 0.12208329\n",
      "Iteration 75, loss = 0.12274603\n",
      "Iteration 76, loss = 0.12193599\n",
      "Iteration 77, loss = 0.12195553\n",
      "Iteration 78, loss = 0.12209867\n",
      "Iteration 79, loss = 0.12228527\n",
      "Iteration 80, loss = 0.12148632\n",
      "Iteration 81, loss = 0.12168102\n",
      "Iteration 82, loss = 0.12053949\n",
      "Iteration 83, loss = 0.12052531\n",
      "Iteration 84, loss = 0.12103296\n",
      "Iteration 85, loss = 0.11898507\n",
      "Iteration 86, loss = 0.12198556\n",
      "Iteration 87, loss = 0.11981834\n",
      "Iteration 88, loss = 0.11942500\n",
      "Iteration 89, loss = 0.11931654\n",
      "Iteration 90, loss = 0.12018737\n",
      "Iteration 91, loss = 0.12020190\n",
      "Iteration 92, loss = 0.11932074\n",
      "Iteration 93, loss = 0.11936609\n",
      "Iteration 94, loss = 0.11906018\n",
      "Iteration 95, loss = 0.11893520\n",
      "Iteration 96, loss = 0.11975986\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.45338063\n",
      "Iteration 2, loss = 0.38391235\n",
      "Iteration 3, loss = 0.31473299\n",
      "Iteration 4, loss = 0.28012899\n",
      "Iteration 5, loss = 0.25735495\n",
      "Iteration 6, loss = 0.23904773\n",
      "Iteration 7, loss = 0.22527027\n",
      "Iteration 8, loss = 0.21489632\n",
      "Iteration 9, loss = 0.20499844\n",
      "Iteration 10, loss = 0.19724269\n",
      "Iteration 11, loss = 0.18969387\n",
      "Iteration 12, loss = 0.18410539\n",
      "Iteration 13, loss = 0.17832236\n",
      "Iteration 14, loss = 0.17220591\n",
      "Iteration 15, loss = 0.16703508\n",
      "Iteration 16, loss = 0.16402862\n",
      "Iteration 17, loss = 0.16016243\n",
      "Iteration 18, loss = 0.15551689\n",
      "Iteration 19, loss = 0.15321766\n",
      "Iteration 20, loss = 0.14916037\n",
      "Iteration 21, loss = 0.14659568\n",
      "Iteration 22, loss = 0.14284147\n",
      "Iteration 23, loss = 0.13976623\n",
      "Iteration 24, loss = 0.13806411\n",
      "Iteration 25, loss = 0.13551059\n",
      "Iteration 26, loss = 0.13272509\n",
      "Iteration 27, loss = 0.12970475\n",
      "Iteration 28, loss = 0.12842388\n",
      "Iteration 29, loss = 0.12609827\n",
      "Iteration 30, loss = 0.12457055\n",
      "Iteration 31, loss = 0.12113476\n",
      "Iteration 32, loss = 0.11954105\n",
      "Iteration 33, loss = 0.11771151\n",
      "Iteration 34, loss = 0.11706178\n",
      "Iteration 35, loss = 0.11461389\n",
      "Iteration 36, loss = 0.11378253\n",
      "Iteration 37, loss = 0.11179982\n",
      "Iteration 38, loss = 0.10987637\n",
      "Iteration 39, loss = 0.10818740\n",
      "Iteration 40, loss = 0.10726265\n",
      "Iteration 41, loss = 0.10620064\n",
      "Iteration 42, loss = 0.10461453\n",
      "Iteration 43, loss = 0.10411031\n",
      "Iteration 44, loss = 0.10162422\n",
      "Iteration 45, loss = 0.10130558\n",
      "Iteration 46, loss = 0.09940882\n",
      "Iteration 47, loss = 0.09919038\n",
      "Iteration 48, loss = 0.09715612\n",
      "Iteration 49, loss = 0.09607360\n",
      "Iteration 50, loss = 0.09592956\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kileymack/opt/anaconda3/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (50) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 1.13302478\n",
      "Iteration 2, loss = 0.37303979\n",
      "Iteration 3, loss = 0.31677272\n",
      "Iteration 4, loss = 0.28340208\n",
      "Iteration 5, loss = 0.26037920\n",
      "Iteration 6, loss = 0.24212492\n",
      "Iteration 7, loss = 0.22767110\n",
      "Iteration 8, loss = 0.21581160\n",
      "Iteration 9, loss = 0.20553427\n",
      "Iteration 10, loss = 0.19712104\n",
      "Iteration 11, loss = 0.18938974\n",
      "Iteration 12, loss = 0.18243195\n",
      "Iteration 13, loss = 0.17673267\n",
      "Iteration 14, loss = 0.17118242\n",
      "Iteration 15, loss = 0.16546337\n",
      "Iteration 16, loss = 0.16064789\n",
      "Iteration 17, loss = 0.15572878\n",
      "Iteration 18, loss = 0.15221019\n",
      "Iteration 19, loss = 0.14802034\n",
      "Iteration 20, loss = 0.14522766\n",
      "Iteration 21, loss = 0.14154119\n",
      "Iteration 22, loss = 0.13832699\n",
      "Iteration 23, loss = 0.13540068\n",
      "Iteration 24, loss = 0.13300605\n",
      "Iteration 25, loss = 0.13049088\n",
      "Iteration 26, loss = 0.12859740\n",
      "Iteration 27, loss = 0.12579631\n",
      "Iteration 28, loss = 0.12374159\n",
      "Iteration 29, loss = 0.12131471\n",
      "Iteration 30, loss = 0.11941692\n",
      "Iteration 31, loss = 0.11754700\n",
      "Iteration 32, loss = 0.11569703\n",
      "Iteration 33, loss = 0.11372123\n",
      "Iteration 34, loss = 0.11192642\n",
      "Iteration 35, loss = 0.10998768\n",
      "Iteration 36, loss = 0.10867747\n",
      "Iteration 37, loss = 0.10707246\n",
      "Iteration 38, loss = 0.10568200\n",
      "Iteration 39, loss = 0.10450544\n",
      "Iteration 40, loss = 0.10312971\n",
      "Iteration 41, loss = 0.10130528\n",
      "Iteration 42, loss = 0.10016568\n",
      "Iteration 43, loss = 0.09896173\n",
      "Iteration 44, loss = 0.09779283\n",
      "Iteration 45, loss = 0.09629557\n",
      "Iteration 46, loss = 0.09524696\n",
      "Iteration 47, loss = 0.09424295\n",
      "Iteration 48, loss = 0.09313566\n",
      "Iteration 49, loss = 0.09236478\n",
      "Iteration 50, loss = 0.09129581\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kileymack/opt/anaconda3/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (50) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 1.58111654\n",
      "Iteration 2, loss = 0.31851691\n",
      "Iteration 3, loss = 0.25343646\n",
      "Iteration 4, loss = 0.22185343\n",
      "Iteration 5, loss = 0.20201506\n",
      "Iteration 6, loss = 0.18743743\n",
      "Iteration 7, loss = 0.16922282\n",
      "Iteration 8, loss = 0.16092229\n",
      "Iteration 9, loss = 0.15575909\n",
      "Iteration 10, loss = 0.14483664\n",
      "Iteration 11, loss = 0.13627238\n",
      "Iteration 12, loss = 0.13149719\n",
      "Iteration 13, loss = 0.12539791\n",
      "Iteration 14, loss = 0.11616881\n",
      "Iteration 15, loss = 0.11203527\n",
      "Iteration 16, loss = 0.10686851\n",
      "Iteration 17, loss = 0.10552265\n",
      "Iteration 18, loss = 0.10048521\n",
      "Iteration 19, loss = 0.09543827\n",
      "Iteration 20, loss = 0.09586289\n",
      "Iteration 21, loss = 0.09019389\n",
      "Iteration 22, loss = 0.08581433\n",
      "Iteration 23, loss = 0.08849903\n",
      "Iteration 24, loss = 0.08610830\n",
      "Iteration 25, loss = 0.07915110\n",
      "Iteration 26, loss = 0.07747108\n",
      "Iteration 27, loss = 0.07516655\n",
      "Iteration 28, loss = 0.07490685\n",
      "Iteration 29, loss = 0.07372384\n",
      "Iteration 30, loss = 0.07196165\n",
      "Iteration 31, loss = 0.06889322\n",
      "Iteration 32, loss = 0.06714344\n",
      "Iteration 33, loss = 0.06529591\n",
      "Iteration 34, loss = 0.06320735\n",
      "Iteration 35, loss = 0.06323640\n",
      "Iteration 36, loss = 0.06038880\n",
      "Iteration 37, loss = 0.06365006\n",
      "Iteration 38, loss = 0.05786159\n",
      "Iteration 39, loss = 0.05642215\n",
      "Iteration 40, loss = 0.05540536\n",
      "Iteration 41, loss = 0.05865224\n",
      "Iteration 42, loss = 0.05242827\n",
      "Iteration 43, loss = 0.05447825\n",
      "Iteration 44, loss = 0.05402387\n",
      "Iteration 45, loss = 0.05343717\n",
      "Iteration 46, loss = 0.04989339\n",
      "Iteration 47, loss = 0.05173414\n",
      "Iteration 48, loss = 0.04891058\n",
      "Iteration 49, loss = 0.04900123\n",
      "Iteration 50, loss = 0.04754306\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kileymack/opt/anaconda3/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (50) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 1.69207889\n",
      "Iteration 2, loss = 0.32301710\n",
      "Iteration 3, loss = 0.26544210\n",
      "Iteration 4, loss = 0.23315930\n",
      "Iteration 5, loss = 0.20728109\n",
      "Iteration 6, loss = 0.19072516\n",
      "Iteration 7, loss = 0.17381974\n",
      "Iteration 8, loss = 0.16077154\n",
      "Iteration 9, loss = 0.14803279\n",
      "Iteration 10, loss = 0.14027003\n",
      "Iteration 11, loss = 0.13392599\n",
      "Iteration 12, loss = 0.12365962\n",
      "Iteration 13, loss = 0.11820976\n",
      "Iteration 14, loss = 0.11363940\n",
      "Iteration 15, loss = 0.10719395\n",
      "Iteration 16, loss = 0.10348134\n",
      "Iteration 17, loss = 0.09933128\n",
      "Iteration 18, loss = 0.09379409\n",
      "Iteration 19, loss = 0.09115370\n",
      "Iteration 20, loss = 0.08727851\n",
      "Iteration 21, loss = 0.08380845\n",
      "Iteration 22, loss = 0.08230260\n",
      "Iteration 23, loss = 0.07893760\n",
      "Iteration 24, loss = 0.07761256\n",
      "Iteration 25, loss = 0.07549364\n",
      "Iteration 26, loss = 0.07159868\n",
      "Iteration 27, loss = 0.07014051\n",
      "Iteration 28, loss = 0.07017781\n",
      "Iteration 29, loss = 0.06781407\n",
      "Iteration 30, loss = 0.06570494\n",
      "Iteration 31, loss = 0.06480767\n",
      "Iteration 32, loss = 0.06220086\n",
      "Iteration 33, loss = 0.06176892\n",
      "Iteration 34, loss = 0.06161880\n",
      "Iteration 35, loss = 0.05956090\n",
      "Iteration 36, loss = 0.05780417\n",
      "Iteration 37, loss = 0.05604947\n",
      "Iteration 38, loss = 0.05691712\n",
      "Iteration 39, loss = 0.05494065\n",
      "Iteration 40, loss = 0.05343968\n",
      "Iteration 41, loss = 0.05526452\n",
      "Iteration 42, loss = 0.05281836\n",
      "Iteration 43, loss = 0.05100603\n",
      "Iteration 44, loss = 0.05080522\n",
      "Iteration 45, loss = 0.05067996\n",
      "Iteration 46, loss = 0.04873251\n",
      "Iteration 47, loss = 0.04941696\n",
      "Iteration 48, loss = 0.04816024\n",
      "Iteration 49, loss = 0.04753554\n",
      "Iteration 50, loss = 0.04669290\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kileymack/opt/anaconda3/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (50) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 1.45338063\n",
      "Iteration 2, loss = 0.38391235\n",
      "Iteration 3, loss = 0.31473299\n",
      "Iteration 4, loss = 0.28012899\n",
      "Iteration 5, loss = 0.25735495\n",
      "Iteration 6, loss = 0.23904773\n",
      "Iteration 7, loss = 0.22527027\n",
      "Iteration 8, loss = 0.21489632\n",
      "Iteration 9, loss = 0.20499844\n",
      "Iteration 10, loss = 0.19724269\n",
      "Iteration 11, loss = 0.18969387\n",
      "Iteration 12, loss = 0.18410539\n",
      "Iteration 13, loss = 0.17832236\n",
      "Iteration 14, loss = 0.17220591\n",
      "Iteration 15, loss = 0.16703508\n",
      "Iteration 16, loss = 0.16402862\n",
      "Iteration 17, loss = 0.16016243\n",
      "Iteration 18, loss = 0.15551689\n",
      "Iteration 19, loss = 0.15321766\n",
      "Iteration 20, loss = 0.14916037\n",
      "Iteration 21, loss = 0.14659568\n",
      "Iteration 22, loss = 0.14284147\n",
      "Iteration 23, loss = 0.13976623\n",
      "Iteration 24, loss = 0.13806411\n",
      "Iteration 25, loss = 0.13551059\n",
      "Iteration 26, loss = 0.13272509\n",
      "Iteration 27, loss = 0.12970475\n",
      "Iteration 28, loss = 0.12842388\n",
      "Iteration 29, loss = 0.12609827\n",
      "Iteration 30, loss = 0.12457055\n",
      "Iteration 31, loss = 0.12113476\n",
      "Iteration 32, loss = 0.11954105\n",
      "Iteration 33, loss = 0.11771151\n",
      "Iteration 34, loss = 0.11706178\n",
      "Iteration 35, loss = 0.11461389\n",
      "Iteration 36, loss = 0.11378253\n",
      "Iteration 37, loss = 0.11179982\n",
      "Iteration 38, loss = 0.10987637\n",
      "Iteration 39, loss = 0.10818740\n",
      "Iteration 40, loss = 0.10726265\n",
      "Iteration 41, loss = 0.10620064\n",
      "Iteration 42, loss = 0.10461453\n",
      "Iteration 43, loss = 0.10411031\n",
      "Iteration 44, loss = 0.10162422\n",
      "Iteration 45, loss = 0.10130558\n",
      "Iteration 46, loss = 0.09940882\n",
      "Iteration 47, loss = 0.09919038\n",
      "Iteration 48, loss = 0.09715612\n",
      "Iteration 49, loss = 0.09607360\n",
      "Iteration 50, loss = 0.09592956\n",
      "Iteration 51, loss = 0.09460906\n",
      "Iteration 52, loss = 0.09374562\n",
      "Iteration 53, loss = 0.09227425\n",
      "Iteration 54, loss = 0.09127842\n",
      "Iteration 55, loss = 0.09112570\n",
      "Iteration 56, loss = 0.09018870\n",
      "Iteration 57, loss = 0.08832949\n",
      "Iteration 58, loss = 0.08802702\n",
      "Iteration 59, loss = 0.08726715\n",
      "Iteration 60, loss = 0.08563408\n",
      "Iteration 61, loss = 0.08581973\n",
      "Iteration 62, loss = 0.08483833\n",
      "Iteration 63, loss = 0.08422714\n",
      "Iteration 64, loss = 0.08332970\n",
      "Iteration 65, loss = 0.08223280\n",
      "Iteration 66, loss = 0.08209588\n",
      "Iteration 67, loss = 0.08140096\n",
      "Iteration 68, loss = 0.08062616\n",
      "Iteration 69, loss = 0.07923871\n",
      "Iteration 70, loss = 0.07880287\n",
      "Iteration 71, loss = 0.07826281\n",
      "Iteration 72, loss = 0.07778521\n",
      "Iteration 73, loss = 0.07699210\n",
      "Iteration 74, loss = 0.07661580\n",
      "Iteration 75, loss = 0.07654835\n",
      "Iteration 76, loss = 0.07569708\n",
      "Iteration 77, loss = 0.07489573\n",
      "Iteration 78, loss = 0.07404361\n",
      "Iteration 79, loss = 0.07305905\n",
      "Iteration 80, loss = 0.07410530\n",
      "Iteration 81, loss = 0.07233418\n",
      "Iteration 82, loss = 0.07193672\n",
      "Iteration 83, loss = 0.07154808\n",
      "Iteration 84, loss = 0.07182149\n",
      "Iteration 85, loss = 0.07050553\n",
      "Iteration 86, loss = 0.06979444\n",
      "Iteration 87, loss = 0.06942979\n",
      "Iteration 88, loss = 0.06893566\n",
      "Iteration 89, loss = 0.06838566\n",
      "Iteration 90, loss = 0.06863119\n",
      "Iteration 91, loss = 0.06771838\n",
      "Iteration 92, loss = 0.06743942\n",
      "Iteration 93, loss = 0.06660408\n",
      "Iteration 94, loss = 0.06597139\n",
      "Iteration 95, loss = 0.06689008\n",
      "Iteration 96, loss = 0.06557197\n",
      "Iteration 97, loss = 0.06556378\n",
      "Iteration 98, loss = 0.06476250\n",
      "Iteration 99, loss = 0.06404818\n",
      "Iteration 100, loss = 0.06384188\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kileymack/opt/anaconda3/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 1.13302478\n",
      "Iteration 2, loss = 0.37303979\n",
      "Iteration 3, loss = 0.31677272\n",
      "Iteration 4, loss = 0.28340208\n",
      "Iteration 5, loss = 0.26037920\n",
      "Iteration 6, loss = 0.24212492\n",
      "Iteration 7, loss = 0.22767110\n",
      "Iteration 8, loss = 0.21581160\n",
      "Iteration 9, loss = 0.20553427\n",
      "Iteration 10, loss = 0.19712104\n",
      "Iteration 11, loss = 0.18938974\n",
      "Iteration 12, loss = 0.18243195\n",
      "Iteration 13, loss = 0.17673267\n",
      "Iteration 14, loss = 0.17118242\n",
      "Iteration 15, loss = 0.16546337\n",
      "Iteration 16, loss = 0.16064789\n",
      "Iteration 17, loss = 0.15572878\n",
      "Iteration 18, loss = 0.15221019\n",
      "Iteration 19, loss = 0.14802034\n",
      "Iteration 20, loss = 0.14522766\n",
      "Iteration 21, loss = 0.14154119\n",
      "Iteration 22, loss = 0.13832699\n",
      "Iteration 23, loss = 0.13540068\n",
      "Iteration 24, loss = 0.13300605\n",
      "Iteration 25, loss = 0.13049088\n",
      "Iteration 26, loss = 0.12859740\n",
      "Iteration 27, loss = 0.12579631\n",
      "Iteration 28, loss = 0.12374159\n",
      "Iteration 29, loss = 0.12131471\n",
      "Iteration 30, loss = 0.11941692\n",
      "Iteration 31, loss = 0.11754700\n",
      "Iteration 32, loss = 0.11569703\n",
      "Iteration 33, loss = 0.11372123\n",
      "Iteration 34, loss = 0.11192642\n",
      "Iteration 35, loss = 0.10998768\n",
      "Iteration 36, loss = 0.10867747\n",
      "Iteration 37, loss = 0.10707246\n",
      "Iteration 38, loss = 0.10568200\n",
      "Iteration 39, loss = 0.10450544\n",
      "Iteration 40, loss = 0.10312971\n",
      "Iteration 41, loss = 0.10130528\n",
      "Iteration 42, loss = 0.10016568\n",
      "Iteration 43, loss = 0.09896173\n",
      "Iteration 44, loss = 0.09779283\n",
      "Iteration 45, loss = 0.09629557\n",
      "Iteration 46, loss = 0.09524696\n",
      "Iteration 47, loss = 0.09424295\n",
      "Iteration 48, loss = 0.09313566\n",
      "Iteration 49, loss = 0.09236478\n",
      "Iteration 50, loss = 0.09129581\n",
      "Iteration 51, loss = 0.09025521\n",
      "Iteration 52, loss = 0.08933760\n",
      "Iteration 53, loss = 0.08852804\n",
      "Iteration 54, loss = 0.08722348\n",
      "Iteration 55, loss = 0.08656141\n",
      "Iteration 56, loss = 0.08553347\n",
      "Iteration 57, loss = 0.08477023\n",
      "Iteration 58, loss = 0.08400872\n",
      "Iteration 59, loss = 0.08335596\n",
      "Iteration 60, loss = 0.08232087\n",
      "Iteration 61, loss = 0.08154203\n",
      "Iteration 62, loss = 0.08094537\n",
      "Iteration 63, loss = 0.08056253\n",
      "Iteration 64, loss = 0.07972185\n",
      "Iteration 65, loss = 0.07914405\n",
      "Iteration 66, loss = 0.07820288\n",
      "Iteration 67, loss = 0.07760151\n",
      "Iteration 68, loss = 0.07667745\n",
      "Iteration 69, loss = 0.07625259\n",
      "Iteration 70, loss = 0.07547637\n",
      "Iteration 71, loss = 0.07521990\n",
      "Iteration 72, loss = 0.07426017\n",
      "Iteration 73, loss = 0.07367662\n",
      "Iteration 74, loss = 0.07312701\n",
      "Iteration 75, loss = 0.07266976\n",
      "Iteration 76, loss = 0.07188878\n",
      "Iteration 77, loss = 0.07156068\n",
      "Iteration 78, loss = 0.07101122\n",
      "Iteration 79, loss = 0.07014970\n",
      "Iteration 80, loss = 0.06988912\n",
      "Iteration 81, loss = 0.06942304\n",
      "Iteration 82, loss = 0.06877957\n",
      "Iteration 83, loss = 0.06841663\n",
      "Iteration 84, loss = 0.06781291\n",
      "Iteration 85, loss = 0.06738716\n",
      "Iteration 86, loss = 0.06690501\n",
      "Iteration 87, loss = 0.06638393\n",
      "Iteration 88, loss = 0.06569233\n",
      "Iteration 89, loss = 0.06548021\n",
      "Iteration 90, loss = 0.06502859\n",
      "Iteration 91, loss = 0.06452518\n",
      "Iteration 92, loss = 0.06413430\n",
      "Iteration 93, loss = 0.06388564\n",
      "Iteration 94, loss = 0.06330742\n",
      "Iteration 95, loss = 0.06295088\n",
      "Iteration 96, loss = 0.06257414\n",
      "Iteration 97, loss = 0.06236681\n",
      "Iteration 98, loss = 0.06202745\n",
      "Iteration 99, loss = 0.06132183\n",
      "Iteration 100, loss = 0.06103564\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kileymack/opt/anaconda3/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 1.58111654\n",
      "Iteration 2, loss = 0.31851691\n",
      "Iteration 3, loss = 0.25343646\n",
      "Iteration 4, loss = 0.22185343\n",
      "Iteration 5, loss = 0.20201506\n",
      "Iteration 6, loss = 0.18743743\n",
      "Iteration 7, loss = 0.16922282\n",
      "Iteration 8, loss = 0.16092229\n",
      "Iteration 9, loss = 0.15575909\n",
      "Iteration 10, loss = 0.14483664\n",
      "Iteration 11, loss = 0.13627238\n",
      "Iteration 12, loss = 0.13149719\n",
      "Iteration 13, loss = 0.12539791\n",
      "Iteration 14, loss = 0.11616881\n",
      "Iteration 15, loss = 0.11203527\n",
      "Iteration 16, loss = 0.10686851\n",
      "Iteration 17, loss = 0.10552265\n",
      "Iteration 18, loss = 0.10048521\n",
      "Iteration 19, loss = 0.09543827\n",
      "Iteration 20, loss = 0.09586289\n",
      "Iteration 21, loss = 0.09019389\n",
      "Iteration 22, loss = 0.08581433\n",
      "Iteration 23, loss = 0.08849903\n",
      "Iteration 24, loss = 0.08610830\n",
      "Iteration 25, loss = 0.07915110\n",
      "Iteration 26, loss = 0.07747108\n",
      "Iteration 27, loss = 0.07516655\n",
      "Iteration 28, loss = 0.07490685\n",
      "Iteration 29, loss = 0.07372384\n",
      "Iteration 30, loss = 0.07196165\n",
      "Iteration 31, loss = 0.06889322\n",
      "Iteration 32, loss = 0.06714344\n",
      "Iteration 33, loss = 0.06529591\n",
      "Iteration 34, loss = 0.06320735\n",
      "Iteration 35, loss = 0.06323640\n",
      "Iteration 36, loss = 0.06038880\n",
      "Iteration 37, loss = 0.06365006\n",
      "Iteration 38, loss = 0.05786159\n",
      "Iteration 39, loss = 0.05642215\n",
      "Iteration 40, loss = 0.05540536\n",
      "Iteration 41, loss = 0.05865224\n",
      "Iteration 42, loss = 0.05242827\n",
      "Iteration 43, loss = 0.05447825\n",
      "Iteration 44, loss = 0.05402387\n",
      "Iteration 45, loss = 0.05343717\n",
      "Iteration 46, loss = 0.04989339\n",
      "Iteration 47, loss = 0.05173414\n",
      "Iteration 48, loss = 0.04891058\n",
      "Iteration 49, loss = 0.04900123\n",
      "Iteration 50, loss = 0.04754306\n",
      "Iteration 51, loss = 0.04768123\n",
      "Iteration 52, loss = 0.04719951\n",
      "Iteration 53, loss = 0.04552938\n",
      "Iteration 54, loss = 0.04619427\n",
      "Iteration 55, loss = 0.04502908\n",
      "Iteration 56, loss = 0.04503762\n",
      "Iteration 57, loss = 0.04346065\n",
      "Iteration 58, loss = 0.04410892\n",
      "Iteration 59, loss = 0.04347929\n",
      "Iteration 60, loss = 0.04257567\n",
      "Iteration 61, loss = 0.04209916\n",
      "Iteration 62, loss = 0.04078102\n",
      "Iteration 63, loss = 0.04156999\n",
      "Iteration 64, loss = 0.04018867\n",
      "Iteration 65, loss = 0.03953219\n",
      "Iteration 66, loss = 0.04048102\n",
      "Iteration 67, loss = 0.03970935\n",
      "Iteration 68, loss = 0.03902828\n",
      "Iteration 69, loss = 0.03903646\n",
      "Iteration 70, loss = 0.03859932\n",
      "Iteration 71, loss = 0.03699940\n",
      "Iteration 72, loss = 0.03829739\n",
      "Iteration 73, loss = 0.03736966\n",
      "Iteration 74, loss = 0.03632447\n",
      "Iteration 75, loss = 0.03662347\n",
      "Iteration 76, loss = 0.03656138\n",
      "Iteration 77, loss = 0.03478029\n",
      "Iteration 78, loss = 0.03618492\n",
      "Iteration 79, loss = 0.03512752\n",
      "Iteration 80, loss = 0.03539945\n",
      "Iteration 81, loss = 0.03570312\n",
      "Iteration 82, loss = 0.03367528\n",
      "Iteration 83, loss = 0.03396642\n",
      "Iteration 84, loss = 0.03362125\n",
      "Iteration 85, loss = 0.03366934\n",
      "Iteration 86, loss = 0.03412865\n",
      "Iteration 87, loss = 0.03340200\n",
      "Iteration 88, loss = 0.03265785\n",
      "Iteration 89, loss = 0.03179022\n",
      "Iteration 90, loss = 0.03194262\n",
      "Iteration 91, loss = 0.03379056\n",
      "Iteration 92, loss = 0.03149235\n",
      "Iteration 93, loss = 0.03198228\n",
      "Iteration 94, loss = 0.03089518\n",
      "Iteration 95, loss = 0.03121227\n",
      "Iteration 96, loss = 0.03175890\n",
      "Iteration 97, loss = 0.02993526\n",
      "Iteration 98, loss = 0.03081227\n",
      "Iteration 99, loss = 0.03092180\n",
      "Iteration 100, loss = 0.03029315\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kileymack/opt/anaconda3/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 1.69207889\n",
      "Iteration 2, loss = 0.32301710\n",
      "Iteration 3, loss = 0.26544210\n",
      "Iteration 4, loss = 0.23315930\n",
      "Iteration 5, loss = 0.20728109\n",
      "Iteration 6, loss = 0.19072516\n",
      "Iteration 7, loss = 0.17381974\n",
      "Iteration 8, loss = 0.16077154\n",
      "Iteration 9, loss = 0.14803279\n",
      "Iteration 10, loss = 0.14027003\n",
      "Iteration 11, loss = 0.13392599\n",
      "Iteration 12, loss = 0.12365962\n",
      "Iteration 13, loss = 0.11820976\n",
      "Iteration 14, loss = 0.11363940\n",
      "Iteration 15, loss = 0.10719395\n",
      "Iteration 16, loss = 0.10348134\n",
      "Iteration 17, loss = 0.09933128\n",
      "Iteration 18, loss = 0.09379409\n",
      "Iteration 19, loss = 0.09115370\n",
      "Iteration 20, loss = 0.08727851\n",
      "Iteration 21, loss = 0.08380845\n",
      "Iteration 22, loss = 0.08230260\n",
      "Iteration 23, loss = 0.07893760\n",
      "Iteration 24, loss = 0.07761256\n",
      "Iteration 25, loss = 0.07549364\n",
      "Iteration 26, loss = 0.07159868\n",
      "Iteration 27, loss = 0.07014051\n",
      "Iteration 28, loss = 0.07017781\n",
      "Iteration 29, loss = 0.06781407\n",
      "Iteration 30, loss = 0.06570494\n",
      "Iteration 31, loss = 0.06480767\n",
      "Iteration 32, loss = 0.06220086\n",
      "Iteration 33, loss = 0.06176892\n",
      "Iteration 34, loss = 0.06161880\n",
      "Iteration 35, loss = 0.05956090\n",
      "Iteration 36, loss = 0.05780417\n",
      "Iteration 37, loss = 0.05604947\n",
      "Iteration 38, loss = 0.05691712\n",
      "Iteration 39, loss = 0.05494065\n",
      "Iteration 40, loss = 0.05343968\n",
      "Iteration 41, loss = 0.05526452\n",
      "Iteration 42, loss = 0.05281836\n",
      "Iteration 43, loss = 0.05100603\n",
      "Iteration 44, loss = 0.05080522\n",
      "Iteration 45, loss = 0.05067996\n",
      "Iteration 46, loss = 0.04873251\n",
      "Iteration 47, loss = 0.04941696\n",
      "Iteration 48, loss = 0.04816024\n",
      "Iteration 49, loss = 0.04753554\n",
      "Iteration 50, loss = 0.04669290\n",
      "Iteration 51, loss = 0.04645205\n",
      "Iteration 52, loss = 0.04567125\n",
      "Iteration 53, loss = 0.04580998\n",
      "Iteration 54, loss = 0.04409186\n",
      "Iteration 55, loss = 0.04409336\n",
      "Iteration 56, loss = 0.04342063\n",
      "Iteration 57, loss = 0.04310619\n",
      "Iteration 58, loss = 0.04293405\n",
      "Iteration 59, loss = 0.04234464\n",
      "Iteration 60, loss = 0.04201304\n",
      "Iteration 61, loss = 0.04137259\n",
      "Iteration 62, loss = 0.04051388\n",
      "Iteration 63, loss = 0.04093313\n",
      "Iteration 64, loss = 0.04071249\n",
      "Iteration 65, loss = 0.03953651\n",
      "Iteration 66, loss = 0.03908024\n",
      "Iteration 67, loss = 0.03849701\n",
      "Iteration 68, loss = 0.03850678\n",
      "Iteration 69, loss = 0.03875589\n",
      "Iteration 70, loss = 0.03889405\n",
      "Iteration 71, loss = 0.03725556\n",
      "Iteration 72, loss = 0.03737525\n",
      "Iteration 73, loss = 0.03708508\n",
      "Iteration 74, loss = 0.03631942\n",
      "Iteration 75, loss = 0.03682797\n",
      "Iteration 76, loss = 0.03632685\n",
      "Iteration 77, loss = 0.03624097\n",
      "Iteration 78, loss = 0.03661757\n",
      "Iteration 79, loss = 0.03501009\n",
      "Iteration 80, loss = 0.03475613\n",
      "Iteration 81, loss = 0.03538901\n",
      "Iteration 82, loss = 0.03451835\n",
      "Iteration 83, loss = 0.03456433\n",
      "Iteration 84, loss = 0.03511033\n",
      "Iteration 85, loss = 0.03436189\n",
      "Iteration 86, loss = 0.03364419\n",
      "Iteration 87, loss = 0.03372789\n",
      "Iteration 88, loss = 0.03354349\n",
      "Iteration 89, loss = 0.03325212\n",
      "Iteration 90, loss = 0.03344580\n",
      "Iteration 91, loss = 0.03269307\n",
      "Iteration 92, loss = 0.03314798\n",
      "Iteration 93, loss = 0.03253125\n",
      "Iteration 94, loss = 0.03261305\n",
      "Iteration 95, loss = 0.03182714\n",
      "Iteration 96, loss = 0.03189320\n",
      "Iteration 97, loss = 0.03206128\n",
      "Iteration 98, loss = 0.03223576\n",
      "Iteration 99, loss = 0.03188111\n",
      "Iteration 100, loss = 0.03109391\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kileymack/opt/anaconda3/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 1.45338063\n",
      "Iteration 2, loss = 0.38391235\n",
      "Iteration 3, loss = 0.31473299\n",
      "Iteration 4, loss = 0.28012899\n",
      "Iteration 5, loss = 0.25735495\n",
      "Iteration 6, loss = 0.23904773\n",
      "Iteration 7, loss = 0.22527027\n",
      "Iteration 8, loss = 0.21489632\n",
      "Iteration 9, loss = 0.20499844\n",
      "Iteration 10, loss = 0.19724269\n",
      "Iteration 11, loss = 0.18969387\n",
      "Iteration 12, loss = 0.18410539\n",
      "Iteration 13, loss = 0.17832236\n",
      "Iteration 14, loss = 0.17220591\n",
      "Iteration 15, loss = 0.16703508\n",
      "Iteration 16, loss = 0.16402862\n",
      "Iteration 17, loss = 0.16016243\n",
      "Iteration 18, loss = 0.15551689\n",
      "Iteration 19, loss = 0.15321766\n",
      "Iteration 20, loss = 0.14916037\n",
      "Iteration 21, loss = 0.14659568\n",
      "Iteration 22, loss = 0.14284147\n",
      "Iteration 23, loss = 0.13976623\n",
      "Iteration 24, loss = 0.13806411\n",
      "Iteration 25, loss = 0.13551059\n",
      "Iteration 26, loss = 0.13272509\n",
      "Iteration 27, loss = 0.12970475\n",
      "Iteration 28, loss = 0.12842388\n",
      "Iteration 29, loss = 0.12609827\n",
      "Iteration 30, loss = 0.12457055\n",
      "Iteration 31, loss = 0.12113476\n",
      "Iteration 32, loss = 0.11954105\n",
      "Iteration 33, loss = 0.11771151\n",
      "Iteration 34, loss = 0.11706178\n",
      "Iteration 35, loss = 0.11461389\n",
      "Iteration 36, loss = 0.11378253\n",
      "Iteration 37, loss = 0.11179982\n",
      "Iteration 38, loss = 0.10987637\n",
      "Iteration 39, loss = 0.10818740\n",
      "Iteration 40, loss = 0.10726265\n",
      "Iteration 41, loss = 0.10620064\n",
      "Iteration 42, loss = 0.10461453\n",
      "Iteration 43, loss = 0.10411031\n",
      "Iteration 44, loss = 0.10162422\n",
      "Iteration 45, loss = 0.10130558\n",
      "Iteration 46, loss = 0.09940882\n",
      "Iteration 47, loss = 0.09919038\n",
      "Iteration 48, loss = 0.09715612\n",
      "Iteration 49, loss = 0.09607360\n",
      "Iteration 50, loss = 0.09592956\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kileymack/opt/anaconda3/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (50) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 1.13302478\n",
      "Iteration 2, loss = 0.37303979\n",
      "Iteration 3, loss = 0.31677272\n",
      "Iteration 4, loss = 0.28340208\n",
      "Iteration 5, loss = 0.26037920\n",
      "Iteration 6, loss = 0.24212492\n",
      "Iteration 7, loss = 0.22767110\n",
      "Iteration 8, loss = 0.21581160\n",
      "Iteration 9, loss = 0.20553427\n",
      "Iteration 10, loss = 0.19712104\n",
      "Iteration 11, loss = 0.18938974\n",
      "Iteration 12, loss = 0.18243195\n",
      "Iteration 13, loss = 0.17673267\n",
      "Iteration 14, loss = 0.17118242\n",
      "Iteration 15, loss = 0.16546337\n",
      "Iteration 16, loss = 0.16064789\n",
      "Iteration 17, loss = 0.15572878\n",
      "Iteration 18, loss = 0.15221019\n",
      "Iteration 19, loss = 0.14802034\n",
      "Iteration 20, loss = 0.14522766\n",
      "Iteration 21, loss = 0.14154119\n",
      "Iteration 22, loss = 0.13832699\n",
      "Iteration 23, loss = 0.13540068\n",
      "Iteration 24, loss = 0.13300605\n",
      "Iteration 25, loss = 0.13049088\n",
      "Iteration 26, loss = 0.12859740\n",
      "Iteration 27, loss = 0.12579631\n",
      "Iteration 28, loss = 0.12374159\n",
      "Iteration 29, loss = 0.12131471\n",
      "Iteration 30, loss = 0.11941692\n",
      "Iteration 31, loss = 0.11754700\n",
      "Iteration 32, loss = 0.11569703\n",
      "Iteration 33, loss = 0.11372123\n",
      "Iteration 34, loss = 0.11192642\n",
      "Iteration 35, loss = 0.10998768\n",
      "Iteration 36, loss = 0.10867747\n",
      "Iteration 37, loss = 0.10707246\n",
      "Iteration 38, loss = 0.10568200\n",
      "Iteration 39, loss = 0.10450544\n",
      "Iteration 40, loss = 0.10312971\n",
      "Iteration 41, loss = 0.10130528\n",
      "Iteration 42, loss = 0.10016568\n",
      "Iteration 43, loss = 0.09896173\n",
      "Iteration 44, loss = 0.09779283\n",
      "Iteration 45, loss = 0.09629557\n",
      "Iteration 46, loss = 0.09524696\n",
      "Iteration 47, loss = 0.09424295\n",
      "Iteration 48, loss = 0.09313566\n",
      "Iteration 49, loss = 0.09236478\n",
      "Iteration 50, loss = 0.09129581\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kileymack/opt/anaconda3/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (50) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 1.58111654\n",
      "Iteration 2, loss = 0.31851691\n",
      "Iteration 3, loss = 0.25343646\n",
      "Iteration 4, loss = 0.22185343\n",
      "Iteration 5, loss = 0.20201506\n",
      "Iteration 6, loss = 0.18743743\n",
      "Iteration 7, loss = 0.16922282\n",
      "Iteration 8, loss = 0.16092229\n",
      "Iteration 9, loss = 0.15575909\n",
      "Iteration 10, loss = 0.14483664\n",
      "Iteration 11, loss = 0.13627238\n",
      "Iteration 12, loss = 0.13149719\n",
      "Iteration 13, loss = 0.12539791\n",
      "Iteration 14, loss = 0.11616881\n",
      "Iteration 15, loss = 0.11203527\n",
      "Iteration 16, loss = 0.10686851\n",
      "Iteration 17, loss = 0.10552265\n",
      "Iteration 18, loss = 0.10048521\n",
      "Iteration 19, loss = 0.09543827\n",
      "Iteration 20, loss = 0.09586289\n",
      "Iteration 21, loss = 0.09019389\n",
      "Iteration 22, loss = 0.08581433\n",
      "Iteration 23, loss = 0.08849903\n",
      "Iteration 24, loss = 0.08610830\n",
      "Iteration 25, loss = 0.07915110\n",
      "Iteration 26, loss = 0.07747108\n",
      "Iteration 27, loss = 0.07516655\n",
      "Iteration 28, loss = 0.07490685\n",
      "Iteration 29, loss = 0.07372384\n",
      "Iteration 30, loss = 0.07196165\n",
      "Iteration 31, loss = 0.06889322\n",
      "Iteration 32, loss = 0.06714344\n",
      "Iteration 33, loss = 0.06529591\n",
      "Iteration 34, loss = 0.06320735\n",
      "Iteration 35, loss = 0.06323640\n",
      "Iteration 36, loss = 0.06038880\n",
      "Iteration 37, loss = 0.06365006\n",
      "Iteration 38, loss = 0.05786159\n",
      "Iteration 39, loss = 0.05642215\n",
      "Iteration 40, loss = 0.05540536\n",
      "Iteration 41, loss = 0.05865224\n",
      "Iteration 42, loss = 0.05242827\n",
      "Iteration 43, loss = 0.05447825\n",
      "Iteration 44, loss = 0.05402387\n",
      "Iteration 45, loss = 0.05343717\n",
      "Iteration 46, loss = 0.04989339\n",
      "Iteration 47, loss = 0.05173414\n",
      "Iteration 48, loss = 0.04891058\n",
      "Iteration 49, loss = 0.04900123\n",
      "Iteration 50, loss = 0.04754306\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kileymack/opt/anaconda3/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (50) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 1.69207889\n",
      "Iteration 2, loss = 0.32301710\n",
      "Iteration 3, loss = 0.26544210\n",
      "Iteration 4, loss = 0.23315930\n",
      "Iteration 5, loss = 0.20728109\n",
      "Iteration 6, loss = 0.19072516\n",
      "Iteration 7, loss = 0.17381974\n",
      "Iteration 8, loss = 0.16077154\n",
      "Iteration 9, loss = 0.14803279\n",
      "Iteration 10, loss = 0.14027003\n",
      "Iteration 11, loss = 0.13392599\n",
      "Iteration 12, loss = 0.12365962\n",
      "Iteration 13, loss = 0.11820976\n",
      "Iteration 14, loss = 0.11363940\n",
      "Iteration 15, loss = 0.10719395\n",
      "Iteration 16, loss = 0.10348134\n",
      "Iteration 17, loss = 0.09933128\n",
      "Iteration 18, loss = 0.09379409\n",
      "Iteration 19, loss = 0.09115370\n",
      "Iteration 20, loss = 0.08727851\n",
      "Iteration 21, loss = 0.08380845\n",
      "Iteration 22, loss = 0.08230260\n",
      "Iteration 23, loss = 0.07893760\n",
      "Iteration 24, loss = 0.07761256\n",
      "Iteration 25, loss = 0.07549364\n",
      "Iteration 26, loss = 0.07159868\n",
      "Iteration 27, loss = 0.07014051\n",
      "Iteration 28, loss = 0.07017781\n",
      "Iteration 29, loss = 0.06781407\n",
      "Iteration 30, loss = 0.06570494\n",
      "Iteration 31, loss = 0.06480767\n",
      "Iteration 32, loss = 0.06220086\n",
      "Iteration 33, loss = 0.06176892\n",
      "Iteration 34, loss = 0.06161880\n",
      "Iteration 35, loss = 0.05956090\n",
      "Iteration 36, loss = 0.05780417\n",
      "Iteration 37, loss = 0.05604947\n",
      "Iteration 38, loss = 0.05691712\n",
      "Iteration 39, loss = 0.05494065\n",
      "Iteration 40, loss = 0.05343968\n",
      "Iteration 41, loss = 0.05526452\n",
      "Iteration 42, loss = 0.05281836\n",
      "Iteration 43, loss = 0.05100603\n",
      "Iteration 44, loss = 0.05080522\n",
      "Iteration 45, loss = 0.05067996\n",
      "Iteration 46, loss = 0.04873251\n",
      "Iteration 47, loss = 0.04941696\n",
      "Iteration 48, loss = 0.04816024\n",
      "Iteration 49, loss = 0.04753554\n",
      "Iteration 50, loss = 0.04669290\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kileymack/opt/anaconda3/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (50) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 1.45338063\n",
      "Iteration 2, loss = 0.38391235\n",
      "Iteration 3, loss = 0.31473299\n",
      "Iteration 4, loss = 0.28012899\n",
      "Iteration 5, loss = 0.25735495\n",
      "Iteration 6, loss = 0.23904773\n",
      "Iteration 7, loss = 0.22527027\n",
      "Iteration 8, loss = 0.21489632\n",
      "Iteration 9, loss = 0.20499844\n",
      "Iteration 10, loss = 0.19724269\n",
      "Iteration 11, loss = 0.18969387\n",
      "Iteration 12, loss = 0.18410539\n",
      "Iteration 13, loss = 0.17832236\n",
      "Iteration 14, loss = 0.17220591\n",
      "Iteration 15, loss = 0.16703508\n",
      "Iteration 16, loss = 0.16402862\n",
      "Iteration 17, loss = 0.16016243\n",
      "Iteration 18, loss = 0.15551689\n",
      "Iteration 19, loss = 0.15321766\n",
      "Iteration 20, loss = 0.14916037\n",
      "Iteration 21, loss = 0.14659568\n",
      "Iteration 22, loss = 0.14284147\n",
      "Iteration 23, loss = 0.13976623\n",
      "Iteration 24, loss = 0.13806411\n",
      "Iteration 25, loss = 0.13551059\n",
      "Iteration 26, loss = 0.13272509\n",
      "Iteration 27, loss = 0.12970475\n",
      "Iteration 28, loss = 0.12842388\n",
      "Iteration 29, loss = 0.12609827\n",
      "Iteration 30, loss = 0.12457055\n",
      "Iteration 31, loss = 0.12113476\n",
      "Iteration 32, loss = 0.11954105\n",
      "Iteration 33, loss = 0.11771151\n",
      "Iteration 34, loss = 0.11706178\n",
      "Iteration 35, loss = 0.11461389\n",
      "Iteration 36, loss = 0.11378253\n",
      "Iteration 37, loss = 0.11179982\n",
      "Iteration 38, loss = 0.10987637\n",
      "Iteration 39, loss = 0.10818740\n",
      "Iteration 40, loss = 0.10726265\n",
      "Iteration 41, loss = 0.10620064\n",
      "Iteration 42, loss = 0.10461453\n",
      "Iteration 43, loss = 0.10411031\n",
      "Iteration 44, loss = 0.10162422\n",
      "Iteration 45, loss = 0.10130558\n",
      "Iteration 46, loss = 0.09940882\n",
      "Iteration 47, loss = 0.09919038\n",
      "Iteration 48, loss = 0.09715612\n",
      "Iteration 49, loss = 0.09607360\n",
      "Iteration 50, loss = 0.09592956\n",
      "Iteration 51, loss = 0.09460906\n",
      "Iteration 52, loss = 0.09374562\n",
      "Iteration 53, loss = 0.09227425\n",
      "Iteration 54, loss = 0.09127842\n",
      "Iteration 55, loss = 0.09112570\n",
      "Iteration 56, loss = 0.09018870\n",
      "Iteration 57, loss = 0.08832949\n",
      "Iteration 58, loss = 0.08802702\n",
      "Iteration 59, loss = 0.08726715\n",
      "Iteration 60, loss = 0.08563408\n",
      "Iteration 61, loss = 0.08581973\n",
      "Iteration 62, loss = 0.08483833\n",
      "Iteration 63, loss = 0.08422714\n",
      "Iteration 64, loss = 0.08332970\n",
      "Iteration 65, loss = 0.08223280\n",
      "Iteration 66, loss = 0.08209588\n",
      "Iteration 67, loss = 0.08140096\n",
      "Iteration 68, loss = 0.08062616\n",
      "Iteration 69, loss = 0.07923871\n",
      "Iteration 70, loss = 0.07880287\n",
      "Iteration 71, loss = 0.07826281\n",
      "Iteration 72, loss = 0.07778521\n",
      "Iteration 73, loss = 0.07699210\n",
      "Iteration 74, loss = 0.07661580\n",
      "Iteration 75, loss = 0.07654835\n",
      "Iteration 76, loss = 0.07569708\n",
      "Iteration 77, loss = 0.07489573\n",
      "Iteration 78, loss = 0.07404361\n",
      "Iteration 79, loss = 0.07305905\n",
      "Iteration 80, loss = 0.07410530\n",
      "Iteration 81, loss = 0.07233418\n",
      "Iteration 82, loss = 0.07193672\n",
      "Iteration 83, loss = 0.07154808\n",
      "Iteration 84, loss = 0.07182149\n",
      "Iteration 85, loss = 0.07050553\n",
      "Iteration 86, loss = 0.06979444\n",
      "Iteration 87, loss = 0.06942979\n",
      "Iteration 88, loss = 0.06893566\n",
      "Iteration 89, loss = 0.06838566\n",
      "Iteration 90, loss = 0.06863119\n",
      "Iteration 91, loss = 0.06771838\n",
      "Iteration 92, loss = 0.06743942\n",
      "Iteration 93, loss = 0.06660408\n",
      "Iteration 94, loss = 0.06597139\n",
      "Iteration 95, loss = 0.06689008\n",
      "Iteration 96, loss = 0.06557197\n",
      "Iteration 97, loss = 0.06556378\n",
      "Iteration 98, loss = 0.06476250\n",
      "Iteration 99, loss = 0.06404818\n",
      "Iteration 100, loss = 0.06384188\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kileymack/opt/anaconda3/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 1.13302478\n",
      "Iteration 2, loss = 0.37303979\n",
      "Iteration 3, loss = 0.31677272\n",
      "Iteration 4, loss = 0.28340208\n",
      "Iteration 5, loss = 0.26037920\n",
      "Iteration 6, loss = 0.24212492\n",
      "Iteration 7, loss = 0.22767110\n",
      "Iteration 8, loss = 0.21581160\n",
      "Iteration 9, loss = 0.20553427\n",
      "Iteration 10, loss = 0.19712104\n",
      "Iteration 11, loss = 0.18938974\n",
      "Iteration 12, loss = 0.18243195\n",
      "Iteration 13, loss = 0.17673267\n",
      "Iteration 14, loss = 0.17118242\n",
      "Iteration 15, loss = 0.16546337\n",
      "Iteration 16, loss = 0.16064789\n",
      "Iteration 17, loss = 0.15572878\n",
      "Iteration 18, loss = 0.15221019\n",
      "Iteration 19, loss = 0.14802034\n",
      "Iteration 20, loss = 0.14522766\n",
      "Iteration 21, loss = 0.14154119\n",
      "Iteration 22, loss = 0.13832699\n",
      "Iteration 23, loss = 0.13540068\n",
      "Iteration 24, loss = 0.13300605\n",
      "Iteration 25, loss = 0.13049088\n",
      "Iteration 26, loss = 0.12859740\n",
      "Iteration 27, loss = 0.12579631\n",
      "Iteration 28, loss = 0.12374159\n",
      "Iteration 29, loss = 0.12131471\n",
      "Iteration 30, loss = 0.11941692\n",
      "Iteration 31, loss = 0.11754700\n",
      "Iteration 32, loss = 0.11569703\n",
      "Iteration 33, loss = 0.11372123\n",
      "Iteration 34, loss = 0.11192642\n",
      "Iteration 35, loss = 0.10998768\n",
      "Iteration 36, loss = 0.10867747\n",
      "Iteration 37, loss = 0.10707246\n",
      "Iteration 38, loss = 0.10568200\n",
      "Iteration 39, loss = 0.10450544\n",
      "Iteration 40, loss = 0.10312971\n",
      "Iteration 41, loss = 0.10130528\n",
      "Iteration 42, loss = 0.10016568\n",
      "Iteration 43, loss = 0.09896173\n",
      "Iteration 44, loss = 0.09779283\n",
      "Iteration 45, loss = 0.09629557\n",
      "Iteration 46, loss = 0.09524696\n",
      "Iteration 47, loss = 0.09424295\n",
      "Iteration 48, loss = 0.09313566\n",
      "Iteration 49, loss = 0.09236478\n",
      "Iteration 50, loss = 0.09129581\n",
      "Iteration 51, loss = 0.09025521\n",
      "Iteration 52, loss = 0.08933760\n",
      "Iteration 53, loss = 0.08852804\n",
      "Iteration 54, loss = 0.08722348\n",
      "Iteration 55, loss = 0.08656141\n",
      "Iteration 56, loss = 0.08553347\n",
      "Iteration 57, loss = 0.08477023\n",
      "Iteration 58, loss = 0.08400872\n",
      "Iteration 59, loss = 0.08335596\n",
      "Iteration 60, loss = 0.08232087\n",
      "Iteration 61, loss = 0.08154203\n",
      "Iteration 62, loss = 0.08094537\n",
      "Iteration 63, loss = 0.08056253\n",
      "Iteration 64, loss = 0.07972185\n",
      "Iteration 65, loss = 0.07914405\n",
      "Iteration 66, loss = 0.07820288\n",
      "Iteration 67, loss = 0.07760151\n",
      "Iteration 68, loss = 0.07667745\n",
      "Iteration 69, loss = 0.07625259\n",
      "Iteration 70, loss = 0.07547637\n",
      "Iteration 71, loss = 0.07521990\n",
      "Iteration 72, loss = 0.07426017\n",
      "Iteration 73, loss = 0.07367662\n",
      "Iteration 74, loss = 0.07312701\n",
      "Iteration 75, loss = 0.07266976\n",
      "Iteration 76, loss = 0.07188878\n",
      "Iteration 77, loss = 0.07156068\n",
      "Iteration 78, loss = 0.07101122\n",
      "Iteration 79, loss = 0.07014970\n",
      "Iteration 80, loss = 0.06988912\n",
      "Iteration 81, loss = 0.06942304\n",
      "Iteration 82, loss = 0.06877957\n",
      "Iteration 83, loss = 0.06841663\n",
      "Iteration 84, loss = 0.06781291\n",
      "Iteration 85, loss = 0.06738716\n",
      "Iteration 86, loss = 0.06690501\n",
      "Iteration 87, loss = 0.06638393\n",
      "Iteration 88, loss = 0.06569233\n",
      "Iteration 89, loss = 0.06548021\n",
      "Iteration 90, loss = 0.06502859\n",
      "Iteration 91, loss = 0.06452518\n",
      "Iteration 92, loss = 0.06413430\n",
      "Iteration 93, loss = 0.06388564\n",
      "Iteration 94, loss = 0.06330742\n",
      "Iteration 95, loss = 0.06295088\n",
      "Iteration 96, loss = 0.06257414\n",
      "Iteration 97, loss = 0.06236681\n",
      "Iteration 98, loss = 0.06202745\n",
      "Iteration 99, loss = 0.06132183\n",
      "Iteration 100, loss = 0.06103564\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kileymack/opt/anaconda3/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 1.58111654\n",
      "Iteration 2, loss = 0.31851691\n",
      "Iteration 3, loss = 0.25343646\n",
      "Iteration 4, loss = 0.22185343\n",
      "Iteration 5, loss = 0.20201506\n",
      "Iteration 6, loss = 0.18743743\n",
      "Iteration 7, loss = 0.16922282\n",
      "Iteration 8, loss = 0.16092229\n",
      "Iteration 9, loss = 0.15575909\n",
      "Iteration 10, loss = 0.14483664\n",
      "Iteration 11, loss = 0.13627238\n",
      "Iteration 12, loss = 0.13149719\n",
      "Iteration 13, loss = 0.12539791\n",
      "Iteration 14, loss = 0.11616881\n",
      "Iteration 15, loss = 0.11203527\n",
      "Iteration 16, loss = 0.10686851\n",
      "Iteration 17, loss = 0.10552265\n",
      "Iteration 18, loss = 0.10048521\n",
      "Iteration 19, loss = 0.09543827\n",
      "Iteration 20, loss = 0.09586289\n",
      "Iteration 21, loss = 0.09019389\n",
      "Iteration 22, loss = 0.08581433\n",
      "Iteration 23, loss = 0.08849903\n",
      "Iteration 24, loss = 0.08610830\n",
      "Iteration 25, loss = 0.07915110\n",
      "Iteration 26, loss = 0.07747108\n",
      "Iteration 27, loss = 0.07516655\n",
      "Iteration 28, loss = 0.07490685\n",
      "Iteration 29, loss = 0.07372384\n",
      "Iteration 30, loss = 0.07196165\n",
      "Iteration 31, loss = 0.06889322\n",
      "Iteration 32, loss = 0.06714344\n",
      "Iteration 33, loss = 0.06529591\n",
      "Iteration 34, loss = 0.06320735\n",
      "Iteration 35, loss = 0.06323640\n",
      "Iteration 36, loss = 0.06038880\n",
      "Iteration 37, loss = 0.06365006\n",
      "Iteration 38, loss = 0.05786159\n",
      "Iteration 39, loss = 0.05642215\n",
      "Iteration 40, loss = 0.05540536\n",
      "Iteration 41, loss = 0.05865224\n",
      "Iteration 42, loss = 0.05242827\n",
      "Iteration 43, loss = 0.05447825\n",
      "Iteration 44, loss = 0.05402387\n",
      "Iteration 45, loss = 0.05343717\n",
      "Iteration 46, loss = 0.04989339\n",
      "Iteration 47, loss = 0.05173414\n",
      "Iteration 48, loss = 0.04891058\n",
      "Iteration 49, loss = 0.04900123\n",
      "Iteration 50, loss = 0.04754306\n",
      "Iteration 51, loss = 0.04768123\n",
      "Iteration 52, loss = 0.04719951\n",
      "Iteration 53, loss = 0.04552938\n",
      "Iteration 54, loss = 0.04619427\n",
      "Iteration 55, loss = 0.04502908\n",
      "Iteration 56, loss = 0.04503762\n",
      "Iteration 57, loss = 0.04346065\n",
      "Iteration 58, loss = 0.04410892\n",
      "Iteration 59, loss = 0.04347929\n",
      "Iteration 60, loss = 0.04257567\n",
      "Iteration 61, loss = 0.04209916\n",
      "Iteration 62, loss = 0.04078102\n",
      "Iteration 63, loss = 0.04156999\n",
      "Iteration 64, loss = 0.04018867\n",
      "Iteration 65, loss = 0.03953219\n",
      "Iteration 66, loss = 0.04048102\n",
      "Iteration 67, loss = 0.03970935\n",
      "Iteration 68, loss = 0.03902828\n",
      "Iteration 69, loss = 0.03903646\n",
      "Iteration 70, loss = 0.03859932\n",
      "Iteration 71, loss = 0.03699940\n",
      "Iteration 72, loss = 0.03829739\n",
      "Iteration 73, loss = 0.03736966\n",
      "Iteration 74, loss = 0.03632447\n",
      "Iteration 75, loss = 0.03662347\n",
      "Iteration 76, loss = 0.03656138\n",
      "Iteration 77, loss = 0.03478029\n",
      "Iteration 78, loss = 0.03618492\n",
      "Iteration 79, loss = 0.03512752\n",
      "Iteration 80, loss = 0.03539945\n",
      "Iteration 81, loss = 0.03570312\n",
      "Iteration 82, loss = 0.03367528\n",
      "Iteration 83, loss = 0.03396642\n",
      "Iteration 84, loss = 0.03362125\n",
      "Iteration 85, loss = 0.03366934\n",
      "Iteration 86, loss = 0.03412865\n",
      "Iteration 87, loss = 0.03340200\n",
      "Iteration 88, loss = 0.03265785\n",
      "Iteration 89, loss = 0.03179022\n",
      "Iteration 90, loss = 0.03194262\n",
      "Iteration 91, loss = 0.03379056\n",
      "Iteration 92, loss = 0.03149235\n",
      "Iteration 93, loss = 0.03198228\n",
      "Iteration 94, loss = 0.03089518\n",
      "Iteration 95, loss = 0.03121227\n",
      "Iteration 96, loss = 0.03175890\n",
      "Iteration 97, loss = 0.02993526\n",
      "Iteration 98, loss = 0.03081227\n",
      "Iteration 99, loss = 0.03092180\n",
      "Iteration 100, loss = 0.03029315\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kileymack/opt/anaconda3/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 1.69207889\n",
      "Iteration 2, loss = 0.32301710\n",
      "Iteration 3, loss = 0.26544210\n",
      "Iteration 4, loss = 0.23315930\n",
      "Iteration 5, loss = 0.20728109\n",
      "Iteration 6, loss = 0.19072516\n",
      "Iteration 7, loss = 0.17381974\n",
      "Iteration 8, loss = 0.16077154\n",
      "Iteration 9, loss = 0.14803279\n",
      "Iteration 10, loss = 0.14027003\n",
      "Iteration 11, loss = 0.13392599\n",
      "Iteration 12, loss = 0.12365962\n",
      "Iteration 13, loss = 0.11820976\n",
      "Iteration 14, loss = 0.11363940\n",
      "Iteration 15, loss = 0.10719395\n",
      "Iteration 16, loss = 0.10348134\n",
      "Iteration 17, loss = 0.09933128\n",
      "Iteration 18, loss = 0.09379409\n",
      "Iteration 19, loss = 0.09115370\n",
      "Iteration 20, loss = 0.08727851\n",
      "Iteration 21, loss = 0.08380845\n",
      "Iteration 22, loss = 0.08230260\n",
      "Iteration 23, loss = 0.07893760\n",
      "Iteration 24, loss = 0.07761256\n",
      "Iteration 25, loss = 0.07549364\n",
      "Iteration 26, loss = 0.07159868\n",
      "Iteration 27, loss = 0.07014051\n",
      "Iteration 28, loss = 0.07017781\n",
      "Iteration 29, loss = 0.06781407\n",
      "Iteration 30, loss = 0.06570494\n",
      "Iteration 31, loss = 0.06480767\n",
      "Iteration 32, loss = 0.06220086\n",
      "Iteration 33, loss = 0.06176892\n",
      "Iteration 34, loss = 0.06161880\n",
      "Iteration 35, loss = 0.05956090\n",
      "Iteration 36, loss = 0.05780417\n",
      "Iteration 37, loss = 0.05604947\n",
      "Iteration 38, loss = 0.05691712\n",
      "Iteration 39, loss = 0.05494065\n",
      "Iteration 40, loss = 0.05343968\n",
      "Iteration 41, loss = 0.05526452\n",
      "Iteration 42, loss = 0.05281836\n",
      "Iteration 43, loss = 0.05100603\n",
      "Iteration 44, loss = 0.05080522\n",
      "Iteration 45, loss = 0.05067996\n",
      "Iteration 46, loss = 0.04873251\n",
      "Iteration 47, loss = 0.04941696\n",
      "Iteration 48, loss = 0.04816024\n",
      "Iteration 49, loss = 0.04753554\n",
      "Iteration 50, loss = 0.04669290\n",
      "Iteration 51, loss = 0.04645205\n",
      "Iteration 52, loss = 0.04567125\n",
      "Iteration 53, loss = 0.04580998\n",
      "Iteration 54, loss = 0.04409186\n",
      "Iteration 55, loss = 0.04409336\n",
      "Iteration 56, loss = 0.04342063\n",
      "Iteration 57, loss = 0.04310619\n",
      "Iteration 58, loss = 0.04293405\n",
      "Iteration 59, loss = 0.04234464\n",
      "Iteration 60, loss = 0.04201304\n",
      "Iteration 61, loss = 0.04137259\n",
      "Iteration 62, loss = 0.04051388\n",
      "Iteration 63, loss = 0.04093313\n",
      "Iteration 64, loss = 0.04071249\n",
      "Iteration 65, loss = 0.03953651\n",
      "Iteration 66, loss = 0.03908024\n",
      "Iteration 67, loss = 0.03849701\n",
      "Iteration 68, loss = 0.03850678\n",
      "Iteration 69, loss = 0.03875589\n",
      "Iteration 70, loss = 0.03889405\n",
      "Iteration 71, loss = 0.03725556\n",
      "Iteration 72, loss = 0.03737525\n",
      "Iteration 73, loss = 0.03708508\n",
      "Iteration 74, loss = 0.03631942\n",
      "Iteration 75, loss = 0.03682797\n",
      "Iteration 76, loss = 0.03632685\n",
      "Iteration 77, loss = 0.03624097\n",
      "Iteration 78, loss = 0.03661757\n",
      "Iteration 79, loss = 0.03501009\n",
      "Iteration 80, loss = 0.03475613\n",
      "Iteration 81, loss = 0.03538901\n",
      "Iteration 82, loss = 0.03451835\n",
      "Iteration 83, loss = 0.03456433\n",
      "Iteration 84, loss = 0.03511033\n",
      "Iteration 85, loss = 0.03436189\n",
      "Iteration 86, loss = 0.03364419\n",
      "Iteration 87, loss = 0.03372789\n",
      "Iteration 88, loss = 0.03354349\n",
      "Iteration 89, loss = 0.03325212\n",
      "Iteration 90, loss = 0.03344580\n",
      "Iteration 91, loss = 0.03269307\n",
      "Iteration 92, loss = 0.03314798\n",
      "Iteration 93, loss = 0.03253125\n",
      "Iteration 94, loss = 0.03261305\n",
      "Iteration 95, loss = 0.03182714\n",
      "Iteration 96, loss = 0.03189320\n",
      "Iteration 97, loss = 0.03206128\n",
      "Iteration 98, loss = 0.03223576\n",
      "Iteration 99, loss = 0.03188111\n",
      "Iteration 100, loss = 0.03109391\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kileymack/opt/anaconda3/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 1.60295431\n",
      "Iteration 2, loss = 0.40671617\n",
      "Iteration 3, loss = 0.33380716\n",
      "Iteration 4, loss = 0.29419694\n",
      "Iteration 5, loss = 0.26877158\n",
      "Iteration 6, loss = 0.25007124\n",
      "Iteration 7, loss = 0.23634425\n",
      "Iteration 8, loss = 0.22515378\n",
      "Iteration 9, loss = 0.21606987\n",
      "Iteration 10, loss = 0.20796691\n",
      "Iteration 11, loss = 0.20082626\n",
      "Iteration 12, loss = 0.19463396\n",
      "Iteration 13, loss = 0.18864694\n",
      "Iteration 14, loss = 0.18360918\n",
      "Iteration 15, loss = 0.17966730\n",
      "Iteration 16, loss = 0.17409531\n",
      "Iteration 17, loss = 0.17028509\n",
      "Iteration 18, loss = 0.16615929\n",
      "Iteration 19, loss = 0.16230008\n",
      "Iteration 20, loss = 0.15851811\n",
      "Iteration 21, loss = 0.15536006\n",
      "Iteration 22, loss = 0.15238620\n",
      "Iteration 23, loss = 0.14899116\n",
      "Iteration 24, loss = 0.14672116\n",
      "Iteration 25, loss = 0.14411764\n",
      "Iteration 26, loss = 0.14105240\n",
      "Iteration 27, loss = 0.13877460\n",
      "Iteration 28, loss = 0.13739149\n",
      "Iteration 29, loss = 0.13463821\n",
      "Iteration 30, loss = 0.13257596\n",
      "Iteration 31, loss = 0.13067789\n",
      "Iteration 32, loss = 0.12866029\n",
      "Iteration 33, loss = 0.12709274\n",
      "Iteration 34, loss = 0.12512690\n",
      "Iteration 35, loss = 0.12359941\n",
      "Iteration 36, loss = 0.12177539\n",
      "Iteration 37, loss = 0.11981742\n",
      "Iteration 38, loss = 0.11872007\n",
      "Iteration 39, loss = 0.11689265\n",
      "Iteration 40, loss = 0.11606466\n",
      "Iteration 41, loss = 0.11427654\n",
      "Iteration 42, loss = 0.11265955\n",
      "Iteration 43, loss = 0.11154899\n",
      "Iteration 44, loss = 0.11032811\n",
      "Iteration 45, loss = 0.10941192\n",
      "Iteration 46, loss = 0.10800362\n",
      "Iteration 47, loss = 0.10673537\n",
      "Iteration 48, loss = 0.10568475\n",
      "Iteration 49, loss = 0.10453486\n",
      "Iteration 50, loss = 0.10345947\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kileymack/opt/anaconda3/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (50) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 1.22573392\n",
      "Iteration 2, loss = 0.38931056\n",
      "Iteration 3, loss = 0.33174881\n",
      "Iteration 4, loss = 0.29887941\n",
      "Iteration 5, loss = 0.27491607\n",
      "Iteration 6, loss = 0.25685442\n",
      "Iteration 7, loss = 0.24208261\n",
      "Iteration 8, loss = 0.22999136\n",
      "Iteration 9, loss = 0.21963169\n",
      "Iteration 10, loss = 0.21028474\n",
      "Iteration 11, loss = 0.20263824\n",
      "Iteration 12, loss = 0.19558763\n",
      "Iteration 13, loss = 0.18894601\n",
      "Iteration 14, loss = 0.18343216\n",
      "Iteration 15, loss = 0.17765175\n",
      "Iteration 16, loss = 0.17274996\n",
      "Iteration 17, loss = 0.16807657\n",
      "Iteration 18, loss = 0.16382116\n",
      "Iteration 19, loss = 0.15975389\n",
      "Iteration 20, loss = 0.15636464\n",
      "Iteration 21, loss = 0.15299031\n",
      "Iteration 22, loss = 0.14960710\n",
      "Iteration 23, loss = 0.14658472\n",
      "Iteration 24, loss = 0.14363363\n",
      "Iteration 25, loss = 0.14116148\n",
      "Iteration 26, loss = 0.13885385\n",
      "Iteration 27, loss = 0.13620612\n",
      "Iteration 28, loss = 0.13419484\n",
      "Iteration 29, loss = 0.13169559\n",
      "Iteration 30, loss = 0.12971828\n",
      "Iteration 31, loss = 0.12766208\n",
      "Iteration 32, loss = 0.12543829\n",
      "Iteration 33, loss = 0.12376512\n",
      "Iteration 34, loss = 0.12203893\n",
      "Iteration 35, loss = 0.12004813\n",
      "Iteration 36, loss = 0.11879152\n",
      "Iteration 37, loss = 0.11735319\n",
      "Iteration 38, loss = 0.11554428\n",
      "Iteration 39, loss = 0.11442245\n",
      "Iteration 40, loss = 0.11288728\n",
      "Iteration 41, loss = 0.11125734\n",
      "Iteration 42, loss = 0.11010576\n",
      "Iteration 43, loss = 0.10899481\n",
      "Iteration 44, loss = 0.10730293\n",
      "Iteration 45, loss = 0.10607493\n",
      "Iteration 46, loss = 0.10509363\n",
      "Iteration 47, loss = 0.10405984\n",
      "Iteration 48, loss = 0.10306537\n",
      "Iteration 49, loss = 0.10213501\n",
      "Iteration 50, loss = 0.10086361\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kileymack/opt/anaconda3/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (50) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 1.86549242\n",
      "Iteration 2, loss = 0.33339977\n",
      "Iteration 3, loss = 0.26706902\n",
      "Iteration 4, loss = 0.23596067\n",
      "Iteration 5, loss = 0.20484092\n",
      "Iteration 6, loss = 0.18941699\n",
      "Iteration 7, loss = 0.17989739\n",
      "Iteration 8, loss = 0.16566869\n",
      "Iteration 9, loss = 0.15831923\n",
      "Iteration 10, loss = 0.15025322\n",
      "Iteration 11, loss = 0.14304365\n",
      "Iteration 12, loss = 0.13549413\n",
      "Iteration 13, loss = 0.13229708\n",
      "Iteration 14, loss = 0.12503592\n",
      "Iteration 15, loss = 0.12078036\n",
      "Iteration 16, loss = 0.11469771\n",
      "Iteration 17, loss = 0.11239382\n",
      "Iteration 18, loss = 0.10790414\n",
      "Iteration 19, loss = 0.10471148\n",
      "Iteration 20, loss = 0.10219866\n",
      "Iteration 21, loss = 0.09697674\n",
      "Iteration 22, loss = 0.09458249\n",
      "Iteration 23, loss = 0.09442796\n",
      "Iteration 24, loss = 0.09263628\n",
      "Iteration 25, loss = 0.08926552\n",
      "Iteration 26, loss = 0.08548748\n",
      "Iteration 27, loss = 0.08656813\n",
      "Iteration 28, loss = 0.08225488\n",
      "Iteration 29, loss = 0.08204768\n",
      "Iteration 30, loss = 0.08027059\n",
      "Iteration 31, loss = 0.07770675\n",
      "Iteration 32, loss = 0.07553244\n",
      "Iteration 33, loss = 0.07556178\n",
      "Iteration 34, loss = 0.07259852\n",
      "Iteration 35, loss = 0.07284546\n",
      "Iteration 36, loss = 0.07129528\n",
      "Iteration 37, loss = 0.06885676\n",
      "Iteration 38, loss = 0.06918874\n",
      "Iteration 39, loss = 0.06709593\n",
      "Iteration 40, loss = 0.06622294\n",
      "Iteration 41, loss = 0.06516714\n",
      "Iteration 42, loss = 0.06238640\n",
      "Iteration 43, loss = 0.06431879\n",
      "Iteration 44, loss = 0.06222340\n",
      "Iteration 45, loss = 0.06202202\n",
      "Iteration 46, loss = 0.06305549\n",
      "Iteration 47, loss = 0.05972790\n",
      "Iteration 48, loss = 0.05808523\n",
      "Iteration 49, loss = 0.05652438\n",
      "Iteration 50, loss = 0.05729282\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kileymack/opt/anaconda3/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (50) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 1.94387579\n",
      "Iteration 2, loss = 0.34068089\n",
      "Iteration 3, loss = 0.27648870\n",
      "Iteration 4, loss = 0.24107712\n",
      "Iteration 5, loss = 0.21219089\n",
      "Iteration 6, loss = 0.19149125\n",
      "Iteration 7, loss = 0.17788320\n",
      "Iteration 8, loss = 0.16510264\n",
      "Iteration 9, loss = 0.15409484\n",
      "Iteration 10, loss = 0.14888843\n",
      "Iteration 11, loss = 0.13655268\n",
      "Iteration 12, loss = 0.13351511\n",
      "Iteration 13, loss = 0.12492345\n",
      "Iteration 14, loss = 0.11917601\n",
      "Iteration 15, loss = 0.11624154\n",
      "Iteration 16, loss = 0.11165290\n",
      "Iteration 17, loss = 0.10731599\n",
      "Iteration 18, loss = 0.10253138\n",
      "Iteration 19, loss = 0.10096505\n",
      "Iteration 20, loss = 0.09789561\n",
      "Iteration 21, loss = 0.09259976\n",
      "Iteration 22, loss = 0.09196727\n",
      "Iteration 23, loss = 0.08833999\n",
      "Iteration 24, loss = 0.08846699\n",
      "Iteration 25, loss = 0.08355995\n",
      "Iteration 26, loss = 0.08268814\n",
      "Iteration 27, loss = 0.08088953\n",
      "Iteration 28, loss = 0.07921305\n",
      "Iteration 29, loss = 0.07653826\n",
      "Iteration 30, loss = 0.07670052\n",
      "Iteration 31, loss = 0.07460169\n",
      "Iteration 32, loss = 0.07325099\n",
      "Iteration 33, loss = 0.07120896\n",
      "Iteration 34, loss = 0.07017538\n",
      "Iteration 35, loss = 0.06896567\n",
      "Iteration 36, loss = 0.06761540\n",
      "Iteration 37, loss = 0.06728626\n",
      "Iteration 38, loss = 0.06534170\n",
      "Iteration 39, loss = 0.06417898\n",
      "Iteration 40, loss = 0.06383937\n",
      "Iteration 41, loss = 0.06291154\n",
      "Iteration 42, loss = 0.06168710\n",
      "Iteration 43, loss = 0.06022897\n",
      "Iteration 44, loss = 0.05986157\n",
      "Iteration 45, loss = 0.05794373\n",
      "Iteration 46, loss = 0.06007479\n",
      "Iteration 47, loss = 0.05761878\n",
      "Iteration 48, loss = 0.05664265\n",
      "Iteration 49, loss = 0.05656948\n",
      "Iteration 50, loss = 0.05655428\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kileymack/opt/anaconda3/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (50) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 1.60295431\n",
      "Iteration 2, loss = 0.40671617\n",
      "Iteration 3, loss = 0.33380716\n",
      "Iteration 4, loss = 0.29419694\n",
      "Iteration 5, loss = 0.26877158\n",
      "Iteration 6, loss = 0.25007124\n",
      "Iteration 7, loss = 0.23634425\n",
      "Iteration 8, loss = 0.22515378\n",
      "Iteration 9, loss = 0.21606987\n",
      "Iteration 10, loss = 0.20796691\n",
      "Iteration 11, loss = 0.20082626\n",
      "Iteration 12, loss = 0.19463396\n",
      "Iteration 13, loss = 0.18864694\n",
      "Iteration 14, loss = 0.18360918\n",
      "Iteration 15, loss = 0.17966730\n",
      "Iteration 16, loss = 0.17409531\n",
      "Iteration 17, loss = 0.17028509\n",
      "Iteration 18, loss = 0.16615929\n",
      "Iteration 19, loss = 0.16230008\n",
      "Iteration 20, loss = 0.15851811\n",
      "Iteration 21, loss = 0.15536006\n",
      "Iteration 22, loss = 0.15238620\n",
      "Iteration 23, loss = 0.14899116\n",
      "Iteration 24, loss = 0.14672116\n",
      "Iteration 25, loss = 0.14411764\n",
      "Iteration 26, loss = 0.14105240\n",
      "Iteration 27, loss = 0.13877460\n",
      "Iteration 28, loss = 0.13739149\n",
      "Iteration 29, loss = 0.13463821\n",
      "Iteration 30, loss = 0.13257596\n",
      "Iteration 31, loss = 0.13067789\n",
      "Iteration 32, loss = 0.12866029\n",
      "Iteration 33, loss = 0.12709274\n",
      "Iteration 34, loss = 0.12512690\n",
      "Iteration 35, loss = 0.12359941\n",
      "Iteration 36, loss = 0.12177539\n",
      "Iteration 37, loss = 0.11981742\n",
      "Iteration 38, loss = 0.11872007\n",
      "Iteration 39, loss = 0.11689265\n",
      "Iteration 40, loss = 0.11606466\n",
      "Iteration 41, loss = 0.11427654\n",
      "Iteration 42, loss = 0.11265955\n",
      "Iteration 43, loss = 0.11154899\n",
      "Iteration 44, loss = 0.11032811\n",
      "Iteration 45, loss = 0.10941192\n",
      "Iteration 46, loss = 0.10800362\n",
      "Iteration 47, loss = 0.10673537\n",
      "Iteration 48, loss = 0.10568475\n",
      "Iteration 49, loss = 0.10453486\n",
      "Iteration 50, loss = 0.10345947\n",
      "Iteration 51, loss = 0.10225684\n",
      "Iteration 52, loss = 0.10094671\n",
      "Iteration 53, loss = 0.10002507\n",
      "Iteration 54, loss = 0.09913258\n",
      "Iteration 55, loss = 0.09827959\n",
      "Iteration 56, loss = 0.09712081\n",
      "Iteration 57, loss = 0.09623791\n",
      "Iteration 58, loss = 0.09541099\n",
      "Iteration 59, loss = 0.09484700\n",
      "Iteration 60, loss = 0.09348496\n",
      "Iteration 61, loss = 0.09249324\n",
      "Iteration 62, loss = 0.09201165\n",
      "Iteration 63, loss = 0.09076011\n",
      "Iteration 64, loss = 0.09019318\n",
      "Iteration 65, loss = 0.08878885\n",
      "Iteration 66, loss = 0.08848696\n",
      "Iteration 67, loss = 0.08769138\n",
      "Iteration 68, loss = 0.08739248\n",
      "Iteration 69, loss = 0.08663055\n",
      "Iteration 70, loss = 0.08585805\n",
      "Iteration 71, loss = 0.08494419\n",
      "Iteration 72, loss = 0.08425809\n",
      "Iteration 73, loss = 0.08371521\n",
      "Iteration 74, loss = 0.08271945\n",
      "Iteration 75, loss = 0.08261502\n",
      "Iteration 76, loss = 0.08191166\n",
      "Iteration 77, loss = 0.08124714\n",
      "Iteration 78, loss = 0.08102311\n",
      "Iteration 79, loss = 0.07995484\n",
      "Iteration 80, loss = 0.07935654\n",
      "Iteration 81, loss = 0.07897526\n",
      "Iteration 82, loss = 0.07806485\n",
      "Iteration 83, loss = 0.07795590\n",
      "Iteration 84, loss = 0.07746545\n",
      "Iteration 85, loss = 0.07624402\n",
      "Iteration 86, loss = 0.07625668\n",
      "Iteration 87, loss = 0.07553168\n",
      "Iteration 88, loss = 0.07535639\n",
      "Iteration 89, loss = 0.07442174\n",
      "Iteration 90, loss = 0.07446864\n",
      "Iteration 91, loss = 0.07396518\n",
      "Iteration 92, loss = 0.07389366\n",
      "Iteration 93, loss = 0.07316913\n",
      "Iteration 94, loss = 0.07285308\n",
      "Iteration 95, loss = 0.07216395\n",
      "Iteration 96, loss = 0.07195419\n",
      "Iteration 97, loss = 0.07107147\n",
      "Iteration 98, loss = 0.07082477\n",
      "Iteration 99, loss = 0.07030152\n",
      "Iteration 100, loss = 0.07015159\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kileymack/opt/anaconda3/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 1.22573392\n",
      "Iteration 2, loss = 0.38931056\n",
      "Iteration 3, loss = 0.33174881\n",
      "Iteration 4, loss = 0.29887941\n",
      "Iteration 5, loss = 0.27491607\n",
      "Iteration 6, loss = 0.25685442\n",
      "Iteration 7, loss = 0.24208261\n",
      "Iteration 8, loss = 0.22999136\n",
      "Iteration 9, loss = 0.21963169\n",
      "Iteration 10, loss = 0.21028474\n",
      "Iteration 11, loss = 0.20263824\n",
      "Iteration 12, loss = 0.19558763\n",
      "Iteration 13, loss = 0.18894601\n",
      "Iteration 14, loss = 0.18343216\n",
      "Iteration 15, loss = 0.17765175\n",
      "Iteration 16, loss = 0.17274996\n",
      "Iteration 17, loss = 0.16807657\n",
      "Iteration 18, loss = 0.16382116\n",
      "Iteration 19, loss = 0.15975389\n",
      "Iteration 20, loss = 0.15636464\n",
      "Iteration 21, loss = 0.15299031\n",
      "Iteration 22, loss = 0.14960710\n",
      "Iteration 23, loss = 0.14658472\n",
      "Iteration 24, loss = 0.14363363\n",
      "Iteration 25, loss = 0.14116148\n",
      "Iteration 26, loss = 0.13885385\n",
      "Iteration 27, loss = 0.13620612\n",
      "Iteration 28, loss = 0.13419484\n",
      "Iteration 29, loss = 0.13169559\n",
      "Iteration 30, loss = 0.12971828\n",
      "Iteration 31, loss = 0.12766208\n",
      "Iteration 32, loss = 0.12543829\n",
      "Iteration 33, loss = 0.12376512\n",
      "Iteration 34, loss = 0.12203893\n",
      "Iteration 35, loss = 0.12004813\n",
      "Iteration 36, loss = 0.11879152\n",
      "Iteration 37, loss = 0.11735319\n",
      "Iteration 38, loss = 0.11554428\n",
      "Iteration 39, loss = 0.11442245\n",
      "Iteration 40, loss = 0.11288728\n",
      "Iteration 41, loss = 0.11125734\n",
      "Iteration 42, loss = 0.11010576\n",
      "Iteration 43, loss = 0.10899481\n",
      "Iteration 44, loss = 0.10730293\n",
      "Iteration 45, loss = 0.10607493\n",
      "Iteration 46, loss = 0.10509363\n",
      "Iteration 47, loss = 0.10405984\n",
      "Iteration 48, loss = 0.10306537\n",
      "Iteration 49, loss = 0.10213501\n",
      "Iteration 50, loss = 0.10086361\n",
      "Iteration 51, loss = 0.10013621\n",
      "Iteration 52, loss = 0.09879570\n",
      "Iteration 53, loss = 0.09822717\n",
      "Iteration 54, loss = 0.09737817\n",
      "Iteration 55, loss = 0.09632476\n",
      "Iteration 56, loss = 0.09556218\n",
      "Iteration 57, loss = 0.09445759\n",
      "Iteration 58, loss = 0.09384971\n",
      "Iteration 59, loss = 0.09302364\n",
      "Iteration 60, loss = 0.09197843\n",
      "Iteration 61, loss = 0.09121116\n",
      "Iteration 62, loss = 0.09067606\n",
      "Iteration 63, loss = 0.08967580\n",
      "Iteration 64, loss = 0.08895107\n",
      "Iteration 65, loss = 0.08835306\n",
      "Iteration 66, loss = 0.08758680\n",
      "Iteration 67, loss = 0.08661738\n",
      "Iteration 68, loss = 0.08611297\n",
      "Iteration 69, loss = 0.08561911\n",
      "Iteration 70, loss = 0.08506854\n",
      "Iteration 71, loss = 0.08408204\n",
      "Iteration 72, loss = 0.08373268\n",
      "Iteration 73, loss = 0.08271049\n",
      "Iteration 74, loss = 0.08205552\n",
      "Iteration 75, loss = 0.08181523\n",
      "Iteration 76, loss = 0.08103587\n",
      "Iteration 77, loss = 0.08057800\n",
      "Iteration 78, loss = 0.07981207\n",
      "Iteration 79, loss = 0.07949133\n",
      "Iteration 80, loss = 0.07877330\n",
      "Iteration 81, loss = 0.07830289\n",
      "Iteration 82, loss = 0.07776469\n",
      "Iteration 83, loss = 0.07723927\n",
      "Iteration 84, loss = 0.07693225\n",
      "Iteration 85, loss = 0.07646202\n",
      "Iteration 86, loss = 0.07562227\n",
      "Iteration 87, loss = 0.07539997\n",
      "Iteration 88, loss = 0.07472372\n",
      "Iteration 89, loss = 0.07450868\n",
      "Iteration 90, loss = 0.07428958\n",
      "Iteration 91, loss = 0.07337960\n",
      "Iteration 92, loss = 0.07304578\n",
      "Iteration 93, loss = 0.07258490\n",
      "Iteration 94, loss = 0.07194511\n",
      "Iteration 95, loss = 0.07177103\n",
      "Iteration 96, loss = 0.07131035\n",
      "Iteration 97, loss = 0.07078070\n",
      "Iteration 98, loss = 0.07062996\n",
      "Iteration 99, loss = 0.07002603\n",
      "Iteration 100, loss = 0.06947035\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kileymack/opt/anaconda3/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 1.86549242\n",
      "Iteration 2, loss = 0.33339977\n",
      "Iteration 3, loss = 0.26706902\n",
      "Iteration 4, loss = 0.23596067\n",
      "Iteration 5, loss = 0.20484092\n",
      "Iteration 6, loss = 0.18941699\n",
      "Iteration 7, loss = 0.17989739\n",
      "Iteration 8, loss = 0.16566869\n",
      "Iteration 9, loss = 0.15831923\n",
      "Iteration 10, loss = 0.15025322\n",
      "Iteration 11, loss = 0.14304365\n",
      "Iteration 12, loss = 0.13549413\n",
      "Iteration 13, loss = 0.13229708\n",
      "Iteration 14, loss = 0.12503592\n",
      "Iteration 15, loss = 0.12078036\n",
      "Iteration 16, loss = 0.11469771\n",
      "Iteration 17, loss = 0.11239382\n",
      "Iteration 18, loss = 0.10790414\n",
      "Iteration 19, loss = 0.10471148\n",
      "Iteration 20, loss = 0.10219866\n",
      "Iteration 21, loss = 0.09697674\n",
      "Iteration 22, loss = 0.09458249\n",
      "Iteration 23, loss = 0.09442796\n",
      "Iteration 24, loss = 0.09263628\n",
      "Iteration 25, loss = 0.08926552\n",
      "Iteration 26, loss = 0.08548748\n",
      "Iteration 27, loss = 0.08656813\n",
      "Iteration 28, loss = 0.08225488\n",
      "Iteration 29, loss = 0.08204768\n",
      "Iteration 30, loss = 0.08027059\n",
      "Iteration 31, loss = 0.07770675\n",
      "Iteration 32, loss = 0.07553244\n",
      "Iteration 33, loss = 0.07556178\n",
      "Iteration 34, loss = 0.07259852\n",
      "Iteration 35, loss = 0.07284546\n",
      "Iteration 36, loss = 0.07129528\n",
      "Iteration 37, loss = 0.06885676\n",
      "Iteration 38, loss = 0.06918874\n",
      "Iteration 39, loss = 0.06709593\n",
      "Iteration 40, loss = 0.06622294\n",
      "Iteration 41, loss = 0.06516714\n",
      "Iteration 42, loss = 0.06238640\n",
      "Iteration 43, loss = 0.06431879\n",
      "Iteration 44, loss = 0.06222340\n",
      "Iteration 45, loss = 0.06202202\n",
      "Iteration 46, loss = 0.06305549\n",
      "Iteration 47, loss = 0.05972790\n",
      "Iteration 48, loss = 0.05808523\n",
      "Iteration 49, loss = 0.05652438\n",
      "Iteration 50, loss = 0.05729282\n",
      "Iteration 51, loss = 0.05565622\n",
      "Iteration 52, loss = 0.05796799\n",
      "Iteration 53, loss = 0.05629684\n",
      "Iteration 54, loss = 0.05595208\n",
      "Iteration 55, loss = 0.05330597\n",
      "Iteration 56, loss = 0.05349645\n",
      "Iteration 57, loss = 0.05244369\n",
      "Iteration 58, loss = 0.05261063\n",
      "Iteration 59, loss = 0.05138813\n",
      "Iteration 60, loss = 0.05189190\n",
      "Iteration 61, loss = 0.05036085\n",
      "Iteration 62, loss = 0.05045366\n",
      "Iteration 63, loss = 0.04914347\n",
      "Iteration 64, loss = 0.05061806\n",
      "Iteration 65, loss = 0.04771308\n",
      "Iteration 66, loss = 0.04784301\n",
      "Iteration 67, loss = 0.04788998\n",
      "Iteration 68, loss = 0.04732257\n",
      "Iteration 69, loss = 0.04712506\n",
      "Iteration 70, loss = 0.04617195\n",
      "Iteration 71, loss = 0.04668606\n",
      "Iteration 72, loss = 0.04658727\n",
      "Iteration 73, loss = 0.04457674\n",
      "Iteration 74, loss = 0.04487378\n",
      "Iteration 75, loss = 0.04474491\n",
      "Iteration 76, loss = 0.04443139\n",
      "Iteration 77, loss = 0.04406673\n",
      "Iteration 78, loss = 0.04460548\n",
      "Iteration 79, loss = 0.04209773\n",
      "Iteration 80, loss = 0.04196006\n",
      "Iteration 81, loss = 0.04268562\n",
      "Iteration 82, loss = 0.04160769\n",
      "Iteration 83, loss = 0.04231372\n",
      "Iteration 84, loss = 0.04085942\n",
      "Iteration 85, loss = 0.04111906\n",
      "Iteration 86, loss = 0.04141635\n",
      "Iteration 87, loss = 0.03994245\n",
      "Iteration 88, loss = 0.04074934\n",
      "Iteration 89, loss = 0.03969979\n",
      "Iteration 90, loss = 0.03974406\n",
      "Iteration 91, loss = 0.03959085\n",
      "Iteration 92, loss = 0.03839601\n",
      "Iteration 93, loss = 0.04037773\n",
      "Iteration 94, loss = 0.03827742\n",
      "Iteration 95, loss = 0.03812884\n",
      "Iteration 96, loss = 0.04009286\n",
      "Iteration 97, loss = 0.03619125\n",
      "Iteration 98, loss = 0.03728905\n",
      "Iteration 99, loss = 0.03734180\n",
      "Iteration 100, loss = 0.03662989\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kileymack/opt/anaconda3/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 1.94387579\n",
      "Iteration 2, loss = 0.34068089\n",
      "Iteration 3, loss = 0.27648870\n",
      "Iteration 4, loss = 0.24107712\n",
      "Iteration 5, loss = 0.21219089\n",
      "Iteration 6, loss = 0.19149125\n",
      "Iteration 7, loss = 0.17788320\n",
      "Iteration 8, loss = 0.16510264\n",
      "Iteration 9, loss = 0.15409484\n",
      "Iteration 10, loss = 0.14888843\n",
      "Iteration 11, loss = 0.13655268\n",
      "Iteration 12, loss = 0.13351511\n",
      "Iteration 13, loss = 0.12492345\n",
      "Iteration 14, loss = 0.11917601\n",
      "Iteration 15, loss = 0.11624154\n",
      "Iteration 16, loss = 0.11165290\n",
      "Iteration 17, loss = 0.10731599\n",
      "Iteration 18, loss = 0.10253138\n",
      "Iteration 19, loss = 0.10096505\n",
      "Iteration 20, loss = 0.09789561\n",
      "Iteration 21, loss = 0.09259976\n",
      "Iteration 22, loss = 0.09196727\n",
      "Iteration 23, loss = 0.08833999\n",
      "Iteration 24, loss = 0.08846699\n",
      "Iteration 25, loss = 0.08355995\n",
      "Iteration 26, loss = 0.08268814\n",
      "Iteration 27, loss = 0.08088953\n",
      "Iteration 28, loss = 0.07921305\n",
      "Iteration 29, loss = 0.07653826\n",
      "Iteration 30, loss = 0.07670052\n",
      "Iteration 31, loss = 0.07460169\n",
      "Iteration 32, loss = 0.07325099\n",
      "Iteration 33, loss = 0.07120896\n",
      "Iteration 34, loss = 0.07017538\n",
      "Iteration 35, loss = 0.06896567\n",
      "Iteration 36, loss = 0.06761540\n",
      "Iteration 37, loss = 0.06728626\n",
      "Iteration 38, loss = 0.06534170\n",
      "Iteration 39, loss = 0.06417898\n",
      "Iteration 40, loss = 0.06383937\n",
      "Iteration 41, loss = 0.06291154\n",
      "Iteration 42, loss = 0.06168710\n",
      "Iteration 43, loss = 0.06022897\n",
      "Iteration 44, loss = 0.05986157\n",
      "Iteration 45, loss = 0.05794373\n",
      "Iteration 46, loss = 0.06007479\n",
      "Iteration 47, loss = 0.05761878\n",
      "Iteration 48, loss = 0.05664265\n",
      "Iteration 49, loss = 0.05656948\n",
      "Iteration 50, loss = 0.05655428\n",
      "Iteration 51, loss = 0.05412188\n",
      "Iteration 52, loss = 0.05296805\n",
      "Iteration 53, loss = 0.05338880\n",
      "Iteration 54, loss = 0.05297252\n",
      "Iteration 55, loss = 0.05249656\n",
      "Iteration 56, loss = 0.05240320\n",
      "Iteration 57, loss = 0.05208648\n",
      "Iteration 58, loss = 0.05010442\n",
      "Iteration 59, loss = 0.05087728\n",
      "Iteration 60, loss = 0.05022008\n",
      "Iteration 61, loss = 0.04916962\n",
      "Iteration 62, loss = 0.04916266\n",
      "Iteration 63, loss = 0.04803641\n",
      "Iteration 64, loss = 0.04787025\n",
      "Iteration 65, loss = 0.04789879\n",
      "Iteration 66, loss = 0.04737697\n",
      "Iteration 67, loss = 0.04671794\n",
      "Iteration 68, loss = 0.04607679\n",
      "Iteration 69, loss = 0.04686306\n",
      "Iteration 70, loss = 0.04554341\n",
      "Iteration 71, loss = 0.04546001\n",
      "Iteration 72, loss = 0.04555499\n",
      "Iteration 73, loss = 0.04490529\n",
      "Iteration 74, loss = 0.04434546\n",
      "Iteration 75, loss = 0.04381980\n",
      "Iteration 76, loss = 0.04491479\n",
      "Iteration 77, loss = 0.04355824\n",
      "Iteration 78, loss = 0.04303459\n",
      "Iteration 79, loss = 0.04324332\n",
      "Iteration 80, loss = 0.04284999\n",
      "Iteration 81, loss = 0.04290003\n",
      "Iteration 82, loss = 0.04217358\n",
      "Iteration 83, loss = 0.04172415\n",
      "Iteration 84, loss = 0.04184897\n",
      "Iteration 85, loss = 0.04130623\n",
      "Iteration 86, loss = 0.04157495\n",
      "Iteration 87, loss = 0.04070564\n",
      "Iteration 88, loss = 0.04095159\n",
      "Iteration 89, loss = 0.04026835\n",
      "Iteration 90, loss = 0.04118779\n",
      "Iteration 91, loss = 0.04014345\n",
      "Iteration 92, loss = 0.03937140\n",
      "Iteration 93, loss = 0.03977644\n",
      "Iteration 94, loss = 0.03945955\n",
      "Iteration 95, loss = 0.03958956\n",
      "Iteration 96, loss = 0.03881036\n",
      "Iteration 97, loss = 0.03872920\n",
      "Iteration 98, loss = 0.03894394\n",
      "Iteration 99, loss = 0.03851590\n",
      "Iteration 100, loss = 0.03840433\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kileymack/opt/anaconda3/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 1.60295431\n",
      "Iteration 2, loss = 0.40671617\n",
      "Iteration 3, loss = 0.33380716\n",
      "Iteration 4, loss = 0.29419694\n",
      "Iteration 5, loss = 0.26877158\n",
      "Iteration 6, loss = 0.25007124\n",
      "Iteration 7, loss = 0.23634425\n",
      "Iteration 8, loss = 0.22515378\n",
      "Iteration 9, loss = 0.21606987\n",
      "Iteration 10, loss = 0.20796691\n",
      "Iteration 11, loss = 0.20082626\n",
      "Iteration 12, loss = 0.19463396\n",
      "Iteration 13, loss = 0.18864694\n",
      "Iteration 14, loss = 0.18360918\n",
      "Iteration 15, loss = 0.17966730\n",
      "Iteration 16, loss = 0.17409531\n",
      "Iteration 17, loss = 0.17028509\n",
      "Iteration 18, loss = 0.16615929\n",
      "Iteration 19, loss = 0.16230008\n",
      "Iteration 20, loss = 0.15851811\n",
      "Iteration 21, loss = 0.15536006\n",
      "Iteration 22, loss = 0.15238620\n",
      "Iteration 23, loss = 0.14899116\n",
      "Iteration 24, loss = 0.14672116\n",
      "Iteration 25, loss = 0.14411764\n",
      "Iteration 26, loss = 0.14105240\n",
      "Iteration 27, loss = 0.13877460\n",
      "Iteration 28, loss = 0.13739149\n",
      "Iteration 29, loss = 0.13463821\n",
      "Iteration 30, loss = 0.13257596\n",
      "Iteration 31, loss = 0.13067789\n",
      "Iteration 32, loss = 0.12866029\n",
      "Iteration 33, loss = 0.12709274\n",
      "Iteration 34, loss = 0.12512690\n",
      "Iteration 35, loss = 0.12359941\n",
      "Iteration 36, loss = 0.12177539\n",
      "Iteration 37, loss = 0.11981742\n",
      "Iteration 38, loss = 0.11872007\n",
      "Iteration 39, loss = 0.11689265\n",
      "Iteration 40, loss = 0.11606466\n",
      "Iteration 41, loss = 0.11427654\n",
      "Iteration 42, loss = 0.11265955\n",
      "Iteration 43, loss = 0.11154899\n",
      "Iteration 44, loss = 0.11032811\n",
      "Iteration 45, loss = 0.10941192\n",
      "Iteration 46, loss = 0.10800362\n",
      "Iteration 47, loss = 0.10673537\n",
      "Iteration 48, loss = 0.10568475\n",
      "Iteration 49, loss = 0.10453486\n",
      "Iteration 50, loss = 0.10345947\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kileymack/opt/anaconda3/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (50) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 1.22573392\n",
      "Iteration 2, loss = 0.38931056\n",
      "Iteration 3, loss = 0.33174881\n",
      "Iteration 4, loss = 0.29887941\n",
      "Iteration 5, loss = 0.27491607\n",
      "Iteration 6, loss = 0.25685442\n",
      "Iteration 7, loss = 0.24208261\n",
      "Iteration 8, loss = 0.22999136\n",
      "Iteration 9, loss = 0.21963169\n",
      "Iteration 10, loss = 0.21028474\n",
      "Iteration 11, loss = 0.20263824\n",
      "Iteration 12, loss = 0.19558763\n",
      "Iteration 13, loss = 0.18894601\n",
      "Iteration 14, loss = 0.18343216\n",
      "Iteration 15, loss = 0.17765175\n",
      "Iteration 16, loss = 0.17274996\n",
      "Iteration 17, loss = 0.16807657\n",
      "Iteration 18, loss = 0.16382116\n",
      "Iteration 19, loss = 0.15975389\n",
      "Iteration 20, loss = 0.15636464\n",
      "Iteration 21, loss = 0.15299031\n",
      "Iteration 22, loss = 0.14960710\n",
      "Iteration 23, loss = 0.14658472\n",
      "Iteration 24, loss = 0.14363363\n",
      "Iteration 25, loss = 0.14116148\n",
      "Iteration 26, loss = 0.13885385\n",
      "Iteration 27, loss = 0.13620612\n",
      "Iteration 28, loss = 0.13419484\n",
      "Iteration 29, loss = 0.13169559\n",
      "Iteration 30, loss = 0.12971828\n",
      "Iteration 31, loss = 0.12766208\n",
      "Iteration 32, loss = 0.12543829\n",
      "Iteration 33, loss = 0.12376512\n",
      "Iteration 34, loss = 0.12203893\n",
      "Iteration 35, loss = 0.12004813\n",
      "Iteration 36, loss = 0.11879152\n",
      "Iteration 37, loss = 0.11735319\n",
      "Iteration 38, loss = 0.11554428\n",
      "Iteration 39, loss = 0.11442245\n",
      "Iteration 40, loss = 0.11288728\n",
      "Iteration 41, loss = 0.11125734\n",
      "Iteration 42, loss = 0.11010576\n",
      "Iteration 43, loss = 0.10899481\n",
      "Iteration 44, loss = 0.10730293\n",
      "Iteration 45, loss = 0.10607493\n",
      "Iteration 46, loss = 0.10509363\n",
      "Iteration 47, loss = 0.10405984\n",
      "Iteration 48, loss = 0.10306537\n",
      "Iteration 49, loss = 0.10213501\n",
      "Iteration 50, loss = 0.10086361\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kileymack/opt/anaconda3/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (50) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 1.86549242\n",
      "Iteration 2, loss = 0.33339977\n",
      "Iteration 3, loss = 0.26706902\n",
      "Iteration 4, loss = 0.23596067\n",
      "Iteration 5, loss = 0.20484092\n",
      "Iteration 6, loss = 0.18941699\n",
      "Iteration 7, loss = 0.17989739\n",
      "Iteration 8, loss = 0.16566869\n",
      "Iteration 9, loss = 0.15831923\n",
      "Iteration 10, loss = 0.15025322\n",
      "Iteration 11, loss = 0.14304365\n",
      "Iteration 12, loss = 0.13549413\n",
      "Iteration 13, loss = 0.13229708\n",
      "Iteration 14, loss = 0.12503592\n",
      "Iteration 15, loss = 0.12078036\n",
      "Iteration 16, loss = 0.11469771\n",
      "Iteration 17, loss = 0.11239382\n",
      "Iteration 18, loss = 0.10790414\n",
      "Iteration 19, loss = 0.10471148\n",
      "Iteration 20, loss = 0.10219866\n",
      "Iteration 21, loss = 0.09697674\n",
      "Iteration 22, loss = 0.09458249\n",
      "Iteration 23, loss = 0.09442796\n",
      "Iteration 24, loss = 0.09263628\n",
      "Iteration 25, loss = 0.08926552\n",
      "Iteration 26, loss = 0.08548748\n",
      "Iteration 27, loss = 0.08656813\n",
      "Iteration 28, loss = 0.08225488\n",
      "Iteration 29, loss = 0.08204768\n",
      "Iteration 30, loss = 0.08027059\n",
      "Iteration 31, loss = 0.07770675\n",
      "Iteration 32, loss = 0.07553244\n",
      "Iteration 33, loss = 0.07556178\n",
      "Iteration 34, loss = 0.07259852\n",
      "Iteration 35, loss = 0.07284546\n",
      "Iteration 36, loss = 0.07129528\n",
      "Iteration 37, loss = 0.06885676\n",
      "Iteration 38, loss = 0.06918874\n",
      "Iteration 39, loss = 0.06709593\n",
      "Iteration 40, loss = 0.06622294\n",
      "Iteration 41, loss = 0.06516714\n",
      "Iteration 42, loss = 0.06238640\n",
      "Iteration 43, loss = 0.06431879\n",
      "Iteration 44, loss = 0.06222340\n",
      "Iteration 45, loss = 0.06202202\n",
      "Iteration 46, loss = 0.06305549\n",
      "Iteration 47, loss = 0.05972790\n",
      "Iteration 48, loss = 0.05808523\n",
      "Iteration 49, loss = 0.05652438\n",
      "Iteration 50, loss = 0.05729282\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kileymack/opt/anaconda3/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (50) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 1.94387579\n",
      "Iteration 2, loss = 0.34068089\n",
      "Iteration 3, loss = 0.27648870\n",
      "Iteration 4, loss = 0.24107712\n",
      "Iteration 5, loss = 0.21219089\n",
      "Iteration 6, loss = 0.19149125\n",
      "Iteration 7, loss = 0.17788320\n",
      "Iteration 8, loss = 0.16510264\n",
      "Iteration 9, loss = 0.15409484\n",
      "Iteration 10, loss = 0.14888843\n",
      "Iteration 11, loss = 0.13655268\n",
      "Iteration 12, loss = 0.13351511\n",
      "Iteration 13, loss = 0.12492345\n",
      "Iteration 14, loss = 0.11917601\n",
      "Iteration 15, loss = 0.11624154\n",
      "Iteration 16, loss = 0.11165290\n",
      "Iteration 17, loss = 0.10731599\n",
      "Iteration 18, loss = 0.10253138\n",
      "Iteration 19, loss = 0.10096505\n",
      "Iteration 20, loss = 0.09789561\n",
      "Iteration 21, loss = 0.09259976\n",
      "Iteration 22, loss = 0.09196727\n",
      "Iteration 23, loss = 0.08833999\n",
      "Iteration 24, loss = 0.08846699\n",
      "Iteration 25, loss = 0.08355995\n",
      "Iteration 26, loss = 0.08268814\n",
      "Iteration 27, loss = 0.08088953\n",
      "Iteration 28, loss = 0.07921305\n",
      "Iteration 29, loss = 0.07653826\n",
      "Iteration 30, loss = 0.07670052\n",
      "Iteration 31, loss = 0.07460169\n",
      "Iteration 32, loss = 0.07325099\n",
      "Iteration 33, loss = 0.07120896\n",
      "Iteration 34, loss = 0.07017538\n",
      "Iteration 35, loss = 0.06896567\n",
      "Iteration 36, loss = 0.06761540\n",
      "Iteration 37, loss = 0.06728626\n",
      "Iteration 38, loss = 0.06534170\n",
      "Iteration 39, loss = 0.06417898\n",
      "Iteration 40, loss = 0.06383937\n",
      "Iteration 41, loss = 0.06291154\n",
      "Iteration 42, loss = 0.06168710\n",
      "Iteration 43, loss = 0.06022897\n",
      "Iteration 44, loss = 0.05986157\n",
      "Iteration 45, loss = 0.05794373\n",
      "Iteration 46, loss = 0.06007479\n",
      "Iteration 47, loss = 0.05761878\n",
      "Iteration 48, loss = 0.05664265\n",
      "Iteration 49, loss = 0.05656948\n",
      "Iteration 50, loss = 0.05655428\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kileymack/opt/anaconda3/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (50) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 1.60295431\n",
      "Iteration 2, loss = 0.40671617\n",
      "Iteration 3, loss = 0.33380716\n",
      "Iteration 4, loss = 0.29419694\n",
      "Iteration 5, loss = 0.26877158\n",
      "Iteration 6, loss = 0.25007124\n",
      "Iteration 7, loss = 0.23634425\n",
      "Iteration 8, loss = 0.22515378\n",
      "Iteration 9, loss = 0.21606987\n",
      "Iteration 10, loss = 0.20796691\n",
      "Iteration 11, loss = 0.20082626\n",
      "Iteration 12, loss = 0.19463396\n",
      "Iteration 13, loss = 0.18864694\n",
      "Iteration 14, loss = 0.18360918\n",
      "Iteration 15, loss = 0.17966730\n",
      "Iteration 16, loss = 0.17409531\n",
      "Iteration 17, loss = 0.17028509\n",
      "Iteration 18, loss = 0.16615929\n",
      "Iteration 19, loss = 0.16230008\n",
      "Iteration 20, loss = 0.15851811\n",
      "Iteration 21, loss = 0.15536006\n",
      "Iteration 22, loss = 0.15238620\n",
      "Iteration 23, loss = 0.14899116\n",
      "Iteration 24, loss = 0.14672116\n",
      "Iteration 25, loss = 0.14411764\n",
      "Iteration 26, loss = 0.14105240\n",
      "Iteration 27, loss = 0.13877460\n",
      "Iteration 28, loss = 0.13739149\n",
      "Iteration 29, loss = 0.13463821\n",
      "Iteration 30, loss = 0.13257596\n",
      "Iteration 31, loss = 0.13067789\n",
      "Iteration 32, loss = 0.12866029\n",
      "Iteration 33, loss = 0.12709274\n",
      "Iteration 34, loss = 0.12512690\n",
      "Iteration 35, loss = 0.12359941\n",
      "Iteration 36, loss = 0.12177539\n",
      "Iteration 37, loss = 0.11981742\n",
      "Iteration 38, loss = 0.11872007\n",
      "Iteration 39, loss = 0.11689265\n",
      "Iteration 40, loss = 0.11606466\n",
      "Iteration 41, loss = 0.11427654\n",
      "Iteration 42, loss = 0.11265955\n",
      "Iteration 43, loss = 0.11154899\n",
      "Iteration 44, loss = 0.11032811\n",
      "Iteration 45, loss = 0.10941192\n",
      "Iteration 46, loss = 0.10800362\n",
      "Iteration 47, loss = 0.10673537\n",
      "Iteration 48, loss = 0.10568475\n",
      "Iteration 49, loss = 0.10453486\n",
      "Iteration 50, loss = 0.10345947\n",
      "Iteration 51, loss = 0.10225684\n",
      "Iteration 52, loss = 0.10094671\n",
      "Iteration 53, loss = 0.10002507\n",
      "Iteration 54, loss = 0.09913258\n",
      "Iteration 55, loss = 0.09827959\n",
      "Iteration 56, loss = 0.09712081\n",
      "Iteration 57, loss = 0.09623791\n",
      "Iteration 58, loss = 0.09541099\n",
      "Iteration 59, loss = 0.09484700\n",
      "Iteration 60, loss = 0.09348496\n",
      "Iteration 61, loss = 0.09249324\n",
      "Iteration 62, loss = 0.09201165\n",
      "Iteration 63, loss = 0.09076011\n",
      "Iteration 64, loss = 0.09019318\n",
      "Iteration 65, loss = 0.08878885\n",
      "Iteration 66, loss = 0.08848696\n",
      "Iteration 67, loss = 0.08769138\n",
      "Iteration 68, loss = 0.08739248\n",
      "Iteration 69, loss = 0.08663055\n",
      "Iteration 70, loss = 0.08585805\n",
      "Iteration 71, loss = 0.08494419\n",
      "Iteration 72, loss = 0.08425809\n",
      "Iteration 73, loss = 0.08371521\n",
      "Iteration 74, loss = 0.08271945\n",
      "Iteration 75, loss = 0.08261502\n",
      "Iteration 76, loss = 0.08191166\n",
      "Iteration 77, loss = 0.08124714\n",
      "Iteration 78, loss = 0.08102311\n",
      "Iteration 79, loss = 0.07995484\n",
      "Iteration 80, loss = 0.07935654\n",
      "Iteration 81, loss = 0.07897526\n",
      "Iteration 82, loss = 0.07806485\n",
      "Iteration 83, loss = 0.07795590\n",
      "Iteration 84, loss = 0.07746545\n",
      "Iteration 85, loss = 0.07624402\n",
      "Iteration 86, loss = 0.07625668\n",
      "Iteration 87, loss = 0.07553168\n",
      "Iteration 88, loss = 0.07535639\n",
      "Iteration 89, loss = 0.07442174\n",
      "Iteration 90, loss = 0.07446864\n",
      "Iteration 91, loss = 0.07396518\n",
      "Iteration 92, loss = 0.07389366\n",
      "Iteration 93, loss = 0.07316913\n",
      "Iteration 94, loss = 0.07285308\n",
      "Iteration 95, loss = 0.07216395\n",
      "Iteration 96, loss = 0.07195419\n",
      "Iteration 97, loss = 0.07107147\n",
      "Iteration 98, loss = 0.07082477\n",
      "Iteration 99, loss = 0.07030152\n",
      "Iteration 100, loss = 0.07015159\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kileymack/opt/anaconda3/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 1.22573392\n",
      "Iteration 2, loss = 0.38931056\n",
      "Iteration 3, loss = 0.33174881\n",
      "Iteration 4, loss = 0.29887941\n",
      "Iteration 5, loss = 0.27491607\n",
      "Iteration 6, loss = 0.25685442\n",
      "Iteration 7, loss = 0.24208261\n",
      "Iteration 8, loss = 0.22999136\n",
      "Iteration 9, loss = 0.21963169\n",
      "Iteration 10, loss = 0.21028474\n",
      "Iteration 11, loss = 0.20263824\n",
      "Iteration 12, loss = 0.19558763\n",
      "Iteration 13, loss = 0.18894601\n",
      "Iteration 14, loss = 0.18343216\n",
      "Iteration 15, loss = 0.17765175\n",
      "Iteration 16, loss = 0.17274996\n",
      "Iteration 17, loss = 0.16807657\n",
      "Iteration 18, loss = 0.16382116\n",
      "Iteration 19, loss = 0.15975389\n",
      "Iteration 20, loss = 0.15636464\n",
      "Iteration 21, loss = 0.15299031\n",
      "Iteration 22, loss = 0.14960710\n",
      "Iteration 23, loss = 0.14658472\n",
      "Iteration 24, loss = 0.14363363\n",
      "Iteration 25, loss = 0.14116148\n",
      "Iteration 26, loss = 0.13885385\n",
      "Iteration 27, loss = 0.13620612\n",
      "Iteration 28, loss = 0.13419484\n",
      "Iteration 29, loss = 0.13169559\n",
      "Iteration 30, loss = 0.12971828\n",
      "Iteration 31, loss = 0.12766208\n",
      "Iteration 32, loss = 0.12543829\n",
      "Iteration 33, loss = 0.12376512\n",
      "Iteration 34, loss = 0.12203893\n",
      "Iteration 35, loss = 0.12004813\n",
      "Iteration 36, loss = 0.11879152\n",
      "Iteration 37, loss = 0.11735319\n",
      "Iteration 38, loss = 0.11554428\n",
      "Iteration 39, loss = 0.11442245\n",
      "Iteration 40, loss = 0.11288728\n",
      "Iteration 41, loss = 0.11125734\n",
      "Iteration 42, loss = 0.11010576\n",
      "Iteration 43, loss = 0.10899481\n",
      "Iteration 44, loss = 0.10730293\n",
      "Iteration 45, loss = 0.10607493\n",
      "Iteration 46, loss = 0.10509363\n",
      "Iteration 47, loss = 0.10405984\n",
      "Iteration 48, loss = 0.10306537\n",
      "Iteration 49, loss = 0.10213501\n",
      "Iteration 50, loss = 0.10086361\n",
      "Iteration 51, loss = 0.10013621\n",
      "Iteration 52, loss = 0.09879570\n",
      "Iteration 53, loss = 0.09822717\n",
      "Iteration 54, loss = 0.09737817\n",
      "Iteration 55, loss = 0.09632476\n",
      "Iteration 56, loss = 0.09556218\n",
      "Iteration 57, loss = 0.09445759\n",
      "Iteration 58, loss = 0.09384971\n",
      "Iteration 59, loss = 0.09302364\n",
      "Iteration 60, loss = 0.09197843\n",
      "Iteration 61, loss = 0.09121116\n",
      "Iteration 62, loss = 0.09067606\n",
      "Iteration 63, loss = 0.08967580\n",
      "Iteration 64, loss = 0.08895107\n",
      "Iteration 65, loss = 0.08835306\n",
      "Iteration 66, loss = 0.08758680\n",
      "Iteration 67, loss = 0.08661738\n",
      "Iteration 68, loss = 0.08611297\n",
      "Iteration 69, loss = 0.08561911\n",
      "Iteration 70, loss = 0.08506854\n",
      "Iteration 71, loss = 0.08408204\n",
      "Iteration 72, loss = 0.08373268\n",
      "Iteration 73, loss = 0.08271049\n",
      "Iteration 74, loss = 0.08205552\n",
      "Iteration 75, loss = 0.08181523\n",
      "Iteration 76, loss = 0.08103587\n",
      "Iteration 77, loss = 0.08057800\n",
      "Iteration 78, loss = 0.07981207\n",
      "Iteration 79, loss = 0.07949133\n",
      "Iteration 80, loss = 0.07877330\n",
      "Iteration 81, loss = 0.07830289\n",
      "Iteration 82, loss = 0.07776469\n",
      "Iteration 83, loss = 0.07723927\n",
      "Iteration 84, loss = 0.07693225\n",
      "Iteration 85, loss = 0.07646202\n",
      "Iteration 86, loss = 0.07562227\n",
      "Iteration 87, loss = 0.07539997\n",
      "Iteration 88, loss = 0.07472372\n",
      "Iteration 89, loss = 0.07450868\n",
      "Iteration 90, loss = 0.07428958\n",
      "Iteration 91, loss = 0.07337960\n",
      "Iteration 92, loss = 0.07304578\n",
      "Iteration 93, loss = 0.07258490\n",
      "Iteration 94, loss = 0.07194511\n",
      "Iteration 95, loss = 0.07177103\n",
      "Iteration 96, loss = 0.07131035\n",
      "Iteration 97, loss = 0.07078070\n",
      "Iteration 98, loss = 0.07062996\n",
      "Iteration 99, loss = 0.07002603\n",
      "Iteration 100, loss = 0.06947035\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kileymack/opt/anaconda3/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 1.86549242\n",
      "Iteration 2, loss = 0.33339977\n",
      "Iteration 3, loss = 0.26706902\n",
      "Iteration 4, loss = 0.23596067\n",
      "Iteration 5, loss = 0.20484092\n",
      "Iteration 6, loss = 0.18941699\n",
      "Iteration 7, loss = 0.17989739\n",
      "Iteration 8, loss = 0.16566869\n",
      "Iteration 9, loss = 0.15831923\n",
      "Iteration 10, loss = 0.15025322\n",
      "Iteration 11, loss = 0.14304365\n",
      "Iteration 12, loss = 0.13549413\n",
      "Iteration 13, loss = 0.13229708\n",
      "Iteration 14, loss = 0.12503592\n",
      "Iteration 15, loss = 0.12078036\n",
      "Iteration 16, loss = 0.11469771\n",
      "Iteration 17, loss = 0.11239382\n",
      "Iteration 18, loss = 0.10790414\n",
      "Iteration 19, loss = 0.10471148\n",
      "Iteration 20, loss = 0.10219866\n",
      "Iteration 21, loss = 0.09697674\n",
      "Iteration 22, loss = 0.09458249\n",
      "Iteration 23, loss = 0.09442796\n",
      "Iteration 24, loss = 0.09263628\n",
      "Iteration 25, loss = 0.08926552\n",
      "Iteration 26, loss = 0.08548748\n",
      "Iteration 27, loss = 0.08656813\n",
      "Iteration 28, loss = 0.08225488\n",
      "Iteration 29, loss = 0.08204768\n",
      "Iteration 30, loss = 0.08027059\n",
      "Iteration 31, loss = 0.07770675\n",
      "Iteration 32, loss = 0.07553244\n",
      "Iteration 33, loss = 0.07556178\n",
      "Iteration 34, loss = 0.07259852\n",
      "Iteration 35, loss = 0.07284546\n",
      "Iteration 36, loss = 0.07129528\n",
      "Iteration 37, loss = 0.06885676\n",
      "Iteration 38, loss = 0.06918874\n",
      "Iteration 39, loss = 0.06709593\n",
      "Iteration 40, loss = 0.06622294\n",
      "Iteration 41, loss = 0.06516714\n",
      "Iteration 42, loss = 0.06238640\n",
      "Iteration 43, loss = 0.06431879\n",
      "Iteration 44, loss = 0.06222340\n",
      "Iteration 45, loss = 0.06202202\n",
      "Iteration 46, loss = 0.06305549\n",
      "Iteration 47, loss = 0.05972790\n",
      "Iteration 48, loss = 0.05808523\n",
      "Iteration 49, loss = 0.05652438\n",
      "Iteration 50, loss = 0.05729282\n",
      "Iteration 51, loss = 0.05565622\n",
      "Iteration 52, loss = 0.05796799\n",
      "Iteration 53, loss = 0.05629684\n",
      "Iteration 54, loss = 0.05595208\n",
      "Iteration 55, loss = 0.05330597\n",
      "Iteration 56, loss = 0.05349645\n",
      "Iteration 57, loss = 0.05244369\n",
      "Iteration 58, loss = 0.05261063\n",
      "Iteration 59, loss = 0.05138813\n",
      "Iteration 60, loss = 0.05189190\n",
      "Iteration 61, loss = 0.05036085\n",
      "Iteration 62, loss = 0.05045366\n",
      "Iteration 63, loss = 0.04914347\n",
      "Iteration 64, loss = 0.05061806\n",
      "Iteration 65, loss = 0.04771308\n",
      "Iteration 66, loss = 0.04784301\n",
      "Iteration 67, loss = 0.04788998\n",
      "Iteration 68, loss = 0.04732257\n",
      "Iteration 69, loss = 0.04712506\n",
      "Iteration 70, loss = 0.04617195\n",
      "Iteration 71, loss = 0.04668606\n",
      "Iteration 72, loss = 0.04658727\n",
      "Iteration 73, loss = 0.04457674\n",
      "Iteration 74, loss = 0.04487378\n",
      "Iteration 75, loss = 0.04474491\n",
      "Iteration 76, loss = 0.04443139\n",
      "Iteration 77, loss = 0.04406673\n",
      "Iteration 78, loss = 0.04460548\n",
      "Iteration 79, loss = 0.04209773\n",
      "Iteration 80, loss = 0.04196006\n",
      "Iteration 81, loss = 0.04268562\n",
      "Iteration 82, loss = 0.04160769\n",
      "Iteration 83, loss = 0.04231372\n",
      "Iteration 84, loss = 0.04085942\n",
      "Iteration 85, loss = 0.04111906\n",
      "Iteration 86, loss = 0.04141635\n",
      "Iteration 87, loss = 0.03994245\n",
      "Iteration 88, loss = 0.04074934\n",
      "Iteration 89, loss = 0.03969979\n",
      "Iteration 90, loss = 0.03974406\n",
      "Iteration 91, loss = 0.03959085\n",
      "Iteration 92, loss = 0.03839601\n",
      "Iteration 93, loss = 0.04037773\n",
      "Iteration 94, loss = 0.03827742\n",
      "Iteration 95, loss = 0.03812884\n",
      "Iteration 96, loss = 0.04009286\n",
      "Iteration 97, loss = 0.03619125\n",
      "Iteration 98, loss = 0.03728905\n",
      "Iteration 99, loss = 0.03734180\n",
      "Iteration 100, loss = 0.03662989\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kileymack/opt/anaconda3/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 1.94387579\n",
      "Iteration 2, loss = 0.34068089\n",
      "Iteration 3, loss = 0.27648870\n",
      "Iteration 4, loss = 0.24107712\n",
      "Iteration 5, loss = 0.21219089\n",
      "Iteration 6, loss = 0.19149125\n",
      "Iteration 7, loss = 0.17788320\n",
      "Iteration 8, loss = 0.16510264\n",
      "Iteration 9, loss = 0.15409484\n",
      "Iteration 10, loss = 0.14888843\n",
      "Iteration 11, loss = 0.13655268\n",
      "Iteration 12, loss = 0.13351511\n",
      "Iteration 13, loss = 0.12492345\n",
      "Iteration 14, loss = 0.11917601\n",
      "Iteration 15, loss = 0.11624154\n",
      "Iteration 16, loss = 0.11165290\n",
      "Iteration 17, loss = 0.10731599\n",
      "Iteration 18, loss = 0.10253138\n",
      "Iteration 19, loss = 0.10096505\n",
      "Iteration 20, loss = 0.09789561\n",
      "Iteration 21, loss = 0.09259976\n",
      "Iteration 22, loss = 0.09196727\n",
      "Iteration 23, loss = 0.08833999\n",
      "Iteration 24, loss = 0.08846699\n",
      "Iteration 25, loss = 0.08355995\n",
      "Iteration 26, loss = 0.08268814\n",
      "Iteration 27, loss = 0.08088953\n",
      "Iteration 28, loss = 0.07921305\n",
      "Iteration 29, loss = 0.07653826\n",
      "Iteration 30, loss = 0.07670052\n",
      "Iteration 31, loss = 0.07460169\n",
      "Iteration 32, loss = 0.07325099\n",
      "Iteration 33, loss = 0.07120896\n",
      "Iteration 34, loss = 0.07017538\n",
      "Iteration 35, loss = 0.06896567\n",
      "Iteration 36, loss = 0.06761540\n",
      "Iteration 37, loss = 0.06728626\n",
      "Iteration 38, loss = 0.06534170\n",
      "Iteration 39, loss = 0.06417898\n",
      "Iteration 40, loss = 0.06383937\n",
      "Iteration 41, loss = 0.06291154\n",
      "Iteration 42, loss = 0.06168710\n",
      "Iteration 43, loss = 0.06022897\n",
      "Iteration 44, loss = 0.05986157\n",
      "Iteration 45, loss = 0.05794373\n",
      "Iteration 46, loss = 0.06007479\n",
      "Iteration 47, loss = 0.05761878\n",
      "Iteration 48, loss = 0.05664265\n",
      "Iteration 49, loss = 0.05656948\n",
      "Iteration 50, loss = 0.05655428\n",
      "Iteration 51, loss = 0.05412188\n",
      "Iteration 52, loss = 0.05296805\n",
      "Iteration 53, loss = 0.05338880\n",
      "Iteration 54, loss = 0.05297252\n",
      "Iteration 55, loss = 0.05249656\n",
      "Iteration 56, loss = 0.05240320\n",
      "Iteration 57, loss = 0.05208648\n",
      "Iteration 58, loss = 0.05010442\n",
      "Iteration 59, loss = 0.05087728\n",
      "Iteration 60, loss = 0.05022008\n",
      "Iteration 61, loss = 0.04916962\n",
      "Iteration 62, loss = 0.04916266\n",
      "Iteration 63, loss = 0.04803641\n",
      "Iteration 64, loss = 0.04787025\n",
      "Iteration 65, loss = 0.04789879\n",
      "Iteration 66, loss = 0.04737697\n",
      "Iteration 67, loss = 0.04671794\n",
      "Iteration 68, loss = 0.04607679\n",
      "Iteration 69, loss = 0.04686306\n",
      "Iteration 70, loss = 0.04554341\n",
      "Iteration 71, loss = 0.04546001\n",
      "Iteration 72, loss = 0.04555499\n",
      "Iteration 73, loss = 0.04490529\n",
      "Iteration 74, loss = 0.04434546\n",
      "Iteration 75, loss = 0.04381980\n",
      "Iteration 76, loss = 0.04491479\n",
      "Iteration 77, loss = 0.04355824\n",
      "Iteration 78, loss = 0.04303459\n",
      "Iteration 79, loss = 0.04324332\n",
      "Iteration 80, loss = 0.04284999\n",
      "Iteration 81, loss = 0.04290003\n",
      "Iteration 82, loss = 0.04217358\n",
      "Iteration 83, loss = 0.04172415\n",
      "Iteration 84, loss = 0.04184897\n",
      "Iteration 85, loss = 0.04130623\n",
      "Iteration 86, loss = 0.04157495\n",
      "Iteration 87, loss = 0.04070564\n",
      "Iteration 88, loss = 0.04095159\n",
      "Iteration 89, loss = 0.04026835\n",
      "Iteration 90, loss = 0.04118779\n",
      "Iteration 91, loss = 0.04014345\n",
      "Iteration 92, loss = 0.03937140\n",
      "Iteration 93, loss = 0.03977644\n",
      "Iteration 94, loss = 0.03945955\n",
      "Iteration 95, loss = 0.03958956\n",
      "Iteration 96, loss = 0.03881036\n",
      "Iteration 97, loss = 0.03872920\n",
      "Iteration 98, loss = 0.03894394\n",
      "Iteration 99, loss = 0.03851590\n",
      "Iteration 100, loss = 0.03840433\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kileymack/opt/anaconda3/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 1.52268951\n",
      "Iteration 2, loss = 0.41956230\n",
      "Iteration 3, loss = 0.34384592\n",
      "Iteration 4, loss = 0.30650088\n",
      "Iteration 5, loss = 0.28199576\n",
      "Iteration 6, loss = 0.26380468\n",
      "Iteration 7, loss = 0.24947227\n",
      "Iteration 8, loss = 0.23740323\n",
      "Iteration 9, loss = 0.22738911\n",
      "Iteration 10, loss = 0.21879583\n",
      "Iteration 11, loss = 0.21199860\n",
      "Iteration 12, loss = 0.20529802\n",
      "Iteration 13, loss = 0.19903034\n",
      "Iteration 14, loss = 0.19451262\n",
      "Iteration 15, loss = 0.18869610\n",
      "Iteration 16, loss = 0.18452067\n",
      "Iteration 17, loss = 0.18106083\n",
      "Iteration 18, loss = 0.17740916\n",
      "Iteration 19, loss = 0.17318312\n",
      "Iteration 20, loss = 0.17034845\n",
      "Iteration 21, loss = 0.16699599\n",
      "Iteration 22, loss = 0.16410226\n",
      "Iteration 23, loss = 0.16147813\n",
      "Iteration 24, loss = 0.15879815\n",
      "Iteration 25, loss = 0.15683370\n",
      "Iteration 26, loss = 0.15410066\n",
      "Iteration 27, loss = 0.15145825\n",
      "Iteration 28, loss = 0.14940328\n",
      "Iteration 29, loss = 0.14726271\n",
      "Iteration 30, loss = 0.14532382\n",
      "Iteration 31, loss = 0.14314101\n",
      "Iteration 32, loss = 0.14197686\n",
      "Iteration 33, loss = 0.13992171\n",
      "Iteration 34, loss = 0.13862846\n",
      "Iteration 35, loss = 0.13668243\n",
      "Iteration 36, loss = 0.13567522\n",
      "Iteration 37, loss = 0.13407963\n",
      "Iteration 38, loss = 0.13236110\n",
      "Iteration 39, loss = 0.13183047\n",
      "Iteration 40, loss = 0.13030065\n",
      "Iteration 41, loss = 0.12814604\n",
      "Iteration 42, loss = 0.12696856\n",
      "Iteration 43, loss = 0.12585552\n",
      "Iteration 44, loss = 0.12499302\n",
      "Iteration 45, loss = 0.12397075\n",
      "Iteration 46, loss = 0.12276055\n",
      "Iteration 47, loss = 0.12119400\n",
      "Iteration 48, loss = 0.11968581\n",
      "Iteration 49, loss = 0.11892045\n",
      "Iteration 50, loss = 0.11786256\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kileymack/opt/anaconda3/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (50) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 1.15146347\n",
      "Iteration 2, loss = 0.40209014\n",
      "Iteration 3, loss = 0.34393833\n",
      "Iteration 4, loss = 0.31087211\n",
      "Iteration 5, loss = 0.28751635\n",
      "Iteration 6, loss = 0.27003127\n",
      "Iteration 7, loss = 0.25609219\n",
      "Iteration 8, loss = 0.24330564\n",
      "Iteration 9, loss = 0.23263102\n",
      "Iteration 10, loss = 0.22326464\n",
      "Iteration 11, loss = 0.21517074\n",
      "Iteration 12, loss = 0.20776664\n",
      "Iteration 13, loss = 0.20113692\n",
      "Iteration 14, loss = 0.19535123\n",
      "Iteration 15, loss = 0.19052177\n",
      "Iteration 16, loss = 0.18583759\n",
      "Iteration 17, loss = 0.18150438\n",
      "Iteration 18, loss = 0.17758882\n",
      "Iteration 19, loss = 0.17393098\n",
      "Iteration 20, loss = 0.17024657\n",
      "Iteration 21, loss = 0.16724215\n",
      "Iteration 22, loss = 0.16429068\n",
      "Iteration 23, loss = 0.16160023\n",
      "Iteration 24, loss = 0.15879805\n",
      "Iteration 25, loss = 0.15633439\n",
      "Iteration 26, loss = 0.15430977\n",
      "Iteration 27, loss = 0.15211231\n",
      "Iteration 28, loss = 0.14970110\n",
      "Iteration 29, loss = 0.14790230\n",
      "Iteration 30, loss = 0.14564309\n",
      "Iteration 31, loss = 0.14380834\n",
      "Iteration 32, loss = 0.14252307\n",
      "Iteration 33, loss = 0.14014247\n",
      "Iteration 34, loss = 0.13888672\n",
      "Iteration 35, loss = 0.13708398\n",
      "Iteration 36, loss = 0.13566989\n",
      "Iteration 37, loss = 0.13432020\n",
      "Iteration 38, loss = 0.13274637\n",
      "Iteration 39, loss = 0.13132882\n",
      "Iteration 40, loss = 0.13009561\n",
      "Iteration 41, loss = 0.12896247\n",
      "Iteration 42, loss = 0.12718475\n",
      "Iteration 43, loss = 0.12613637\n",
      "Iteration 44, loss = 0.12502263\n",
      "Iteration 45, loss = 0.12386003\n",
      "Iteration 46, loss = 0.12281989\n",
      "Iteration 47, loss = 0.12153941\n",
      "Iteration 48, loss = 0.12069112\n",
      "Iteration 49, loss = 0.11943591\n",
      "Iteration 50, loss = 0.11826720\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kileymack/opt/anaconda3/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (50) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 1.79537804\n",
      "Iteration 2, loss = 0.35569750\n",
      "Iteration 3, loss = 0.27735984\n",
      "Iteration 4, loss = 0.24005826\n",
      "Iteration 5, loss = 0.22367936\n",
      "Iteration 6, loss = 0.20595972\n",
      "Iteration 7, loss = 0.19279201\n",
      "Iteration 8, loss = 0.18129671\n",
      "Iteration 9, loss = 0.16938469\n",
      "Iteration 10, loss = 0.15986086\n",
      "Iteration 11, loss = 0.15530806\n",
      "Iteration 12, loss = 0.14911715\n",
      "Iteration 13, loss = 0.14219342\n",
      "Iteration 14, loss = 0.13823590\n",
      "Iteration 15, loss = 0.13335924\n",
      "Iteration 16, loss = 0.12708641\n",
      "Iteration 17, loss = 0.12732609\n",
      "Iteration 18, loss = 0.11905568\n",
      "Iteration 19, loss = 0.11778210\n",
      "Iteration 20, loss = 0.11666455\n",
      "Iteration 21, loss = 0.10984433\n",
      "Iteration 22, loss = 0.10835154\n",
      "Iteration 23, loss = 0.10361951\n",
      "Iteration 24, loss = 0.10298653\n",
      "Iteration 25, loss = 0.10011717\n",
      "Iteration 26, loss = 0.10509650\n",
      "Iteration 27, loss = 0.09731059\n",
      "Iteration 28, loss = 0.09426926\n",
      "Iteration 29, loss = 0.09410761\n",
      "Iteration 30, loss = 0.08945032\n",
      "Iteration 31, loss = 0.08996209\n",
      "Iteration 32, loss = 0.09067854\n",
      "Iteration 33, loss = 0.08677973\n",
      "Iteration 34, loss = 0.08682311\n",
      "Iteration 35, loss = 0.08192895\n",
      "Iteration 36, loss = 0.08340829\n",
      "Iteration 37, loss = 0.07882493\n",
      "Iteration 38, loss = 0.07960748\n",
      "Iteration 39, loss = 0.07968499\n",
      "Iteration 40, loss = 0.07715934\n",
      "Iteration 41, loss = 0.07503981\n",
      "Iteration 42, loss = 0.07601929\n",
      "Iteration 43, loss = 0.07573892\n",
      "Iteration 44, loss = 0.07399484\n",
      "Iteration 45, loss = 0.07150400\n",
      "Iteration 46, loss = 0.07239814\n",
      "Iteration 47, loss = 0.07080609\n",
      "Iteration 48, loss = 0.06912387\n",
      "Iteration 49, loss = 0.07047483\n",
      "Iteration 50, loss = 0.06854873\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kileymack/opt/anaconda3/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (50) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 1.82055208\n",
      "Iteration 2, loss = 0.35375178\n",
      "Iteration 3, loss = 0.28792564\n",
      "Iteration 4, loss = 0.25491667\n",
      "Iteration 5, loss = 0.22810535\n",
      "Iteration 6, loss = 0.20780574\n",
      "Iteration 7, loss = 0.19655253\n",
      "Iteration 8, loss = 0.18190261\n",
      "Iteration 9, loss = 0.17301074\n",
      "Iteration 10, loss = 0.16354680\n",
      "Iteration 11, loss = 0.15581673\n",
      "Iteration 12, loss = 0.14787600\n",
      "Iteration 13, loss = 0.14411010\n",
      "Iteration 14, loss = 0.13676072\n",
      "Iteration 15, loss = 0.13331820\n",
      "Iteration 16, loss = 0.12766885\n",
      "Iteration 17, loss = 0.12667783\n",
      "Iteration 18, loss = 0.12127837\n",
      "Iteration 19, loss = 0.11803146\n",
      "Iteration 20, loss = 0.11650837\n",
      "Iteration 21, loss = 0.11150647\n",
      "Iteration 22, loss = 0.10964145\n",
      "Iteration 23, loss = 0.10709935\n",
      "Iteration 24, loss = 0.10477919\n",
      "Iteration 25, loss = 0.10314670\n",
      "Iteration 26, loss = 0.10091271\n",
      "Iteration 27, loss = 0.09912004\n",
      "Iteration 28, loss = 0.09668267\n",
      "Iteration 29, loss = 0.09541126\n",
      "Iteration 30, loss = 0.09379228\n",
      "Iteration 31, loss = 0.09225530\n",
      "Iteration 32, loss = 0.09044663\n",
      "Iteration 33, loss = 0.08932181\n",
      "Iteration 34, loss = 0.08734218\n",
      "Iteration 35, loss = 0.08657005\n",
      "Iteration 36, loss = 0.08525521\n",
      "Iteration 37, loss = 0.08286328\n",
      "Iteration 38, loss = 0.08245075\n",
      "Iteration 39, loss = 0.08181355\n",
      "Iteration 40, loss = 0.08060334\n",
      "Iteration 41, loss = 0.08088245\n",
      "Iteration 42, loss = 0.07917943\n",
      "Iteration 43, loss = 0.07806691\n",
      "Iteration 44, loss = 0.07699744\n",
      "Iteration 45, loss = 0.07573095\n",
      "Iteration 46, loss = 0.07595266\n",
      "Iteration 47, loss = 0.07433728\n",
      "Iteration 48, loss = 0.07440755\n",
      "Iteration 49, loss = 0.07279722\n",
      "Iteration 50, loss = 0.07309599\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kileymack/opt/anaconda3/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (50) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 1.52268951\n",
      "Iteration 2, loss = 0.41956230\n",
      "Iteration 3, loss = 0.34384592\n",
      "Iteration 4, loss = 0.30650088\n",
      "Iteration 5, loss = 0.28199576\n",
      "Iteration 6, loss = 0.26380468\n",
      "Iteration 7, loss = 0.24947227\n",
      "Iteration 8, loss = 0.23740323\n",
      "Iteration 9, loss = 0.22738911\n",
      "Iteration 10, loss = 0.21879583\n",
      "Iteration 11, loss = 0.21199860\n",
      "Iteration 12, loss = 0.20529802\n",
      "Iteration 13, loss = 0.19903034\n",
      "Iteration 14, loss = 0.19451262\n",
      "Iteration 15, loss = 0.18869610\n",
      "Iteration 16, loss = 0.18452067\n",
      "Iteration 17, loss = 0.18106083\n",
      "Iteration 18, loss = 0.17740916\n",
      "Iteration 19, loss = 0.17318312\n",
      "Iteration 20, loss = 0.17034845\n",
      "Iteration 21, loss = 0.16699599\n",
      "Iteration 22, loss = 0.16410226\n",
      "Iteration 23, loss = 0.16147813\n",
      "Iteration 24, loss = 0.15879815\n",
      "Iteration 25, loss = 0.15683370\n",
      "Iteration 26, loss = 0.15410066\n",
      "Iteration 27, loss = 0.15145825\n",
      "Iteration 28, loss = 0.14940328\n",
      "Iteration 29, loss = 0.14726271\n",
      "Iteration 30, loss = 0.14532382\n",
      "Iteration 31, loss = 0.14314101\n",
      "Iteration 32, loss = 0.14197686\n",
      "Iteration 33, loss = 0.13992171\n",
      "Iteration 34, loss = 0.13862846\n",
      "Iteration 35, loss = 0.13668243\n",
      "Iteration 36, loss = 0.13567522\n",
      "Iteration 37, loss = 0.13407963\n",
      "Iteration 38, loss = 0.13236110\n",
      "Iteration 39, loss = 0.13183047\n",
      "Iteration 40, loss = 0.13030065\n",
      "Iteration 41, loss = 0.12814604\n",
      "Iteration 42, loss = 0.12696856\n",
      "Iteration 43, loss = 0.12585552\n",
      "Iteration 44, loss = 0.12499302\n",
      "Iteration 45, loss = 0.12397075\n",
      "Iteration 46, loss = 0.12276055\n",
      "Iteration 47, loss = 0.12119400\n",
      "Iteration 48, loss = 0.11968581\n",
      "Iteration 49, loss = 0.11892045\n",
      "Iteration 50, loss = 0.11786256\n",
      "Iteration 51, loss = 0.11758596\n",
      "Iteration 52, loss = 0.11567055\n",
      "Iteration 53, loss = 0.11517030\n",
      "Iteration 54, loss = 0.11365810\n",
      "Iteration 55, loss = 0.11361076\n",
      "Iteration 56, loss = 0.11259153\n",
      "Iteration 57, loss = 0.11197355\n",
      "Iteration 58, loss = 0.11063299\n",
      "Iteration 59, loss = 0.11051180\n",
      "Iteration 60, loss = 0.10900965\n",
      "Iteration 61, loss = 0.10830782\n",
      "Iteration 62, loss = 0.10775083\n",
      "Iteration 63, loss = 0.10674999\n",
      "Iteration 64, loss = 0.10576233\n",
      "Iteration 65, loss = 0.10501496\n",
      "Iteration 66, loss = 0.10501097\n",
      "Iteration 67, loss = 0.10383758\n",
      "Iteration 68, loss = 0.10357395\n",
      "Iteration 69, loss = 0.10267623\n",
      "Iteration 70, loss = 0.10210891\n",
      "Iteration 71, loss = 0.10118100\n",
      "Iteration 72, loss = 0.10038325\n",
      "Iteration 73, loss = 0.10026924\n",
      "Iteration 74, loss = 0.09999167\n",
      "Iteration 75, loss = 0.09821317\n",
      "Iteration 76, loss = 0.09773510\n",
      "Iteration 77, loss = 0.09728523\n",
      "Iteration 78, loss = 0.09704601\n",
      "Iteration 79, loss = 0.09674299\n",
      "Iteration 80, loss = 0.09576108\n",
      "Iteration 81, loss = 0.09510827\n",
      "Iteration 82, loss = 0.09491011\n",
      "Iteration 83, loss = 0.09432515\n",
      "Iteration 84, loss = 0.09392071\n",
      "Iteration 85, loss = 0.09321445\n",
      "Iteration 86, loss = 0.09264843\n",
      "Iteration 87, loss = 0.09167773\n",
      "Iteration 88, loss = 0.09171813\n",
      "Iteration 89, loss = 0.09145679\n",
      "Iteration 90, loss = 0.09096515\n",
      "Iteration 91, loss = 0.09081475\n",
      "Iteration 92, loss = 0.08973413\n",
      "Iteration 93, loss = 0.08930879\n",
      "Iteration 94, loss = 0.08924039\n",
      "Iteration 95, loss = 0.08793903\n",
      "Iteration 96, loss = 0.08810975\n",
      "Iteration 97, loss = 0.08821030\n",
      "Iteration 98, loss = 0.08718044\n",
      "Iteration 99, loss = 0.08624960\n",
      "Iteration 100, loss = 0.08605742\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kileymack/opt/anaconda3/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 1.15146347\n",
      "Iteration 2, loss = 0.40209014\n",
      "Iteration 3, loss = 0.34393833\n",
      "Iteration 4, loss = 0.31087211\n",
      "Iteration 5, loss = 0.28751635\n",
      "Iteration 6, loss = 0.27003127\n",
      "Iteration 7, loss = 0.25609219\n",
      "Iteration 8, loss = 0.24330564\n",
      "Iteration 9, loss = 0.23263102\n",
      "Iteration 10, loss = 0.22326464\n",
      "Iteration 11, loss = 0.21517074\n",
      "Iteration 12, loss = 0.20776664\n",
      "Iteration 13, loss = 0.20113692\n",
      "Iteration 14, loss = 0.19535123\n",
      "Iteration 15, loss = 0.19052177\n",
      "Iteration 16, loss = 0.18583759\n",
      "Iteration 17, loss = 0.18150438\n",
      "Iteration 18, loss = 0.17758882\n",
      "Iteration 19, loss = 0.17393098\n",
      "Iteration 20, loss = 0.17024657\n",
      "Iteration 21, loss = 0.16724215\n",
      "Iteration 22, loss = 0.16429068\n",
      "Iteration 23, loss = 0.16160023\n",
      "Iteration 24, loss = 0.15879805\n",
      "Iteration 25, loss = 0.15633439\n",
      "Iteration 26, loss = 0.15430977\n",
      "Iteration 27, loss = 0.15211231\n",
      "Iteration 28, loss = 0.14970110\n",
      "Iteration 29, loss = 0.14790230\n",
      "Iteration 30, loss = 0.14564309\n",
      "Iteration 31, loss = 0.14380834\n",
      "Iteration 32, loss = 0.14252307\n",
      "Iteration 33, loss = 0.14014247\n",
      "Iteration 34, loss = 0.13888672\n",
      "Iteration 35, loss = 0.13708398\n",
      "Iteration 36, loss = 0.13566989\n",
      "Iteration 37, loss = 0.13432020\n",
      "Iteration 38, loss = 0.13274637\n",
      "Iteration 39, loss = 0.13132882\n",
      "Iteration 40, loss = 0.13009561\n",
      "Iteration 41, loss = 0.12896247\n",
      "Iteration 42, loss = 0.12718475\n",
      "Iteration 43, loss = 0.12613637\n",
      "Iteration 44, loss = 0.12502263\n",
      "Iteration 45, loss = 0.12386003\n",
      "Iteration 46, loss = 0.12281989\n",
      "Iteration 47, loss = 0.12153941\n",
      "Iteration 48, loss = 0.12069112\n",
      "Iteration 49, loss = 0.11943591\n",
      "Iteration 50, loss = 0.11826720\n",
      "Iteration 51, loss = 0.11733946\n",
      "Iteration 52, loss = 0.11609366\n",
      "Iteration 53, loss = 0.11537866\n",
      "Iteration 54, loss = 0.11423339\n",
      "Iteration 55, loss = 0.11333455\n",
      "Iteration 56, loss = 0.11270729\n",
      "Iteration 57, loss = 0.11143688\n",
      "Iteration 58, loss = 0.11059052\n",
      "Iteration 59, loss = 0.10983559\n",
      "Iteration 60, loss = 0.10887590\n",
      "Iteration 61, loss = 0.10846296\n",
      "Iteration 62, loss = 0.10762884\n",
      "Iteration 63, loss = 0.10661766\n",
      "Iteration 64, loss = 0.10574325\n",
      "Iteration 65, loss = 0.10491461\n",
      "Iteration 66, loss = 0.10434610\n",
      "Iteration 67, loss = 0.10346694\n",
      "Iteration 68, loss = 0.10272283\n",
      "Iteration 69, loss = 0.10233000\n",
      "Iteration 70, loss = 0.10137213\n",
      "Iteration 71, loss = 0.10053517\n",
      "Iteration 72, loss = 0.09980476\n",
      "Iteration 73, loss = 0.09925630\n",
      "Iteration 74, loss = 0.09840893\n",
      "Iteration 75, loss = 0.09746460\n",
      "Iteration 76, loss = 0.09713257\n",
      "Iteration 77, loss = 0.09657343\n",
      "Iteration 78, loss = 0.09599908\n",
      "Iteration 79, loss = 0.09547769\n",
      "Iteration 80, loss = 0.09496898\n",
      "Iteration 81, loss = 0.09412728\n",
      "Iteration 82, loss = 0.09386887\n",
      "Iteration 83, loss = 0.09301486\n",
      "Iteration 84, loss = 0.09229895\n",
      "Iteration 85, loss = 0.09217955\n",
      "Iteration 86, loss = 0.09134132\n",
      "Iteration 87, loss = 0.09064872\n",
      "Iteration 88, loss = 0.09047040\n",
      "Iteration 89, loss = 0.09015247\n",
      "Iteration 90, loss = 0.08940419\n",
      "Iteration 91, loss = 0.08884301\n",
      "Iteration 92, loss = 0.08849517\n",
      "Iteration 93, loss = 0.08780405\n",
      "Iteration 94, loss = 0.08738943\n",
      "Iteration 95, loss = 0.08711571\n",
      "Iteration 96, loss = 0.08653424\n",
      "Iteration 97, loss = 0.08599748\n",
      "Iteration 98, loss = 0.08555007\n",
      "Iteration 99, loss = 0.08489437\n",
      "Iteration 100, loss = 0.08465627\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kileymack/opt/anaconda3/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 1.79537804\n",
      "Iteration 2, loss = 0.35569750\n",
      "Iteration 3, loss = 0.27735984\n",
      "Iteration 4, loss = 0.24005826\n",
      "Iteration 5, loss = 0.22367936\n",
      "Iteration 6, loss = 0.20595972\n",
      "Iteration 7, loss = 0.19279201\n",
      "Iteration 8, loss = 0.18129671\n",
      "Iteration 9, loss = 0.16938469\n",
      "Iteration 10, loss = 0.15986086\n",
      "Iteration 11, loss = 0.15530806\n",
      "Iteration 12, loss = 0.14911715\n",
      "Iteration 13, loss = 0.14219342\n",
      "Iteration 14, loss = 0.13823590\n",
      "Iteration 15, loss = 0.13335924\n",
      "Iteration 16, loss = 0.12708641\n",
      "Iteration 17, loss = 0.12732609\n",
      "Iteration 18, loss = 0.11905568\n",
      "Iteration 19, loss = 0.11778210\n",
      "Iteration 20, loss = 0.11666455\n",
      "Iteration 21, loss = 0.10984433\n",
      "Iteration 22, loss = 0.10835154\n",
      "Iteration 23, loss = 0.10361951\n",
      "Iteration 24, loss = 0.10298653\n",
      "Iteration 25, loss = 0.10011717\n",
      "Iteration 26, loss = 0.10509650\n",
      "Iteration 27, loss = 0.09731059\n",
      "Iteration 28, loss = 0.09426926\n",
      "Iteration 29, loss = 0.09410761\n",
      "Iteration 30, loss = 0.08945032\n",
      "Iteration 31, loss = 0.08996209\n",
      "Iteration 32, loss = 0.09067854\n",
      "Iteration 33, loss = 0.08677973\n",
      "Iteration 34, loss = 0.08682311\n",
      "Iteration 35, loss = 0.08192895\n",
      "Iteration 36, loss = 0.08340829\n",
      "Iteration 37, loss = 0.07882493\n",
      "Iteration 38, loss = 0.07960748\n",
      "Iteration 39, loss = 0.07968499\n",
      "Iteration 40, loss = 0.07715934\n",
      "Iteration 41, loss = 0.07503981\n",
      "Iteration 42, loss = 0.07601929\n",
      "Iteration 43, loss = 0.07573892\n",
      "Iteration 44, loss = 0.07399484\n",
      "Iteration 45, loss = 0.07150400\n",
      "Iteration 46, loss = 0.07239814\n",
      "Iteration 47, loss = 0.07080609\n",
      "Iteration 48, loss = 0.06912387\n",
      "Iteration 49, loss = 0.07047483\n",
      "Iteration 50, loss = 0.06854873\n",
      "Iteration 51, loss = 0.06906241\n",
      "Iteration 52, loss = 0.06484602\n",
      "Iteration 53, loss = 0.06589741\n",
      "Iteration 54, loss = 0.06600969\n",
      "Iteration 55, loss = 0.06517885\n",
      "Iteration 56, loss = 0.06278128\n",
      "Iteration 57, loss = 0.06171216\n",
      "Iteration 58, loss = 0.06413221\n",
      "Iteration 59, loss = 0.06241286\n",
      "Iteration 60, loss = 0.06026483\n",
      "Iteration 61, loss = 0.06071468\n",
      "Iteration 62, loss = 0.06038451\n",
      "Iteration 63, loss = 0.05912760\n",
      "Iteration 64, loss = 0.05839417\n",
      "Iteration 65, loss = 0.05913665\n",
      "Iteration 66, loss = 0.05746514\n",
      "Iteration 67, loss = 0.05736507\n",
      "Iteration 68, loss = 0.05711984\n",
      "Iteration 69, loss = 0.05676827\n",
      "Iteration 70, loss = 0.05646317\n",
      "Iteration 71, loss = 0.05583898\n",
      "Iteration 72, loss = 0.05587802\n",
      "Iteration 73, loss = 0.05567334\n",
      "Iteration 74, loss = 0.05392670\n",
      "Iteration 75, loss = 0.05432131\n",
      "Iteration 76, loss = 0.05316275\n",
      "Iteration 77, loss = 0.05183486\n",
      "Iteration 78, loss = 0.05247515\n",
      "Iteration 79, loss = 0.05340240\n",
      "Iteration 80, loss = 0.05254519\n",
      "Iteration 81, loss = 0.05079238\n",
      "Iteration 82, loss = 0.05102039\n",
      "Iteration 83, loss = 0.05088187\n",
      "Iteration 84, loss = 0.04987148\n",
      "Iteration 85, loss = 0.05178653\n",
      "Iteration 86, loss = 0.04980135\n",
      "Iteration 87, loss = 0.04911498\n",
      "Iteration 88, loss = 0.04972424\n",
      "Iteration 89, loss = 0.04896578\n",
      "Iteration 90, loss = 0.04888881\n",
      "Iteration 91, loss = 0.04831240\n",
      "Iteration 92, loss = 0.04896802\n",
      "Iteration 93, loss = 0.04711544\n",
      "Iteration 94, loss = 0.04837178\n",
      "Iteration 95, loss = 0.04699043\n",
      "Iteration 96, loss = 0.04629151\n",
      "Iteration 97, loss = 0.04745532\n",
      "Iteration 98, loss = 0.04572501\n",
      "Iteration 99, loss = 0.04636623\n",
      "Iteration 100, loss = 0.04439801\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kileymack/opt/anaconda3/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 1.82055208\n",
      "Iteration 2, loss = 0.35375178\n",
      "Iteration 3, loss = 0.28792564\n",
      "Iteration 4, loss = 0.25491667\n",
      "Iteration 5, loss = 0.22810535\n",
      "Iteration 6, loss = 0.20780574\n",
      "Iteration 7, loss = 0.19655253\n",
      "Iteration 8, loss = 0.18190261\n",
      "Iteration 9, loss = 0.17301074\n",
      "Iteration 10, loss = 0.16354680\n",
      "Iteration 11, loss = 0.15581673\n",
      "Iteration 12, loss = 0.14787600\n",
      "Iteration 13, loss = 0.14411010\n",
      "Iteration 14, loss = 0.13676072\n",
      "Iteration 15, loss = 0.13331820\n",
      "Iteration 16, loss = 0.12766885\n",
      "Iteration 17, loss = 0.12667783\n",
      "Iteration 18, loss = 0.12127837\n",
      "Iteration 19, loss = 0.11803146\n",
      "Iteration 20, loss = 0.11650837\n",
      "Iteration 21, loss = 0.11150647\n",
      "Iteration 22, loss = 0.10964145\n",
      "Iteration 23, loss = 0.10709935\n",
      "Iteration 24, loss = 0.10477919\n",
      "Iteration 25, loss = 0.10314670\n",
      "Iteration 26, loss = 0.10091271\n",
      "Iteration 27, loss = 0.09912004\n",
      "Iteration 28, loss = 0.09668267\n",
      "Iteration 29, loss = 0.09541126\n",
      "Iteration 30, loss = 0.09379228\n",
      "Iteration 31, loss = 0.09225530\n",
      "Iteration 32, loss = 0.09044663\n",
      "Iteration 33, loss = 0.08932181\n",
      "Iteration 34, loss = 0.08734218\n",
      "Iteration 35, loss = 0.08657005\n",
      "Iteration 36, loss = 0.08525521\n",
      "Iteration 37, loss = 0.08286328\n",
      "Iteration 38, loss = 0.08245075\n",
      "Iteration 39, loss = 0.08181355\n",
      "Iteration 40, loss = 0.08060334\n",
      "Iteration 41, loss = 0.08088245\n",
      "Iteration 42, loss = 0.07917943\n",
      "Iteration 43, loss = 0.07806691\n",
      "Iteration 44, loss = 0.07699744\n",
      "Iteration 45, loss = 0.07573095\n",
      "Iteration 46, loss = 0.07595266\n",
      "Iteration 47, loss = 0.07433728\n",
      "Iteration 48, loss = 0.07440755\n",
      "Iteration 49, loss = 0.07279722\n",
      "Iteration 50, loss = 0.07309599\n",
      "Iteration 51, loss = 0.07204954\n",
      "Iteration 52, loss = 0.07122810\n",
      "Iteration 53, loss = 0.07030596\n",
      "Iteration 54, loss = 0.07008697\n",
      "Iteration 55, loss = 0.06889575\n",
      "Iteration 56, loss = 0.06937613\n",
      "Iteration 57, loss = 0.06778512\n",
      "Iteration 58, loss = 0.06716843\n",
      "Iteration 59, loss = 0.06718004\n",
      "Iteration 60, loss = 0.06664804\n",
      "Iteration 61, loss = 0.06579025\n",
      "Iteration 62, loss = 0.06520318\n",
      "Iteration 63, loss = 0.06459295\n",
      "Iteration 64, loss = 0.06501089\n",
      "Iteration 65, loss = 0.06452231\n",
      "Iteration 66, loss = 0.06322740\n",
      "Iteration 67, loss = 0.06379505\n",
      "Iteration 68, loss = 0.06197806\n",
      "Iteration 69, loss = 0.06319014\n",
      "Iteration 70, loss = 0.06245737\n",
      "Iteration 71, loss = 0.06072136\n",
      "Iteration 72, loss = 0.06098321\n",
      "Iteration 73, loss = 0.06223373\n",
      "Iteration 74, loss = 0.05989467\n",
      "Iteration 75, loss = 0.06006740\n",
      "Iteration 76, loss = 0.05905619\n",
      "Iteration 77, loss = 0.05933904\n",
      "Iteration 78, loss = 0.05911846\n",
      "Iteration 79, loss = 0.05921713\n",
      "Iteration 80, loss = 0.05819134\n",
      "Iteration 81, loss = 0.05788982\n",
      "Iteration 82, loss = 0.05770271\n",
      "Iteration 83, loss = 0.05759591\n",
      "Iteration 84, loss = 0.05691926\n",
      "Iteration 85, loss = 0.05671552\n",
      "Iteration 86, loss = 0.05730146\n",
      "Iteration 87, loss = 0.05521768\n",
      "Iteration 88, loss = 0.05607367\n",
      "Iteration 89, loss = 0.05521348\n",
      "Iteration 90, loss = 0.05555705\n",
      "Iteration 91, loss = 0.05461693\n",
      "Iteration 92, loss = 0.05479120\n",
      "Iteration 93, loss = 0.05446193\n",
      "Iteration 94, loss = 0.05372125\n",
      "Iteration 95, loss = 0.05331411\n",
      "Iteration 96, loss = 0.05437827\n",
      "Iteration 97, loss = 0.05365237\n",
      "Iteration 98, loss = 0.05340795\n",
      "Iteration 99, loss = 0.05297588\n",
      "Iteration 100, loss = 0.05302534\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kileymack/opt/anaconda3/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 1.52268951\n",
      "Iteration 2, loss = 0.41956230\n",
      "Iteration 3, loss = 0.34384592\n",
      "Iteration 4, loss = 0.30650088\n",
      "Iteration 5, loss = 0.28199576\n",
      "Iteration 6, loss = 0.26380468\n",
      "Iteration 7, loss = 0.24947227\n",
      "Iteration 8, loss = 0.23740323\n",
      "Iteration 9, loss = 0.22738911\n",
      "Iteration 10, loss = 0.21879583\n",
      "Iteration 11, loss = 0.21199860\n",
      "Iteration 12, loss = 0.20529802\n",
      "Iteration 13, loss = 0.19903034\n",
      "Iteration 14, loss = 0.19451262\n",
      "Iteration 15, loss = 0.18869610\n",
      "Iteration 16, loss = 0.18452067\n",
      "Iteration 17, loss = 0.18106083\n",
      "Iteration 18, loss = 0.17740916\n",
      "Iteration 19, loss = 0.17318312\n",
      "Iteration 20, loss = 0.17034845\n",
      "Iteration 21, loss = 0.16699599\n",
      "Iteration 22, loss = 0.16410226\n",
      "Iteration 23, loss = 0.16147813\n",
      "Iteration 24, loss = 0.15879815\n",
      "Iteration 25, loss = 0.15683370\n",
      "Iteration 26, loss = 0.15410066\n",
      "Iteration 27, loss = 0.15145825\n",
      "Iteration 28, loss = 0.14940328\n",
      "Iteration 29, loss = 0.14726271\n",
      "Iteration 30, loss = 0.14532382\n",
      "Iteration 31, loss = 0.14314101\n",
      "Iteration 32, loss = 0.14197686\n",
      "Iteration 33, loss = 0.13992171\n",
      "Iteration 34, loss = 0.13862846\n",
      "Iteration 35, loss = 0.13668243\n",
      "Iteration 36, loss = 0.13567522\n",
      "Iteration 37, loss = 0.13407963\n",
      "Iteration 38, loss = 0.13236110\n",
      "Iteration 39, loss = 0.13183047\n",
      "Iteration 40, loss = 0.13030065\n",
      "Iteration 41, loss = 0.12814604\n",
      "Iteration 42, loss = 0.12696856\n",
      "Iteration 43, loss = 0.12585552\n",
      "Iteration 44, loss = 0.12499302\n",
      "Iteration 45, loss = 0.12397075\n",
      "Iteration 46, loss = 0.12276055\n",
      "Iteration 47, loss = 0.12119400\n",
      "Iteration 48, loss = 0.11968581\n",
      "Iteration 49, loss = 0.11892045\n",
      "Iteration 50, loss = 0.11786256\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kileymack/opt/anaconda3/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (50) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 1.15146347\n",
      "Iteration 2, loss = 0.40209014\n",
      "Iteration 3, loss = 0.34393833\n",
      "Iteration 4, loss = 0.31087211\n",
      "Iteration 5, loss = 0.28751635\n",
      "Iteration 6, loss = 0.27003127\n",
      "Iteration 7, loss = 0.25609219\n",
      "Iteration 8, loss = 0.24330564\n",
      "Iteration 9, loss = 0.23263102\n",
      "Iteration 10, loss = 0.22326464\n",
      "Iteration 11, loss = 0.21517074\n",
      "Iteration 12, loss = 0.20776664\n",
      "Iteration 13, loss = 0.20113692\n",
      "Iteration 14, loss = 0.19535123\n",
      "Iteration 15, loss = 0.19052177\n",
      "Iteration 16, loss = 0.18583759\n",
      "Iteration 17, loss = 0.18150438\n",
      "Iteration 18, loss = 0.17758882\n",
      "Iteration 19, loss = 0.17393098\n",
      "Iteration 20, loss = 0.17024657\n",
      "Iteration 21, loss = 0.16724215\n",
      "Iteration 22, loss = 0.16429068\n",
      "Iteration 23, loss = 0.16160023\n",
      "Iteration 24, loss = 0.15879805\n",
      "Iteration 25, loss = 0.15633439\n",
      "Iteration 26, loss = 0.15430977\n",
      "Iteration 27, loss = 0.15211231\n",
      "Iteration 28, loss = 0.14970110\n",
      "Iteration 29, loss = 0.14790230\n",
      "Iteration 30, loss = 0.14564309\n",
      "Iteration 31, loss = 0.14380834\n",
      "Iteration 32, loss = 0.14252307\n",
      "Iteration 33, loss = 0.14014247\n",
      "Iteration 34, loss = 0.13888672\n",
      "Iteration 35, loss = 0.13708398\n",
      "Iteration 36, loss = 0.13566989\n",
      "Iteration 37, loss = 0.13432020\n",
      "Iteration 38, loss = 0.13274637\n",
      "Iteration 39, loss = 0.13132882\n",
      "Iteration 40, loss = 0.13009561\n",
      "Iteration 41, loss = 0.12896247\n",
      "Iteration 42, loss = 0.12718475\n",
      "Iteration 43, loss = 0.12613637\n",
      "Iteration 44, loss = 0.12502263\n",
      "Iteration 45, loss = 0.12386003\n",
      "Iteration 46, loss = 0.12281989\n",
      "Iteration 47, loss = 0.12153941\n",
      "Iteration 48, loss = 0.12069112\n",
      "Iteration 49, loss = 0.11943591\n",
      "Iteration 50, loss = 0.11826720\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kileymack/opt/anaconda3/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (50) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 1.79537804\n",
      "Iteration 2, loss = 0.35569750\n",
      "Iteration 3, loss = 0.27735984\n",
      "Iteration 4, loss = 0.24005826\n",
      "Iteration 5, loss = 0.22367936\n",
      "Iteration 6, loss = 0.20595972\n",
      "Iteration 7, loss = 0.19279201\n",
      "Iteration 8, loss = 0.18129671\n",
      "Iteration 9, loss = 0.16938469\n",
      "Iteration 10, loss = 0.15986086\n",
      "Iteration 11, loss = 0.15530806\n",
      "Iteration 12, loss = 0.14911715\n",
      "Iteration 13, loss = 0.14219342\n",
      "Iteration 14, loss = 0.13823590\n",
      "Iteration 15, loss = 0.13335924\n",
      "Iteration 16, loss = 0.12708641\n",
      "Iteration 17, loss = 0.12732609\n",
      "Iteration 18, loss = 0.11905568\n",
      "Iteration 19, loss = 0.11778210\n",
      "Iteration 20, loss = 0.11666455\n",
      "Iteration 21, loss = 0.10984433\n",
      "Iteration 22, loss = 0.10835154\n",
      "Iteration 23, loss = 0.10361951\n",
      "Iteration 24, loss = 0.10298653\n",
      "Iteration 25, loss = 0.10011717\n",
      "Iteration 26, loss = 0.10509650\n",
      "Iteration 27, loss = 0.09731059\n",
      "Iteration 28, loss = 0.09426926\n",
      "Iteration 29, loss = 0.09410761\n",
      "Iteration 30, loss = 0.08945032\n",
      "Iteration 31, loss = 0.08996209\n",
      "Iteration 32, loss = 0.09067854\n",
      "Iteration 33, loss = 0.08677973\n",
      "Iteration 34, loss = 0.08682311\n",
      "Iteration 35, loss = 0.08192895\n",
      "Iteration 36, loss = 0.08340829\n",
      "Iteration 37, loss = 0.07882493\n",
      "Iteration 38, loss = 0.07960748\n",
      "Iteration 39, loss = 0.07968499\n",
      "Iteration 40, loss = 0.07715934\n",
      "Iteration 41, loss = 0.07503981\n",
      "Iteration 42, loss = 0.07601929\n",
      "Iteration 43, loss = 0.07573892\n",
      "Iteration 44, loss = 0.07399484\n",
      "Iteration 45, loss = 0.07150400\n",
      "Iteration 46, loss = 0.07239814\n",
      "Iteration 47, loss = 0.07080609\n",
      "Iteration 48, loss = 0.06912387\n",
      "Iteration 49, loss = 0.07047483\n",
      "Iteration 50, loss = 0.06854873\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kileymack/opt/anaconda3/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (50) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 1.82055208\n",
      "Iteration 2, loss = 0.35375178\n",
      "Iteration 3, loss = 0.28792564\n",
      "Iteration 4, loss = 0.25491667\n",
      "Iteration 5, loss = 0.22810535\n",
      "Iteration 6, loss = 0.20780574\n",
      "Iteration 7, loss = 0.19655253\n",
      "Iteration 8, loss = 0.18190261\n",
      "Iteration 9, loss = 0.17301074\n",
      "Iteration 10, loss = 0.16354680\n",
      "Iteration 11, loss = 0.15581673\n",
      "Iteration 12, loss = 0.14787600\n",
      "Iteration 13, loss = 0.14411010\n",
      "Iteration 14, loss = 0.13676072\n",
      "Iteration 15, loss = 0.13331820\n",
      "Iteration 16, loss = 0.12766885\n",
      "Iteration 17, loss = 0.12667783\n",
      "Iteration 18, loss = 0.12127837\n",
      "Iteration 19, loss = 0.11803146\n",
      "Iteration 20, loss = 0.11650837\n",
      "Iteration 21, loss = 0.11150647\n",
      "Iteration 22, loss = 0.10964145\n",
      "Iteration 23, loss = 0.10709935\n",
      "Iteration 24, loss = 0.10477919\n",
      "Iteration 25, loss = 0.10314670\n",
      "Iteration 26, loss = 0.10091271\n",
      "Iteration 27, loss = 0.09912004\n",
      "Iteration 28, loss = 0.09668267\n",
      "Iteration 29, loss = 0.09541126\n",
      "Iteration 30, loss = 0.09379228\n",
      "Iteration 31, loss = 0.09225530\n",
      "Iteration 32, loss = 0.09044663\n",
      "Iteration 33, loss = 0.08932181\n",
      "Iteration 34, loss = 0.08734218\n",
      "Iteration 35, loss = 0.08657005\n",
      "Iteration 36, loss = 0.08525521\n",
      "Iteration 37, loss = 0.08286328\n",
      "Iteration 38, loss = 0.08245075\n",
      "Iteration 39, loss = 0.08181355\n",
      "Iteration 40, loss = 0.08060334\n",
      "Iteration 41, loss = 0.08088245\n",
      "Iteration 42, loss = 0.07917943\n",
      "Iteration 43, loss = 0.07806691\n",
      "Iteration 44, loss = 0.07699744\n",
      "Iteration 45, loss = 0.07573095\n",
      "Iteration 46, loss = 0.07595266\n",
      "Iteration 47, loss = 0.07433728\n",
      "Iteration 48, loss = 0.07440755\n",
      "Iteration 49, loss = 0.07279722\n",
      "Iteration 50, loss = 0.07309599\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kileymack/opt/anaconda3/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (50) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 1.52268951\n",
      "Iteration 2, loss = 0.41956230\n",
      "Iteration 3, loss = 0.34384592\n",
      "Iteration 4, loss = 0.30650088\n",
      "Iteration 5, loss = 0.28199576\n",
      "Iteration 6, loss = 0.26380468\n",
      "Iteration 7, loss = 0.24947227\n",
      "Iteration 8, loss = 0.23740323\n",
      "Iteration 9, loss = 0.22738911\n",
      "Iteration 10, loss = 0.21879583\n",
      "Iteration 11, loss = 0.21199860\n",
      "Iteration 12, loss = 0.20529802\n",
      "Iteration 13, loss = 0.19903034\n",
      "Iteration 14, loss = 0.19451262\n",
      "Iteration 15, loss = 0.18869610\n",
      "Iteration 16, loss = 0.18452067\n",
      "Iteration 17, loss = 0.18106083\n",
      "Iteration 18, loss = 0.17740916\n",
      "Iteration 19, loss = 0.17318312\n",
      "Iteration 20, loss = 0.17034845\n",
      "Iteration 21, loss = 0.16699599\n",
      "Iteration 22, loss = 0.16410226\n",
      "Iteration 23, loss = 0.16147813\n",
      "Iteration 24, loss = 0.15879815\n",
      "Iteration 25, loss = 0.15683370\n",
      "Iteration 26, loss = 0.15410066\n",
      "Iteration 27, loss = 0.15145825\n",
      "Iteration 28, loss = 0.14940328\n",
      "Iteration 29, loss = 0.14726271\n",
      "Iteration 30, loss = 0.14532382\n",
      "Iteration 31, loss = 0.14314101\n",
      "Iteration 32, loss = 0.14197686\n",
      "Iteration 33, loss = 0.13992171\n",
      "Iteration 34, loss = 0.13862846\n",
      "Iteration 35, loss = 0.13668243\n",
      "Iteration 36, loss = 0.13567522\n",
      "Iteration 37, loss = 0.13407963\n",
      "Iteration 38, loss = 0.13236110\n",
      "Iteration 39, loss = 0.13183047\n",
      "Iteration 40, loss = 0.13030065\n",
      "Iteration 41, loss = 0.12814604\n",
      "Iteration 42, loss = 0.12696856\n",
      "Iteration 43, loss = 0.12585552\n",
      "Iteration 44, loss = 0.12499302\n",
      "Iteration 45, loss = 0.12397075\n",
      "Iteration 46, loss = 0.12276055\n",
      "Iteration 47, loss = 0.12119400\n",
      "Iteration 48, loss = 0.11968581\n",
      "Iteration 49, loss = 0.11892045\n",
      "Iteration 50, loss = 0.11786256\n",
      "Iteration 51, loss = 0.11758596\n",
      "Iteration 52, loss = 0.11567055\n",
      "Iteration 53, loss = 0.11517030\n",
      "Iteration 54, loss = 0.11365810\n",
      "Iteration 55, loss = 0.11361076\n",
      "Iteration 56, loss = 0.11259153\n",
      "Iteration 57, loss = 0.11197355\n",
      "Iteration 58, loss = 0.11063299\n",
      "Iteration 59, loss = 0.11051180\n",
      "Iteration 60, loss = 0.10900965\n",
      "Iteration 61, loss = 0.10830782\n",
      "Iteration 62, loss = 0.10775083\n",
      "Iteration 63, loss = 0.10674999\n",
      "Iteration 64, loss = 0.10576233\n",
      "Iteration 65, loss = 0.10501496\n",
      "Iteration 66, loss = 0.10501097\n",
      "Iteration 67, loss = 0.10383758\n",
      "Iteration 68, loss = 0.10357395\n",
      "Iteration 69, loss = 0.10267623\n",
      "Iteration 70, loss = 0.10210891\n",
      "Iteration 71, loss = 0.10118100\n",
      "Iteration 72, loss = 0.10038325\n",
      "Iteration 73, loss = 0.10026924\n",
      "Iteration 74, loss = 0.09999167\n",
      "Iteration 75, loss = 0.09821317\n",
      "Iteration 76, loss = 0.09773510\n",
      "Iteration 77, loss = 0.09728523\n",
      "Iteration 78, loss = 0.09704601\n",
      "Iteration 79, loss = 0.09674299\n",
      "Iteration 80, loss = 0.09576108\n",
      "Iteration 81, loss = 0.09510827\n",
      "Iteration 82, loss = 0.09491011\n",
      "Iteration 83, loss = 0.09432515\n",
      "Iteration 84, loss = 0.09392071\n",
      "Iteration 85, loss = 0.09321445\n",
      "Iteration 86, loss = 0.09264843\n",
      "Iteration 87, loss = 0.09167773\n",
      "Iteration 88, loss = 0.09171813\n",
      "Iteration 89, loss = 0.09145679\n",
      "Iteration 90, loss = 0.09096515\n",
      "Iteration 91, loss = 0.09081475\n",
      "Iteration 92, loss = 0.08973413\n",
      "Iteration 93, loss = 0.08930879\n",
      "Iteration 94, loss = 0.08924039\n",
      "Iteration 95, loss = 0.08793903\n",
      "Iteration 96, loss = 0.08810975\n",
      "Iteration 97, loss = 0.08821030\n",
      "Iteration 98, loss = 0.08718044\n",
      "Iteration 99, loss = 0.08624960\n",
      "Iteration 100, loss = 0.08605742\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kileymack/opt/anaconda3/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 1.15146347\n",
      "Iteration 2, loss = 0.40209014\n",
      "Iteration 3, loss = 0.34393833\n",
      "Iteration 4, loss = 0.31087211\n",
      "Iteration 5, loss = 0.28751635\n",
      "Iteration 6, loss = 0.27003127\n",
      "Iteration 7, loss = 0.25609219\n",
      "Iteration 8, loss = 0.24330564\n",
      "Iteration 9, loss = 0.23263102\n",
      "Iteration 10, loss = 0.22326464\n",
      "Iteration 11, loss = 0.21517074\n",
      "Iteration 12, loss = 0.20776664\n",
      "Iteration 13, loss = 0.20113692\n",
      "Iteration 14, loss = 0.19535123\n",
      "Iteration 15, loss = 0.19052177\n",
      "Iteration 16, loss = 0.18583759\n",
      "Iteration 17, loss = 0.18150438\n",
      "Iteration 18, loss = 0.17758882\n",
      "Iteration 19, loss = 0.17393098\n",
      "Iteration 20, loss = 0.17024657\n",
      "Iteration 21, loss = 0.16724215\n",
      "Iteration 22, loss = 0.16429068\n",
      "Iteration 23, loss = 0.16160023\n",
      "Iteration 24, loss = 0.15879805\n",
      "Iteration 25, loss = 0.15633439\n",
      "Iteration 26, loss = 0.15430977\n",
      "Iteration 27, loss = 0.15211231\n",
      "Iteration 28, loss = 0.14970110\n",
      "Iteration 29, loss = 0.14790230\n",
      "Iteration 30, loss = 0.14564309\n",
      "Iteration 31, loss = 0.14380834\n",
      "Iteration 32, loss = 0.14252307\n",
      "Iteration 33, loss = 0.14014247\n",
      "Iteration 34, loss = 0.13888672\n",
      "Iteration 35, loss = 0.13708398\n",
      "Iteration 36, loss = 0.13566989\n",
      "Iteration 37, loss = 0.13432020\n",
      "Iteration 38, loss = 0.13274637\n",
      "Iteration 39, loss = 0.13132882\n",
      "Iteration 40, loss = 0.13009561\n",
      "Iteration 41, loss = 0.12896247\n",
      "Iteration 42, loss = 0.12718475\n",
      "Iteration 43, loss = 0.12613637\n",
      "Iteration 44, loss = 0.12502263\n",
      "Iteration 45, loss = 0.12386003\n",
      "Iteration 46, loss = 0.12281989\n",
      "Iteration 47, loss = 0.12153941\n",
      "Iteration 48, loss = 0.12069112\n",
      "Iteration 49, loss = 0.11943591\n",
      "Iteration 50, loss = 0.11826720\n",
      "Iteration 51, loss = 0.11733946\n",
      "Iteration 52, loss = 0.11609366\n",
      "Iteration 53, loss = 0.11537866\n",
      "Iteration 54, loss = 0.11423339\n",
      "Iteration 55, loss = 0.11333455\n",
      "Iteration 56, loss = 0.11270729\n",
      "Iteration 57, loss = 0.11143688\n",
      "Iteration 58, loss = 0.11059052\n",
      "Iteration 59, loss = 0.10983559\n",
      "Iteration 60, loss = 0.10887590\n",
      "Iteration 61, loss = 0.10846296\n",
      "Iteration 62, loss = 0.10762884\n",
      "Iteration 63, loss = 0.10661766\n",
      "Iteration 64, loss = 0.10574325\n",
      "Iteration 65, loss = 0.10491461\n",
      "Iteration 66, loss = 0.10434610\n",
      "Iteration 67, loss = 0.10346694\n",
      "Iteration 68, loss = 0.10272283\n",
      "Iteration 69, loss = 0.10233000\n",
      "Iteration 70, loss = 0.10137213\n",
      "Iteration 71, loss = 0.10053517\n",
      "Iteration 72, loss = 0.09980476\n",
      "Iteration 73, loss = 0.09925630\n",
      "Iteration 74, loss = 0.09840893\n",
      "Iteration 75, loss = 0.09746460\n",
      "Iteration 76, loss = 0.09713257\n",
      "Iteration 77, loss = 0.09657343\n",
      "Iteration 78, loss = 0.09599908\n",
      "Iteration 79, loss = 0.09547769\n",
      "Iteration 80, loss = 0.09496898\n",
      "Iteration 81, loss = 0.09412728\n",
      "Iteration 82, loss = 0.09386887\n",
      "Iteration 83, loss = 0.09301486\n",
      "Iteration 84, loss = 0.09229895\n",
      "Iteration 85, loss = 0.09217955\n",
      "Iteration 86, loss = 0.09134132\n",
      "Iteration 87, loss = 0.09064872\n",
      "Iteration 88, loss = 0.09047040\n",
      "Iteration 89, loss = 0.09015247\n",
      "Iteration 90, loss = 0.08940419\n",
      "Iteration 91, loss = 0.08884301\n",
      "Iteration 92, loss = 0.08849517\n",
      "Iteration 93, loss = 0.08780405\n",
      "Iteration 94, loss = 0.08738943\n",
      "Iteration 95, loss = 0.08711571\n",
      "Iteration 96, loss = 0.08653424\n",
      "Iteration 97, loss = 0.08599748\n",
      "Iteration 98, loss = 0.08555007\n",
      "Iteration 99, loss = 0.08489437\n",
      "Iteration 100, loss = 0.08465627\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kileymack/opt/anaconda3/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 1.79537804\n",
      "Iteration 2, loss = 0.35569750\n",
      "Iteration 3, loss = 0.27735984\n",
      "Iteration 4, loss = 0.24005826\n",
      "Iteration 5, loss = 0.22367936\n",
      "Iteration 6, loss = 0.20595972\n",
      "Iteration 7, loss = 0.19279201\n",
      "Iteration 8, loss = 0.18129671\n",
      "Iteration 9, loss = 0.16938469\n",
      "Iteration 10, loss = 0.15986086\n",
      "Iteration 11, loss = 0.15530806\n",
      "Iteration 12, loss = 0.14911715\n",
      "Iteration 13, loss = 0.14219342\n",
      "Iteration 14, loss = 0.13823590\n",
      "Iteration 15, loss = 0.13335924\n",
      "Iteration 16, loss = 0.12708641\n",
      "Iteration 17, loss = 0.12732609\n",
      "Iteration 18, loss = 0.11905568\n",
      "Iteration 19, loss = 0.11778210\n",
      "Iteration 20, loss = 0.11666455\n",
      "Iteration 21, loss = 0.10984433\n",
      "Iteration 22, loss = 0.10835154\n",
      "Iteration 23, loss = 0.10361951\n",
      "Iteration 24, loss = 0.10298653\n",
      "Iteration 25, loss = 0.10011717\n",
      "Iteration 26, loss = 0.10509650\n",
      "Iteration 27, loss = 0.09731059\n",
      "Iteration 28, loss = 0.09426926\n",
      "Iteration 29, loss = 0.09410761\n",
      "Iteration 30, loss = 0.08945032\n",
      "Iteration 31, loss = 0.08996209\n",
      "Iteration 32, loss = 0.09067854\n",
      "Iteration 33, loss = 0.08677973\n",
      "Iteration 34, loss = 0.08682311\n",
      "Iteration 35, loss = 0.08192895\n",
      "Iteration 36, loss = 0.08340829\n",
      "Iteration 37, loss = 0.07882493\n",
      "Iteration 38, loss = 0.07960748\n",
      "Iteration 39, loss = 0.07968499\n",
      "Iteration 40, loss = 0.07715934\n",
      "Iteration 41, loss = 0.07503981\n",
      "Iteration 42, loss = 0.07601929\n",
      "Iteration 43, loss = 0.07573892\n",
      "Iteration 44, loss = 0.07399484\n",
      "Iteration 45, loss = 0.07150400\n",
      "Iteration 46, loss = 0.07239814\n",
      "Iteration 47, loss = 0.07080609\n",
      "Iteration 48, loss = 0.06912387\n",
      "Iteration 49, loss = 0.07047483\n",
      "Iteration 50, loss = 0.06854873\n",
      "Iteration 51, loss = 0.06906241\n",
      "Iteration 52, loss = 0.06484602\n",
      "Iteration 53, loss = 0.06589741\n",
      "Iteration 54, loss = 0.06600969\n",
      "Iteration 55, loss = 0.06517885\n",
      "Iteration 56, loss = 0.06278128\n",
      "Iteration 57, loss = 0.06171216\n",
      "Iteration 58, loss = 0.06413221\n",
      "Iteration 59, loss = 0.06241286\n",
      "Iteration 60, loss = 0.06026483\n",
      "Iteration 61, loss = 0.06071468\n",
      "Iteration 62, loss = 0.06038451\n",
      "Iteration 63, loss = 0.05912760\n",
      "Iteration 64, loss = 0.05839417\n",
      "Iteration 65, loss = 0.05913665\n",
      "Iteration 66, loss = 0.05746514\n",
      "Iteration 67, loss = 0.05736507\n",
      "Iteration 68, loss = 0.05711984\n",
      "Iteration 69, loss = 0.05676827\n",
      "Iteration 70, loss = 0.05646317\n",
      "Iteration 71, loss = 0.05583898\n",
      "Iteration 72, loss = 0.05587802\n",
      "Iteration 73, loss = 0.05567334\n",
      "Iteration 74, loss = 0.05392670\n",
      "Iteration 75, loss = 0.05432131\n",
      "Iteration 76, loss = 0.05316275\n",
      "Iteration 77, loss = 0.05183486\n",
      "Iteration 78, loss = 0.05247515\n",
      "Iteration 79, loss = 0.05340240\n",
      "Iteration 80, loss = 0.05254519\n",
      "Iteration 81, loss = 0.05079238\n",
      "Iteration 82, loss = 0.05102039\n",
      "Iteration 83, loss = 0.05088187\n",
      "Iteration 84, loss = 0.04987148\n",
      "Iteration 85, loss = 0.05178653\n",
      "Iteration 86, loss = 0.04980135\n",
      "Iteration 87, loss = 0.04911498\n",
      "Iteration 88, loss = 0.04972424\n",
      "Iteration 89, loss = 0.04896578\n",
      "Iteration 90, loss = 0.04888881\n",
      "Iteration 91, loss = 0.04831240\n",
      "Iteration 92, loss = 0.04896802\n",
      "Iteration 93, loss = 0.04711544\n",
      "Iteration 94, loss = 0.04837178\n",
      "Iteration 95, loss = 0.04699043\n",
      "Iteration 96, loss = 0.04629151\n",
      "Iteration 97, loss = 0.04745532\n",
      "Iteration 98, loss = 0.04572501\n",
      "Iteration 99, loss = 0.04636623\n",
      "Iteration 100, loss = 0.04439801\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kileymack/opt/anaconda3/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 1.82055208\n",
      "Iteration 2, loss = 0.35375178\n",
      "Iteration 3, loss = 0.28792564\n",
      "Iteration 4, loss = 0.25491667\n",
      "Iteration 5, loss = 0.22810535\n",
      "Iteration 6, loss = 0.20780574\n",
      "Iteration 7, loss = 0.19655253\n",
      "Iteration 8, loss = 0.18190261\n",
      "Iteration 9, loss = 0.17301074\n",
      "Iteration 10, loss = 0.16354680\n",
      "Iteration 11, loss = 0.15581673\n",
      "Iteration 12, loss = 0.14787600\n",
      "Iteration 13, loss = 0.14411010\n",
      "Iteration 14, loss = 0.13676072\n",
      "Iteration 15, loss = 0.13331820\n",
      "Iteration 16, loss = 0.12766885\n",
      "Iteration 17, loss = 0.12667783\n",
      "Iteration 18, loss = 0.12127837\n",
      "Iteration 19, loss = 0.11803146\n",
      "Iteration 20, loss = 0.11650837\n",
      "Iteration 21, loss = 0.11150647\n",
      "Iteration 22, loss = 0.10964145\n",
      "Iteration 23, loss = 0.10709935\n",
      "Iteration 24, loss = 0.10477919\n",
      "Iteration 25, loss = 0.10314670\n",
      "Iteration 26, loss = 0.10091271\n",
      "Iteration 27, loss = 0.09912004\n",
      "Iteration 28, loss = 0.09668267\n",
      "Iteration 29, loss = 0.09541126\n",
      "Iteration 30, loss = 0.09379228\n",
      "Iteration 31, loss = 0.09225530\n",
      "Iteration 32, loss = 0.09044663\n",
      "Iteration 33, loss = 0.08932181\n",
      "Iteration 34, loss = 0.08734218\n",
      "Iteration 35, loss = 0.08657005\n",
      "Iteration 36, loss = 0.08525521\n",
      "Iteration 37, loss = 0.08286328\n",
      "Iteration 38, loss = 0.08245075\n",
      "Iteration 39, loss = 0.08181355\n",
      "Iteration 40, loss = 0.08060334\n",
      "Iteration 41, loss = 0.08088245\n",
      "Iteration 42, loss = 0.07917943\n",
      "Iteration 43, loss = 0.07806691\n",
      "Iteration 44, loss = 0.07699744\n",
      "Iteration 45, loss = 0.07573095\n",
      "Iteration 46, loss = 0.07595266\n",
      "Iteration 47, loss = 0.07433728\n",
      "Iteration 48, loss = 0.07440755\n",
      "Iteration 49, loss = 0.07279722\n",
      "Iteration 50, loss = 0.07309599\n",
      "Iteration 51, loss = 0.07204954\n",
      "Iteration 52, loss = 0.07122810\n",
      "Iteration 53, loss = 0.07030596\n",
      "Iteration 54, loss = 0.07008697\n",
      "Iteration 55, loss = 0.06889575\n",
      "Iteration 56, loss = 0.06937613\n",
      "Iteration 57, loss = 0.06778512\n",
      "Iteration 58, loss = 0.06716843\n",
      "Iteration 59, loss = 0.06718004\n",
      "Iteration 60, loss = 0.06664804\n",
      "Iteration 61, loss = 0.06579025\n",
      "Iteration 62, loss = 0.06520318\n",
      "Iteration 63, loss = 0.06459295\n",
      "Iteration 64, loss = 0.06501089\n",
      "Iteration 65, loss = 0.06452231\n",
      "Iteration 66, loss = 0.06322740\n",
      "Iteration 67, loss = 0.06379505\n",
      "Iteration 68, loss = 0.06197806\n",
      "Iteration 69, loss = 0.06319014\n",
      "Iteration 70, loss = 0.06245737\n",
      "Iteration 71, loss = 0.06072136\n",
      "Iteration 72, loss = 0.06098321\n",
      "Iteration 73, loss = 0.06223373\n",
      "Iteration 74, loss = 0.05989467\n",
      "Iteration 75, loss = 0.06006740\n",
      "Iteration 76, loss = 0.05905619\n",
      "Iteration 77, loss = 0.05933904\n",
      "Iteration 78, loss = 0.05911846\n",
      "Iteration 79, loss = 0.05921713\n",
      "Iteration 80, loss = 0.05819134\n",
      "Iteration 81, loss = 0.05788982\n",
      "Iteration 82, loss = 0.05770271\n",
      "Iteration 83, loss = 0.05759591\n",
      "Iteration 84, loss = 0.05691926\n",
      "Iteration 85, loss = 0.05671552\n",
      "Iteration 86, loss = 0.05730146\n",
      "Iteration 87, loss = 0.05521768\n",
      "Iteration 88, loss = 0.05607367\n",
      "Iteration 89, loss = 0.05521348\n",
      "Iteration 90, loss = 0.05555705\n",
      "Iteration 91, loss = 0.05461693\n",
      "Iteration 92, loss = 0.05479120\n",
      "Iteration 93, loss = 0.05446193\n",
      "Iteration 94, loss = 0.05372125\n",
      "Iteration 95, loss = 0.05331411\n",
      "Iteration 96, loss = 0.05437827\n",
      "Iteration 97, loss = 0.05365237\n",
      "Iteration 98, loss = 0.05340795\n",
      "Iteration 99, loss = 0.05297588\n",
      "Iteration 100, loss = 0.05302534\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kileymack/opt/anaconda3/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 1.50163740\n",
      "Iteration 2, loss = 0.43194306\n",
      "Iteration 3, loss = 0.36275147\n",
      "Iteration 4, loss = 0.32858425\n",
      "Iteration 5, loss = 0.30588348\n",
      "Iteration 6, loss = 0.28794729\n",
      "Iteration 7, loss = 0.27387550\n",
      "Iteration 8, loss = 0.26350600\n",
      "Iteration 9, loss = 0.25356447\n",
      "Iteration 10, loss = 0.24567120\n",
      "Iteration 11, loss = 0.23830783\n",
      "Iteration 12, loss = 0.23254144\n",
      "Iteration 13, loss = 0.22670626\n",
      "Iteration 14, loss = 0.22051237\n",
      "Iteration 15, loss = 0.21557155\n",
      "Iteration 16, loss = 0.21245853\n",
      "Iteration 17, loss = 0.20855779\n",
      "Iteration 18, loss = 0.20378095\n",
      "Iteration 19, loss = 0.20136033\n",
      "Iteration 20, loss = 0.19735238\n",
      "Iteration 21, loss = 0.19500617\n",
      "Iteration 22, loss = 0.19139177\n",
      "Iteration 23, loss = 0.18831765\n",
      "Iteration 24, loss = 0.18656088\n",
      "Iteration 25, loss = 0.18393233\n",
      "Iteration 26, loss = 0.18134463\n",
      "Iteration 27, loss = 0.17813908\n",
      "Iteration 28, loss = 0.17691717\n",
      "Iteration 29, loss = 0.17468771\n",
      "Iteration 30, loss = 0.17343200\n",
      "Iteration 31, loss = 0.17005520\n",
      "Iteration 32, loss = 0.16827957\n",
      "Iteration 33, loss = 0.16665268\n",
      "Iteration 34, loss = 0.16594784\n",
      "Iteration 35, loss = 0.16307640\n",
      "Iteration 36, loss = 0.16255913\n",
      "Iteration 37, loss = 0.16038146\n",
      "Iteration 38, loss = 0.15847857\n",
      "Iteration 39, loss = 0.15647380\n",
      "Iteration 40, loss = 0.15586585\n",
      "Iteration 41, loss = 0.15463716\n",
      "Iteration 42, loss = 0.15335859\n",
      "Iteration 43, loss = 0.15260235\n",
      "Iteration 44, loss = 0.15019665\n",
      "Iteration 45, loss = 0.14947418\n",
      "Iteration 46, loss = 0.14745086\n",
      "Iteration 47, loss = 0.14762990\n",
      "Iteration 48, loss = 0.14562610\n",
      "Iteration 49, loss = 0.14434446\n",
      "Iteration 50, loss = 0.14399398\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kileymack/opt/anaconda3/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (50) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 1.18105933\n",
      "Iteration 2, loss = 0.42112866\n",
      "Iteration 3, loss = 0.36483488\n",
      "Iteration 4, loss = 0.33166615\n",
      "Iteration 5, loss = 0.30880972\n",
      "Iteration 6, loss = 0.29089494\n",
      "Iteration 7, loss = 0.27649669\n",
      "Iteration 8, loss = 0.26455232\n",
      "Iteration 9, loss = 0.25445677\n",
      "Iteration 10, loss = 0.24582540\n",
      "Iteration 11, loss = 0.23804374\n",
      "Iteration 12, loss = 0.23104544\n",
      "Iteration 13, loss = 0.22546150\n",
      "Iteration 14, loss = 0.21970918\n",
      "Iteration 15, loss = 0.21408651\n",
      "Iteration 16, loss = 0.20933101\n",
      "Iteration 17, loss = 0.20430557\n",
      "Iteration 18, loss = 0.20089827\n",
      "Iteration 19, loss = 0.19646189\n",
      "Iteration 20, loss = 0.19375971\n",
      "Iteration 21, loss = 0.18993085\n",
      "Iteration 22, loss = 0.18676812\n",
      "Iteration 23, loss = 0.18374805\n",
      "Iteration 24, loss = 0.18126931\n",
      "Iteration 25, loss = 0.17857049\n",
      "Iteration 26, loss = 0.17653620\n",
      "Iteration 27, loss = 0.17357805\n",
      "Iteration 28, loss = 0.17142093\n",
      "Iteration 29, loss = 0.16886621\n",
      "Iteration 30, loss = 0.16712429\n",
      "Iteration 31, loss = 0.16511738\n",
      "Iteration 32, loss = 0.16302588\n",
      "Iteration 33, loss = 0.16102342\n",
      "Iteration 34, loss = 0.15907785\n",
      "Iteration 35, loss = 0.15707843\n",
      "Iteration 36, loss = 0.15574682\n",
      "Iteration 37, loss = 0.15385871\n",
      "Iteration 38, loss = 0.15248506\n",
      "Iteration 39, loss = 0.15115892\n",
      "Iteration 40, loss = 0.14983040\n",
      "Iteration 41, loss = 0.14796682\n",
      "Iteration 42, loss = 0.14684532\n",
      "Iteration 43, loss = 0.14558816\n",
      "Iteration 44, loss = 0.14403687\n",
      "Iteration 45, loss = 0.14239654\n",
      "Iteration 46, loss = 0.14141237\n",
      "Iteration 47, loss = 0.14026808\n",
      "Iteration 48, loss = 0.13915421\n",
      "Iteration 49, loss = 0.13819331\n",
      "Iteration 50, loss = 0.13710067\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kileymack/opt/anaconda3/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (50) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 1.63053224\n",
      "Iteration 2, loss = 0.36800377\n",
      "Iteration 3, loss = 0.30434566\n",
      "Iteration 4, loss = 0.27091582\n",
      "Iteration 5, loss = 0.25337828\n",
      "Iteration 6, loss = 0.23906900\n",
      "Iteration 7, loss = 0.22113058\n",
      "Iteration 8, loss = 0.21162049\n",
      "Iteration 9, loss = 0.20897136\n",
      "Iteration 10, loss = 0.19671928\n",
      "Iteration 11, loss = 0.19026307\n",
      "Iteration 12, loss = 0.18284757\n",
      "Iteration 13, loss = 0.18075690\n",
      "Iteration 14, loss = 0.16857505\n",
      "Iteration 15, loss = 0.16717012\n",
      "Iteration 16, loss = 0.16492217\n",
      "Iteration 17, loss = 0.15860898\n",
      "Iteration 18, loss = 0.15514787\n",
      "Iteration 19, loss = 0.15292264\n",
      "Iteration 20, loss = 0.15226593\n",
      "Iteration 21, loss = 0.14724660\n",
      "Iteration 22, loss = 0.14374080\n",
      "Iteration 23, loss = 0.14543107\n",
      "Iteration 24, loss = 0.14263647\n",
      "Iteration 25, loss = 0.13485942\n",
      "Iteration 26, loss = 0.13589177\n",
      "Iteration 27, loss = 0.13230521\n",
      "Iteration 28, loss = 0.13171851\n",
      "Iteration 29, loss = 0.13151257\n",
      "Iteration 30, loss = 0.12932776\n",
      "Iteration 31, loss = 0.12812941\n",
      "Iteration 32, loss = 0.12924596\n",
      "Iteration 33, loss = 0.12591246\n",
      "Iteration 34, loss = 0.12169248\n",
      "Iteration 35, loss = 0.12310255\n",
      "Iteration 36, loss = 0.11892190\n",
      "Iteration 37, loss = 0.12145258\n",
      "Iteration 38, loss = 0.11675366\n",
      "Iteration 39, loss = 0.11689659\n",
      "Iteration 40, loss = 0.11529379\n",
      "Iteration 41, loss = 0.11947577\n",
      "Iteration 42, loss = 0.11417094\n",
      "Iteration 43, loss = 0.11215659\n",
      "Iteration 44, loss = 0.11159535\n",
      "Iteration 45, loss = 0.11408417\n",
      "Iteration 46, loss = 0.11238297\n",
      "Iteration 47, loss = 0.10986600\n",
      "Iteration 48, loss = 0.10939612\n",
      "Iteration 49, loss = 0.10998866\n",
      "Iteration 50, loss = 0.10851730\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kileymack/opt/anaconda3/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (50) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 1.74118647\n",
      "Iteration 2, loss = 0.37346328\n",
      "Iteration 3, loss = 0.31744032\n",
      "Iteration 4, loss = 0.28624224\n",
      "Iteration 5, loss = 0.26069538\n",
      "Iteration 6, loss = 0.24696759\n",
      "Iteration 7, loss = 0.23067661\n",
      "Iteration 8, loss = 0.21830264\n",
      "Iteration 9, loss = 0.20815838\n",
      "Iteration 10, loss = 0.20184503\n",
      "Iteration 11, loss = 0.19484426\n",
      "Iteration 12, loss = 0.18572701\n",
      "Iteration 13, loss = 0.18070792\n",
      "Iteration 14, loss = 0.17683941\n",
      "Iteration 15, loss = 0.17122998\n",
      "Iteration 16, loss = 0.16639477\n",
      "Iteration 17, loss = 0.16410691\n",
      "Iteration 18, loss = 0.16003912\n",
      "Iteration 19, loss = 0.15776427\n",
      "Iteration 20, loss = 0.15466355\n",
      "Iteration 21, loss = 0.14972406\n",
      "Iteration 22, loss = 0.14974723\n",
      "Iteration 23, loss = 0.14842463\n",
      "Iteration 24, loss = 0.14525655\n",
      "Iteration 25, loss = 0.14281596\n",
      "Iteration 26, loss = 0.14151314\n",
      "Iteration 27, loss = 0.13833748\n",
      "Iteration 28, loss = 0.13796811\n",
      "Iteration 29, loss = 0.13654175\n",
      "Iteration 30, loss = 0.13419818\n",
      "Iteration 31, loss = 0.13373789\n",
      "Iteration 32, loss = 0.13105485\n",
      "Iteration 33, loss = 0.13044854\n",
      "Iteration 34, loss = 0.13042268\n",
      "Iteration 35, loss = 0.12912243\n",
      "Iteration 36, loss = 0.12601125\n",
      "Iteration 37, loss = 0.12606218\n",
      "Iteration 38, loss = 0.12570914\n",
      "Iteration 39, loss = 0.12559008\n",
      "Iteration 40, loss = 0.12430057\n",
      "Iteration 41, loss = 0.12406255\n",
      "Iteration 42, loss = 0.12311675\n",
      "Iteration 43, loss = 0.12077518\n",
      "Iteration 44, loss = 0.12096347\n",
      "Iteration 45, loss = 0.12112998\n",
      "Iteration 46, loss = 0.11943321\n",
      "Iteration 47, loss = 0.11866679\n",
      "Iteration 48, loss = 0.11907931\n",
      "Iteration 49, loss = 0.11663937\n",
      "Iteration 50, loss = 0.11882034\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kileymack/opt/anaconda3/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (50) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 1.50163740\n",
      "Iteration 2, loss = 0.43194306\n",
      "Iteration 3, loss = 0.36275147\n",
      "Iteration 4, loss = 0.32858425\n",
      "Iteration 5, loss = 0.30588348\n",
      "Iteration 6, loss = 0.28794729\n",
      "Iteration 7, loss = 0.27387550\n",
      "Iteration 8, loss = 0.26350600\n",
      "Iteration 9, loss = 0.25356447\n",
      "Iteration 10, loss = 0.24567120\n",
      "Iteration 11, loss = 0.23830783\n",
      "Iteration 12, loss = 0.23254144\n",
      "Iteration 13, loss = 0.22670626\n",
      "Iteration 14, loss = 0.22051237\n",
      "Iteration 15, loss = 0.21557155\n",
      "Iteration 16, loss = 0.21245853\n",
      "Iteration 17, loss = 0.20855779\n",
      "Iteration 18, loss = 0.20378095\n",
      "Iteration 19, loss = 0.20136033\n",
      "Iteration 20, loss = 0.19735238\n",
      "Iteration 21, loss = 0.19500617\n",
      "Iteration 22, loss = 0.19139177\n",
      "Iteration 23, loss = 0.18831765\n",
      "Iteration 24, loss = 0.18656088\n",
      "Iteration 25, loss = 0.18393233\n",
      "Iteration 26, loss = 0.18134463\n",
      "Iteration 27, loss = 0.17813908\n",
      "Iteration 28, loss = 0.17691717\n",
      "Iteration 29, loss = 0.17468771\n",
      "Iteration 30, loss = 0.17343200\n",
      "Iteration 31, loss = 0.17005520\n",
      "Iteration 32, loss = 0.16827957\n",
      "Iteration 33, loss = 0.16665268\n",
      "Iteration 34, loss = 0.16594784\n",
      "Iteration 35, loss = 0.16307640\n",
      "Iteration 36, loss = 0.16255913\n",
      "Iteration 37, loss = 0.16038146\n",
      "Iteration 38, loss = 0.15847857\n",
      "Iteration 39, loss = 0.15647380\n",
      "Iteration 40, loss = 0.15586585\n",
      "Iteration 41, loss = 0.15463716\n",
      "Iteration 42, loss = 0.15335859\n",
      "Iteration 43, loss = 0.15260235\n",
      "Iteration 44, loss = 0.15019665\n",
      "Iteration 45, loss = 0.14947418\n",
      "Iteration 46, loss = 0.14745086\n",
      "Iteration 47, loss = 0.14762990\n",
      "Iteration 48, loss = 0.14562610\n",
      "Iteration 49, loss = 0.14434446\n",
      "Iteration 50, loss = 0.14399398\n",
      "Iteration 51, loss = 0.14251023\n",
      "Iteration 52, loss = 0.14168763\n",
      "Iteration 53, loss = 0.14050537\n",
      "Iteration 54, loss = 0.13885746\n",
      "Iteration 55, loss = 0.13873806\n",
      "Iteration 56, loss = 0.13796665\n",
      "Iteration 57, loss = 0.13619154\n",
      "Iteration 58, loss = 0.13578349\n",
      "Iteration 59, loss = 0.13470402\n",
      "Iteration 60, loss = 0.13277033\n",
      "Iteration 61, loss = 0.13328575\n",
      "Iteration 62, loss = 0.13198289\n",
      "Iteration 63, loss = 0.13126597\n",
      "Iteration 64, loss = 0.13033243\n",
      "Iteration 65, loss = 0.12931378\n",
      "Iteration 66, loss = 0.12919149\n",
      "Iteration 67, loss = 0.12865559\n",
      "Iteration 68, loss = 0.12772290\n",
      "Iteration 69, loss = 0.12595904\n",
      "Iteration 70, loss = 0.12558107\n",
      "Iteration 71, loss = 0.12511022\n",
      "Iteration 72, loss = 0.12463153\n",
      "Iteration 73, loss = 0.12349932\n",
      "Iteration 74, loss = 0.12360631\n",
      "Iteration 75, loss = 0.12346391\n",
      "Iteration 76, loss = 0.12243166\n",
      "Iteration 77, loss = 0.12158274\n",
      "Iteration 78, loss = 0.12040569\n",
      "Iteration 79, loss = 0.11916232\n",
      "Iteration 80, loss = 0.12117703\n",
      "Iteration 81, loss = 0.11866389\n",
      "Iteration 82, loss = 0.11765341\n",
      "Iteration 83, loss = 0.11748000\n",
      "Iteration 84, loss = 0.11804290\n",
      "Iteration 85, loss = 0.11649995\n",
      "Iteration 86, loss = 0.11594722\n",
      "Iteration 87, loss = 0.11541189\n",
      "Iteration 88, loss = 0.11497729\n",
      "Iteration 89, loss = 0.11398550\n",
      "Iteration 90, loss = 0.11460425\n",
      "Iteration 91, loss = 0.11332116\n",
      "Iteration 92, loss = 0.11335277\n",
      "Iteration 93, loss = 0.11198592\n",
      "Iteration 94, loss = 0.11162032\n",
      "Iteration 95, loss = 0.11214682\n",
      "Iteration 96, loss = 0.11122425\n",
      "Iteration 97, loss = 0.11140464\n",
      "Iteration 98, loss = 0.10985026\n",
      "Iteration 99, loss = 0.10925814\n",
      "Iteration 100, loss = 0.10898448\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kileymack/opt/anaconda3/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 1.18105933\n",
      "Iteration 2, loss = 0.42112866\n",
      "Iteration 3, loss = 0.36483488\n",
      "Iteration 4, loss = 0.33166615\n",
      "Iteration 5, loss = 0.30880972\n",
      "Iteration 6, loss = 0.29089494\n",
      "Iteration 7, loss = 0.27649669\n",
      "Iteration 8, loss = 0.26455232\n",
      "Iteration 9, loss = 0.25445677\n",
      "Iteration 10, loss = 0.24582540\n",
      "Iteration 11, loss = 0.23804374\n",
      "Iteration 12, loss = 0.23104544\n",
      "Iteration 13, loss = 0.22546150\n",
      "Iteration 14, loss = 0.21970918\n",
      "Iteration 15, loss = 0.21408651\n",
      "Iteration 16, loss = 0.20933101\n",
      "Iteration 17, loss = 0.20430557\n",
      "Iteration 18, loss = 0.20089827\n",
      "Iteration 19, loss = 0.19646189\n",
      "Iteration 20, loss = 0.19375971\n",
      "Iteration 21, loss = 0.18993085\n",
      "Iteration 22, loss = 0.18676812\n",
      "Iteration 23, loss = 0.18374805\n",
      "Iteration 24, loss = 0.18126931\n",
      "Iteration 25, loss = 0.17857049\n",
      "Iteration 26, loss = 0.17653620\n",
      "Iteration 27, loss = 0.17357805\n",
      "Iteration 28, loss = 0.17142093\n",
      "Iteration 29, loss = 0.16886621\n",
      "Iteration 30, loss = 0.16712429\n",
      "Iteration 31, loss = 0.16511738\n",
      "Iteration 32, loss = 0.16302588\n",
      "Iteration 33, loss = 0.16102342\n",
      "Iteration 34, loss = 0.15907785\n",
      "Iteration 35, loss = 0.15707843\n",
      "Iteration 36, loss = 0.15574682\n",
      "Iteration 37, loss = 0.15385871\n",
      "Iteration 38, loss = 0.15248506\n",
      "Iteration 39, loss = 0.15115892\n",
      "Iteration 40, loss = 0.14983040\n",
      "Iteration 41, loss = 0.14796682\n",
      "Iteration 42, loss = 0.14684532\n",
      "Iteration 43, loss = 0.14558816\n",
      "Iteration 44, loss = 0.14403687\n",
      "Iteration 45, loss = 0.14239654\n",
      "Iteration 46, loss = 0.14141237\n",
      "Iteration 47, loss = 0.14026808\n",
      "Iteration 48, loss = 0.13915421\n",
      "Iteration 49, loss = 0.13819331\n",
      "Iteration 50, loss = 0.13710067\n",
      "Iteration 51, loss = 0.13594493\n",
      "Iteration 52, loss = 0.13461776\n",
      "Iteration 53, loss = 0.13403627\n",
      "Iteration 54, loss = 0.13249313\n",
      "Iteration 55, loss = 0.13177895\n",
      "Iteration 56, loss = 0.13037836\n",
      "Iteration 57, loss = 0.12939024\n",
      "Iteration 58, loss = 0.12864977\n",
      "Iteration 59, loss = 0.12793250\n",
      "Iteration 60, loss = 0.12659073\n",
      "Iteration 61, loss = 0.12574432\n",
      "Iteration 62, loss = 0.12499077\n",
      "Iteration 63, loss = 0.12463465\n",
      "Iteration 64, loss = 0.12350785\n",
      "Iteration 65, loss = 0.12271521\n",
      "Iteration 66, loss = 0.12190912\n",
      "Iteration 67, loss = 0.12105701\n",
      "Iteration 68, loss = 0.12004038\n",
      "Iteration 69, loss = 0.11941497\n",
      "Iteration 70, loss = 0.11861874\n",
      "Iteration 71, loss = 0.11820188\n",
      "Iteration 72, loss = 0.11723044\n",
      "Iteration 73, loss = 0.11634512\n",
      "Iteration 74, loss = 0.11578905\n",
      "Iteration 75, loss = 0.11522691\n",
      "Iteration 76, loss = 0.11438532\n",
      "Iteration 77, loss = 0.11352666\n",
      "Iteration 78, loss = 0.11323904\n",
      "Iteration 79, loss = 0.11208109\n",
      "Iteration 80, loss = 0.11205429\n",
      "Iteration 81, loss = 0.11120927\n",
      "Iteration 82, loss = 0.11036984\n",
      "Iteration 83, loss = 0.11013549\n",
      "Iteration 84, loss = 0.10908400\n",
      "Iteration 85, loss = 0.10883019\n",
      "Iteration 86, loss = 0.10819511\n",
      "Iteration 87, loss = 0.10759605\n",
      "Iteration 88, loss = 0.10652232\n",
      "Iteration 89, loss = 0.10643424\n",
      "Iteration 90, loss = 0.10584851\n",
      "Iteration 91, loss = 0.10542608\n",
      "Iteration 92, loss = 0.10466555\n",
      "Iteration 93, loss = 0.10442939\n",
      "Iteration 94, loss = 0.10365984\n",
      "Iteration 95, loss = 0.10316269\n",
      "Iteration 96, loss = 0.10250505\n",
      "Iteration 97, loss = 0.10221863\n",
      "Iteration 98, loss = 0.10203616\n",
      "Iteration 99, loss = 0.10115041\n",
      "Iteration 100, loss = 0.10101427\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kileymack/opt/anaconda3/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 1.63053224\n",
      "Iteration 2, loss = 0.36800377\n",
      "Iteration 3, loss = 0.30434566\n",
      "Iteration 4, loss = 0.27091582\n",
      "Iteration 5, loss = 0.25337828\n",
      "Iteration 6, loss = 0.23906900\n",
      "Iteration 7, loss = 0.22113058\n",
      "Iteration 8, loss = 0.21162049\n",
      "Iteration 9, loss = 0.20897136\n",
      "Iteration 10, loss = 0.19671928\n",
      "Iteration 11, loss = 0.19026307\n",
      "Iteration 12, loss = 0.18284757\n",
      "Iteration 13, loss = 0.18075690\n",
      "Iteration 14, loss = 0.16857505\n",
      "Iteration 15, loss = 0.16717012\n",
      "Iteration 16, loss = 0.16492217\n",
      "Iteration 17, loss = 0.15860898\n",
      "Iteration 18, loss = 0.15514787\n",
      "Iteration 19, loss = 0.15292264\n",
      "Iteration 20, loss = 0.15226593\n",
      "Iteration 21, loss = 0.14724660\n",
      "Iteration 22, loss = 0.14374080\n",
      "Iteration 23, loss = 0.14543107\n",
      "Iteration 24, loss = 0.14263647\n",
      "Iteration 25, loss = 0.13485942\n",
      "Iteration 26, loss = 0.13589177\n",
      "Iteration 27, loss = 0.13230521\n",
      "Iteration 28, loss = 0.13171851\n",
      "Iteration 29, loss = 0.13151257\n",
      "Iteration 30, loss = 0.12932776\n",
      "Iteration 31, loss = 0.12812941\n",
      "Iteration 32, loss = 0.12924596\n",
      "Iteration 33, loss = 0.12591246\n",
      "Iteration 34, loss = 0.12169248\n",
      "Iteration 35, loss = 0.12310255\n",
      "Iteration 36, loss = 0.11892190\n",
      "Iteration 37, loss = 0.12145258\n",
      "Iteration 38, loss = 0.11675366\n",
      "Iteration 39, loss = 0.11689659\n",
      "Iteration 40, loss = 0.11529379\n",
      "Iteration 41, loss = 0.11947577\n",
      "Iteration 42, loss = 0.11417094\n",
      "Iteration 43, loss = 0.11215659\n",
      "Iteration 44, loss = 0.11159535\n",
      "Iteration 45, loss = 0.11408417\n",
      "Iteration 46, loss = 0.11238297\n",
      "Iteration 47, loss = 0.10986600\n",
      "Iteration 48, loss = 0.10939612\n",
      "Iteration 49, loss = 0.10998866\n",
      "Iteration 50, loss = 0.10851730\n",
      "Iteration 51, loss = 0.10876386\n",
      "Iteration 52, loss = 0.10794998\n",
      "Iteration 53, loss = 0.10570979\n",
      "Iteration 54, loss = 0.10603867\n",
      "Iteration 55, loss = 0.10659256\n",
      "Iteration 56, loss = 0.10679533\n",
      "Iteration 57, loss = 0.10365312\n",
      "Iteration 58, loss = 0.10585303\n",
      "Iteration 59, loss = 0.10391812\n",
      "Iteration 60, loss = 0.10437027\n",
      "Iteration 61, loss = 0.10754719\n",
      "Iteration 62, loss = 0.10172431\n",
      "Iteration 63, loss = 0.10190850\n",
      "Iteration 64, loss = 0.10105322\n",
      "Iteration 65, loss = 0.10209199\n",
      "Iteration 66, loss = 0.10188718\n",
      "Iteration 67, loss = 0.10248065\n",
      "Iteration 68, loss = 0.10146889\n",
      "Iteration 69, loss = 0.10061162\n",
      "Iteration 70, loss = 0.10042604\n",
      "Iteration 71, loss = 0.09900922\n",
      "Iteration 72, loss = 0.10084689\n",
      "Iteration 73, loss = 0.09769134\n",
      "Iteration 74, loss = 0.09943000\n",
      "Iteration 75, loss = 0.09715785\n",
      "Iteration 76, loss = 0.09746215\n",
      "Iteration 77, loss = 0.09640787\n",
      "Iteration 78, loss = 0.09902508\n",
      "Iteration 79, loss = 0.09713241\n",
      "Iteration 80, loss = 0.09678748\n",
      "Iteration 81, loss = 0.09786190\n",
      "Iteration 82, loss = 0.09402564\n",
      "Iteration 83, loss = 0.09763573\n",
      "Iteration 84, loss = 0.09724111\n",
      "Iteration 85, loss = 0.09582293\n",
      "Iteration 86, loss = 0.09474450\n",
      "Iteration 87, loss = 0.09562231\n",
      "Iteration 88, loss = 0.09590602\n",
      "Iteration 89, loss = 0.09354726\n",
      "Iteration 90, loss = 0.09247104\n",
      "Iteration 91, loss = 0.09694007\n",
      "Iteration 92, loss = 0.09376177\n",
      "Iteration 93, loss = 0.09533974\n",
      "Iteration 94, loss = 0.09166855\n",
      "Iteration 95, loss = 0.09347854\n",
      "Iteration 96, loss = 0.09462972\n",
      "Iteration 97, loss = 0.09182904\n",
      "Iteration 98, loss = 0.09143512\n",
      "Iteration 99, loss = 0.09172267\n",
      "Iteration 100, loss = 0.09291215\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kileymack/opt/anaconda3/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 1.74118647\n",
      "Iteration 2, loss = 0.37346328\n",
      "Iteration 3, loss = 0.31744032\n",
      "Iteration 4, loss = 0.28624224\n",
      "Iteration 5, loss = 0.26069538\n",
      "Iteration 6, loss = 0.24696759\n",
      "Iteration 7, loss = 0.23067661\n",
      "Iteration 8, loss = 0.21830264\n",
      "Iteration 9, loss = 0.20815838\n",
      "Iteration 10, loss = 0.20184503\n",
      "Iteration 11, loss = 0.19484426\n",
      "Iteration 12, loss = 0.18572701\n",
      "Iteration 13, loss = 0.18070792\n",
      "Iteration 14, loss = 0.17683941\n",
      "Iteration 15, loss = 0.17122998\n",
      "Iteration 16, loss = 0.16639477\n",
      "Iteration 17, loss = 0.16410691\n",
      "Iteration 18, loss = 0.16003912\n",
      "Iteration 19, loss = 0.15776427\n",
      "Iteration 20, loss = 0.15466355\n",
      "Iteration 21, loss = 0.14972406\n",
      "Iteration 22, loss = 0.14974723\n",
      "Iteration 23, loss = 0.14842463\n",
      "Iteration 24, loss = 0.14525655\n",
      "Iteration 25, loss = 0.14281596\n",
      "Iteration 26, loss = 0.14151314\n",
      "Iteration 27, loss = 0.13833748\n",
      "Iteration 28, loss = 0.13796811\n",
      "Iteration 29, loss = 0.13654175\n",
      "Iteration 30, loss = 0.13419818\n",
      "Iteration 31, loss = 0.13373789\n",
      "Iteration 32, loss = 0.13105485\n",
      "Iteration 33, loss = 0.13044854\n",
      "Iteration 34, loss = 0.13042268\n",
      "Iteration 35, loss = 0.12912243\n",
      "Iteration 36, loss = 0.12601125\n",
      "Iteration 37, loss = 0.12606218\n",
      "Iteration 38, loss = 0.12570914\n",
      "Iteration 39, loss = 0.12559008\n",
      "Iteration 40, loss = 0.12430057\n",
      "Iteration 41, loss = 0.12406255\n",
      "Iteration 42, loss = 0.12311675\n",
      "Iteration 43, loss = 0.12077518\n",
      "Iteration 44, loss = 0.12096347\n",
      "Iteration 45, loss = 0.12112998\n",
      "Iteration 46, loss = 0.11943321\n",
      "Iteration 47, loss = 0.11866679\n",
      "Iteration 48, loss = 0.11907931\n",
      "Iteration 49, loss = 0.11663937\n",
      "Iteration 50, loss = 0.11882034\n",
      "Iteration 51, loss = 0.11638867\n",
      "Iteration 52, loss = 0.11698529\n",
      "Iteration 53, loss = 0.11732050\n",
      "Iteration 54, loss = 0.11466869\n",
      "Iteration 55, loss = 0.11505154\n",
      "Iteration 56, loss = 0.11426684\n",
      "Iteration 57, loss = 0.11395603\n",
      "Iteration 58, loss = 0.11329393\n",
      "Iteration 59, loss = 0.11356173\n",
      "Iteration 60, loss = 0.11355365\n",
      "Iteration 61, loss = 0.11027543\n",
      "Iteration 62, loss = 0.11199928\n",
      "Iteration 63, loss = 0.11160752\n",
      "Iteration 64, loss = 0.11243251\n",
      "Iteration 65, loss = 0.10992905\n",
      "Iteration 66, loss = 0.11063020\n",
      "Iteration 67, loss = 0.11077747\n",
      "Iteration 68, loss = 0.11008915\n",
      "Iteration 69, loss = 0.11085177\n",
      "Iteration 70, loss = 0.10893044\n",
      "Iteration 71, loss = 0.10957467\n",
      "Iteration 72, loss = 0.10766492\n",
      "Iteration 73, loss = 0.10862409\n",
      "Iteration 74, loss = 0.10974297\n",
      "Iteration 75, loss = 0.10740849\n",
      "Iteration 76, loss = 0.10826660\n",
      "Iteration 77, loss = 0.10768752\n",
      "Iteration 78, loss = 0.10719273\n",
      "Iteration 79, loss = 0.10666771\n",
      "Iteration 80, loss = 0.10662075\n",
      "Iteration 81, loss = 0.10568341\n",
      "Iteration 82, loss = 0.10727333\n",
      "Iteration 83, loss = 0.10666030\n",
      "Iteration 84, loss = 0.10575713\n",
      "Iteration 85, loss = 0.10574702\n",
      "Iteration 86, loss = 0.10524766\n",
      "Iteration 87, loss = 0.10563374\n",
      "Iteration 88, loss = 0.10685063\n",
      "Iteration 89, loss = 0.10493245\n",
      "Iteration 90, loss = 0.10313256\n",
      "Iteration 91, loss = 0.10419775\n",
      "Iteration 92, loss = 0.10503940\n",
      "Iteration 93, loss = 0.10379157\n",
      "Iteration 94, loss = 0.10329629\n",
      "Iteration 95, loss = 0.10345229\n",
      "Iteration 96, loss = 0.10436617\n",
      "Iteration 97, loss = 0.10360320\n",
      "Iteration 98, loss = 0.10507953\n",
      "Iteration 99, loss = 0.10351619\n",
      "Iteration 100, loss = 0.10446143\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kileymack/opt/anaconda3/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 1.50163740\n",
      "Iteration 2, loss = 0.43194306\n",
      "Iteration 3, loss = 0.36275147\n",
      "Iteration 4, loss = 0.32858425\n",
      "Iteration 5, loss = 0.30588348\n",
      "Iteration 6, loss = 0.28794729\n",
      "Iteration 7, loss = 0.27387550\n",
      "Iteration 8, loss = 0.26350600\n",
      "Iteration 9, loss = 0.25356447\n",
      "Iteration 10, loss = 0.24567120\n",
      "Iteration 11, loss = 0.23830783\n",
      "Iteration 12, loss = 0.23254144\n",
      "Iteration 13, loss = 0.22670626\n",
      "Iteration 14, loss = 0.22051237\n",
      "Iteration 15, loss = 0.21557155\n",
      "Iteration 16, loss = 0.21245853\n",
      "Iteration 17, loss = 0.20855779\n",
      "Iteration 18, loss = 0.20378095\n",
      "Iteration 19, loss = 0.20136033\n",
      "Iteration 20, loss = 0.19735238\n",
      "Iteration 21, loss = 0.19500617\n",
      "Iteration 22, loss = 0.19139177\n",
      "Iteration 23, loss = 0.18831765\n",
      "Iteration 24, loss = 0.18656088\n",
      "Iteration 25, loss = 0.18393233\n",
      "Iteration 26, loss = 0.18134463\n",
      "Iteration 27, loss = 0.17813908\n",
      "Iteration 28, loss = 0.17691717\n",
      "Iteration 29, loss = 0.17468771\n",
      "Iteration 30, loss = 0.17343200\n",
      "Iteration 31, loss = 0.17005520\n",
      "Iteration 32, loss = 0.16827957\n",
      "Iteration 33, loss = 0.16665268\n",
      "Iteration 34, loss = 0.16594784\n",
      "Iteration 35, loss = 0.16307640\n",
      "Iteration 36, loss = 0.16255913\n",
      "Iteration 37, loss = 0.16038146\n",
      "Iteration 38, loss = 0.15847857\n",
      "Iteration 39, loss = 0.15647380\n",
      "Iteration 40, loss = 0.15586585\n",
      "Iteration 41, loss = 0.15463716\n",
      "Iteration 42, loss = 0.15335859\n",
      "Iteration 43, loss = 0.15260235\n",
      "Iteration 44, loss = 0.15019665\n",
      "Iteration 45, loss = 0.14947418\n",
      "Iteration 46, loss = 0.14745086\n",
      "Iteration 47, loss = 0.14762990\n",
      "Iteration 48, loss = 0.14562610\n",
      "Iteration 49, loss = 0.14434446\n",
      "Iteration 50, loss = 0.14399398\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kileymack/opt/anaconda3/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (50) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 1.18105933\n",
      "Iteration 2, loss = 0.42112866\n",
      "Iteration 3, loss = 0.36483488\n",
      "Iteration 4, loss = 0.33166615\n",
      "Iteration 5, loss = 0.30880972\n",
      "Iteration 6, loss = 0.29089494\n",
      "Iteration 7, loss = 0.27649669\n",
      "Iteration 8, loss = 0.26455232\n",
      "Iteration 9, loss = 0.25445677\n",
      "Iteration 10, loss = 0.24582540\n",
      "Iteration 11, loss = 0.23804374\n",
      "Iteration 12, loss = 0.23104544\n",
      "Iteration 13, loss = 0.22546150\n",
      "Iteration 14, loss = 0.21970918\n",
      "Iteration 15, loss = 0.21408651\n",
      "Iteration 16, loss = 0.20933101\n",
      "Iteration 17, loss = 0.20430557\n",
      "Iteration 18, loss = 0.20089827\n",
      "Iteration 19, loss = 0.19646189\n",
      "Iteration 20, loss = 0.19375971\n",
      "Iteration 21, loss = 0.18993085\n",
      "Iteration 22, loss = 0.18676812\n",
      "Iteration 23, loss = 0.18374805\n",
      "Iteration 24, loss = 0.18126931\n",
      "Iteration 25, loss = 0.17857049\n",
      "Iteration 26, loss = 0.17653620\n",
      "Iteration 27, loss = 0.17357805\n",
      "Iteration 28, loss = 0.17142093\n",
      "Iteration 29, loss = 0.16886621\n",
      "Iteration 30, loss = 0.16712429\n",
      "Iteration 31, loss = 0.16511738\n",
      "Iteration 32, loss = 0.16302588\n",
      "Iteration 33, loss = 0.16102342\n",
      "Iteration 34, loss = 0.15907785\n",
      "Iteration 35, loss = 0.15707843\n",
      "Iteration 36, loss = 0.15574682\n",
      "Iteration 37, loss = 0.15385871\n",
      "Iteration 38, loss = 0.15248506\n",
      "Iteration 39, loss = 0.15115892\n",
      "Iteration 40, loss = 0.14983040\n",
      "Iteration 41, loss = 0.14796682\n",
      "Iteration 42, loss = 0.14684532\n",
      "Iteration 43, loss = 0.14558816\n",
      "Iteration 44, loss = 0.14403687\n",
      "Iteration 45, loss = 0.14239654\n",
      "Iteration 46, loss = 0.14141237\n",
      "Iteration 47, loss = 0.14026808\n",
      "Iteration 48, loss = 0.13915421\n",
      "Iteration 49, loss = 0.13819331\n",
      "Iteration 50, loss = 0.13710067\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kileymack/opt/anaconda3/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (50) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 1.63053224\n",
      "Iteration 2, loss = 0.36800377\n",
      "Iteration 3, loss = 0.30434566\n",
      "Iteration 4, loss = 0.27091582\n",
      "Iteration 5, loss = 0.25337828\n",
      "Iteration 6, loss = 0.23906900\n",
      "Iteration 7, loss = 0.22113058\n",
      "Iteration 8, loss = 0.21162049\n",
      "Iteration 9, loss = 0.20897136\n",
      "Iteration 10, loss = 0.19671928\n",
      "Iteration 11, loss = 0.19026307\n",
      "Iteration 12, loss = 0.18284757\n",
      "Iteration 13, loss = 0.18075690\n",
      "Iteration 14, loss = 0.16857505\n",
      "Iteration 15, loss = 0.16717012\n",
      "Iteration 16, loss = 0.16492217\n",
      "Iteration 17, loss = 0.15860898\n",
      "Iteration 18, loss = 0.15514787\n",
      "Iteration 19, loss = 0.15292264\n",
      "Iteration 20, loss = 0.15226593\n",
      "Iteration 21, loss = 0.14724660\n",
      "Iteration 22, loss = 0.14374080\n",
      "Iteration 23, loss = 0.14543107\n",
      "Iteration 24, loss = 0.14263647\n",
      "Iteration 25, loss = 0.13485942\n",
      "Iteration 26, loss = 0.13589177\n",
      "Iteration 27, loss = 0.13230521\n",
      "Iteration 28, loss = 0.13171851\n",
      "Iteration 29, loss = 0.13151257\n",
      "Iteration 30, loss = 0.12932776\n",
      "Iteration 31, loss = 0.12812941\n",
      "Iteration 32, loss = 0.12924596\n",
      "Iteration 33, loss = 0.12591246\n",
      "Iteration 34, loss = 0.12169248\n",
      "Iteration 35, loss = 0.12310255\n",
      "Iteration 36, loss = 0.11892190\n",
      "Iteration 37, loss = 0.12145258\n",
      "Iteration 38, loss = 0.11675366\n",
      "Iteration 39, loss = 0.11689659\n",
      "Iteration 40, loss = 0.11529379\n",
      "Iteration 41, loss = 0.11947577\n",
      "Iteration 42, loss = 0.11417094\n",
      "Iteration 43, loss = 0.11215659\n",
      "Iteration 44, loss = 0.11159535\n",
      "Iteration 45, loss = 0.11408417\n",
      "Iteration 46, loss = 0.11238297\n",
      "Iteration 47, loss = 0.10986600\n",
      "Iteration 48, loss = 0.10939612\n",
      "Iteration 49, loss = 0.10998866\n",
      "Iteration 50, loss = 0.10851730\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kileymack/opt/anaconda3/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (50) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 1.74118647\n",
      "Iteration 2, loss = 0.37346328\n",
      "Iteration 3, loss = 0.31744032\n",
      "Iteration 4, loss = 0.28624224\n",
      "Iteration 5, loss = 0.26069538\n",
      "Iteration 6, loss = 0.24696759\n",
      "Iteration 7, loss = 0.23067661\n",
      "Iteration 8, loss = 0.21830264\n",
      "Iteration 9, loss = 0.20815838\n",
      "Iteration 10, loss = 0.20184503\n",
      "Iteration 11, loss = 0.19484426\n",
      "Iteration 12, loss = 0.18572701\n",
      "Iteration 13, loss = 0.18070792\n",
      "Iteration 14, loss = 0.17683941\n",
      "Iteration 15, loss = 0.17122998\n",
      "Iteration 16, loss = 0.16639477\n",
      "Iteration 17, loss = 0.16410691\n",
      "Iteration 18, loss = 0.16003912\n",
      "Iteration 19, loss = 0.15776427\n",
      "Iteration 20, loss = 0.15466355\n",
      "Iteration 21, loss = 0.14972406\n",
      "Iteration 22, loss = 0.14974723\n",
      "Iteration 23, loss = 0.14842463\n",
      "Iteration 24, loss = 0.14525655\n",
      "Iteration 25, loss = 0.14281596\n",
      "Iteration 26, loss = 0.14151314\n",
      "Iteration 27, loss = 0.13833748\n",
      "Iteration 28, loss = 0.13796811\n",
      "Iteration 29, loss = 0.13654175\n",
      "Iteration 30, loss = 0.13419818\n",
      "Iteration 31, loss = 0.13373789\n",
      "Iteration 32, loss = 0.13105485\n",
      "Iteration 33, loss = 0.13044854\n",
      "Iteration 34, loss = 0.13042268\n",
      "Iteration 35, loss = 0.12912243\n",
      "Iteration 36, loss = 0.12601125\n",
      "Iteration 37, loss = 0.12606218\n",
      "Iteration 38, loss = 0.12570914\n",
      "Iteration 39, loss = 0.12559008\n",
      "Iteration 40, loss = 0.12430057\n",
      "Iteration 41, loss = 0.12406255\n",
      "Iteration 42, loss = 0.12311675\n",
      "Iteration 43, loss = 0.12077518\n",
      "Iteration 44, loss = 0.12096347\n",
      "Iteration 45, loss = 0.12112998\n",
      "Iteration 46, loss = 0.11943321\n",
      "Iteration 47, loss = 0.11866679\n",
      "Iteration 48, loss = 0.11907931\n",
      "Iteration 49, loss = 0.11663937\n",
      "Iteration 50, loss = 0.11882034\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kileymack/opt/anaconda3/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (50) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 1.50163740\n",
      "Iteration 2, loss = 0.43194306\n",
      "Iteration 3, loss = 0.36275147\n",
      "Iteration 4, loss = 0.32858425\n",
      "Iteration 5, loss = 0.30588348\n",
      "Iteration 6, loss = 0.28794729\n",
      "Iteration 7, loss = 0.27387550\n",
      "Iteration 8, loss = 0.26350600\n",
      "Iteration 9, loss = 0.25356447\n",
      "Iteration 10, loss = 0.24567120\n",
      "Iteration 11, loss = 0.23830783\n",
      "Iteration 12, loss = 0.23254144\n",
      "Iteration 13, loss = 0.22670626\n",
      "Iteration 14, loss = 0.22051237\n",
      "Iteration 15, loss = 0.21557155\n",
      "Iteration 16, loss = 0.21245853\n",
      "Iteration 17, loss = 0.20855779\n",
      "Iteration 18, loss = 0.20378095\n",
      "Iteration 19, loss = 0.20136033\n",
      "Iteration 20, loss = 0.19735238\n",
      "Iteration 21, loss = 0.19500617\n",
      "Iteration 22, loss = 0.19139177\n",
      "Iteration 23, loss = 0.18831765\n",
      "Iteration 24, loss = 0.18656088\n",
      "Iteration 25, loss = 0.18393233\n",
      "Iteration 26, loss = 0.18134463\n",
      "Iteration 27, loss = 0.17813908\n",
      "Iteration 28, loss = 0.17691717\n",
      "Iteration 29, loss = 0.17468771\n",
      "Iteration 30, loss = 0.17343200\n",
      "Iteration 31, loss = 0.17005520\n",
      "Iteration 32, loss = 0.16827957\n",
      "Iteration 33, loss = 0.16665268\n",
      "Iteration 34, loss = 0.16594784\n",
      "Iteration 35, loss = 0.16307640\n",
      "Iteration 36, loss = 0.16255913\n",
      "Iteration 37, loss = 0.16038146\n",
      "Iteration 38, loss = 0.15847857\n",
      "Iteration 39, loss = 0.15647380\n",
      "Iteration 40, loss = 0.15586585\n",
      "Iteration 41, loss = 0.15463716\n",
      "Iteration 42, loss = 0.15335859\n",
      "Iteration 43, loss = 0.15260235\n",
      "Iteration 44, loss = 0.15019665\n",
      "Iteration 45, loss = 0.14947418\n",
      "Iteration 46, loss = 0.14745086\n",
      "Iteration 47, loss = 0.14762990\n",
      "Iteration 48, loss = 0.14562610\n",
      "Iteration 49, loss = 0.14434446\n",
      "Iteration 50, loss = 0.14399398\n",
      "Iteration 51, loss = 0.14251023\n",
      "Iteration 52, loss = 0.14168763\n",
      "Iteration 53, loss = 0.14050537\n",
      "Iteration 54, loss = 0.13885746\n",
      "Iteration 55, loss = 0.13873806\n",
      "Iteration 56, loss = 0.13796665\n",
      "Iteration 57, loss = 0.13619154\n",
      "Iteration 58, loss = 0.13578349\n",
      "Iteration 59, loss = 0.13470402\n",
      "Iteration 60, loss = 0.13277033\n",
      "Iteration 61, loss = 0.13328575\n",
      "Iteration 62, loss = 0.13198289\n",
      "Iteration 63, loss = 0.13126597\n",
      "Iteration 64, loss = 0.13033243\n",
      "Iteration 65, loss = 0.12931378\n",
      "Iteration 66, loss = 0.12919149\n",
      "Iteration 67, loss = 0.12865559\n",
      "Iteration 68, loss = 0.12772290\n",
      "Iteration 69, loss = 0.12595904\n",
      "Iteration 70, loss = 0.12558107\n",
      "Iteration 71, loss = 0.12511022\n",
      "Iteration 72, loss = 0.12463153\n",
      "Iteration 73, loss = 0.12349932\n",
      "Iteration 74, loss = 0.12360631\n",
      "Iteration 75, loss = 0.12346391\n",
      "Iteration 76, loss = 0.12243166\n",
      "Iteration 77, loss = 0.12158274\n",
      "Iteration 78, loss = 0.12040569\n",
      "Iteration 79, loss = 0.11916232\n",
      "Iteration 80, loss = 0.12117703\n",
      "Iteration 81, loss = 0.11866389\n",
      "Iteration 82, loss = 0.11765341\n",
      "Iteration 83, loss = 0.11748000\n",
      "Iteration 84, loss = 0.11804290\n",
      "Iteration 85, loss = 0.11649995\n",
      "Iteration 86, loss = 0.11594722\n",
      "Iteration 87, loss = 0.11541189\n",
      "Iteration 88, loss = 0.11497729\n",
      "Iteration 89, loss = 0.11398550\n",
      "Iteration 90, loss = 0.11460425\n",
      "Iteration 91, loss = 0.11332116\n",
      "Iteration 92, loss = 0.11335277\n",
      "Iteration 93, loss = 0.11198592\n",
      "Iteration 94, loss = 0.11162032\n",
      "Iteration 95, loss = 0.11214682\n",
      "Iteration 96, loss = 0.11122425\n",
      "Iteration 97, loss = 0.11140464\n",
      "Iteration 98, loss = 0.10985026\n",
      "Iteration 99, loss = 0.10925814\n",
      "Iteration 100, loss = 0.10898448\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kileymack/opt/anaconda3/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 1.18105933\n",
      "Iteration 2, loss = 0.42112866\n",
      "Iteration 3, loss = 0.36483488\n",
      "Iteration 4, loss = 0.33166615\n",
      "Iteration 5, loss = 0.30880972\n",
      "Iteration 6, loss = 0.29089494\n",
      "Iteration 7, loss = 0.27649669\n",
      "Iteration 8, loss = 0.26455232\n",
      "Iteration 9, loss = 0.25445677\n",
      "Iteration 10, loss = 0.24582540\n",
      "Iteration 11, loss = 0.23804374\n",
      "Iteration 12, loss = 0.23104544\n",
      "Iteration 13, loss = 0.22546150\n",
      "Iteration 14, loss = 0.21970918\n",
      "Iteration 15, loss = 0.21408651\n",
      "Iteration 16, loss = 0.20933101\n",
      "Iteration 17, loss = 0.20430557\n",
      "Iteration 18, loss = 0.20089827\n",
      "Iteration 19, loss = 0.19646189\n",
      "Iteration 20, loss = 0.19375971\n",
      "Iteration 21, loss = 0.18993085\n",
      "Iteration 22, loss = 0.18676812\n",
      "Iteration 23, loss = 0.18374805\n",
      "Iteration 24, loss = 0.18126931\n",
      "Iteration 25, loss = 0.17857049\n",
      "Iteration 26, loss = 0.17653620\n",
      "Iteration 27, loss = 0.17357805\n",
      "Iteration 28, loss = 0.17142093\n",
      "Iteration 29, loss = 0.16886621\n",
      "Iteration 30, loss = 0.16712429\n",
      "Iteration 31, loss = 0.16511738\n",
      "Iteration 32, loss = 0.16302588\n",
      "Iteration 33, loss = 0.16102342\n",
      "Iteration 34, loss = 0.15907785\n",
      "Iteration 35, loss = 0.15707843\n",
      "Iteration 36, loss = 0.15574682\n",
      "Iteration 37, loss = 0.15385871\n",
      "Iteration 38, loss = 0.15248506\n",
      "Iteration 39, loss = 0.15115892\n",
      "Iteration 40, loss = 0.14983040\n",
      "Iteration 41, loss = 0.14796682\n",
      "Iteration 42, loss = 0.14684532\n",
      "Iteration 43, loss = 0.14558816\n",
      "Iteration 44, loss = 0.14403687\n",
      "Iteration 45, loss = 0.14239654\n",
      "Iteration 46, loss = 0.14141237\n",
      "Iteration 47, loss = 0.14026808\n",
      "Iteration 48, loss = 0.13915421\n",
      "Iteration 49, loss = 0.13819331\n",
      "Iteration 50, loss = 0.13710067\n",
      "Iteration 51, loss = 0.13594493\n",
      "Iteration 52, loss = 0.13461776\n",
      "Iteration 53, loss = 0.13403627\n",
      "Iteration 54, loss = 0.13249313\n",
      "Iteration 55, loss = 0.13177895\n",
      "Iteration 56, loss = 0.13037836\n",
      "Iteration 57, loss = 0.12939024\n",
      "Iteration 58, loss = 0.12864977\n",
      "Iteration 59, loss = 0.12793250\n",
      "Iteration 60, loss = 0.12659073\n",
      "Iteration 61, loss = 0.12574432\n",
      "Iteration 62, loss = 0.12499077\n",
      "Iteration 63, loss = 0.12463465\n",
      "Iteration 64, loss = 0.12350785\n",
      "Iteration 65, loss = 0.12271521\n",
      "Iteration 66, loss = 0.12190912\n",
      "Iteration 67, loss = 0.12105701\n",
      "Iteration 68, loss = 0.12004038\n",
      "Iteration 69, loss = 0.11941497\n",
      "Iteration 70, loss = 0.11861874\n",
      "Iteration 71, loss = 0.11820188\n",
      "Iteration 72, loss = 0.11723044\n",
      "Iteration 73, loss = 0.11634512\n",
      "Iteration 74, loss = 0.11578905\n",
      "Iteration 75, loss = 0.11522691\n",
      "Iteration 76, loss = 0.11438532\n",
      "Iteration 77, loss = 0.11352666\n",
      "Iteration 78, loss = 0.11323904\n",
      "Iteration 79, loss = 0.11208109\n",
      "Iteration 80, loss = 0.11205429\n",
      "Iteration 81, loss = 0.11120927\n",
      "Iteration 82, loss = 0.11036984\n",
      "Iteration 83, loss = 0.11013549\n",
      "Iteration 84, loss = 0.10908400\n",
      "Iteration 85, loss = 0.10883019\n",
      "Iteration 86, loss = 0.10819511\n",
      "Iteration 87, loss = 0.10759605\n",
      "Iteration 88, loss = 0.10652232\n",
      "Iteration 89, loss = 0.10643424\n",
      "Iteration 90, loss = 0.10584851\n",
      "Iteration 91, loss = 0.10542608\n",
      "Iteration 92, loss = 0.10466555\n",
      "Iteration 93, loss = 0.10442939\n",
      "Iteration 94, loss = 0.10365984\n",
      "Iteration 95, loss = 0.10316269\n",
      "Iteration 96, loss = 0.10250505\n",
      "Iteration 97, loss = 0.10221863\n",
      "Iteration 98, loss = 0.10203616\n",
      "Iteration 99, loss = 0.10115041\n",
      "Iteration 100, loss = 0.10101427\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kileymack/opt/anaconda3/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 1.63053224\n",
      "Iteration 2, loss = 0.36800377\n",
      "Iteration 3, loss = 0.30434566\n",
      "Iteration 4, loss = 0.27091582\n",
      "Iteration 5, loss = 0.25337828\n",
      "Iteration 6, loss = 0.23906900\n",
      "Iteration 7, loss = 0.22113058\n",
      "Iteration 8, loss = 0.21162049\n",
      "Iteration 9, loss = 0.20897136\n",
      "Iteration 10, loss = 0.19671928\n",
      "Iteration 11, loss = 0.19026307\n",
      "Iteration 12, loss = 0.18284757\n",
      "Iteration 13, loss = 0.18075690\n",
      "Iteration 14, loss = 0.16857505\n",
      "Iteration 15, loss = 0.16717012\n",
      "Iteration 16, loss = 0.16492217\n",
      "Iteration 17, loss = 0.15860898\n",
      "Iteration 18, loss = 0.15514787\n",
      "Iteration 19, loss = 0.15292264\n",
      "Iteration 20, loss = 0.15226593\n",
      "Iteration 21, loss = 0.14724660\n",
      "Iteration 22, loss = 0.14374080\n",
      "Iteration 23, loss = 0.14543107\n",
      "Iteration 24, loss = 0.14263647\n",
      "Iteration 25, loss = 0.13485942\n",
      "Iteration 26, loss = 0.13589177\n",
      "Iteration 27, loss = 0.13230521\n",
      "Iteration 28, loss = 0.13171851\n",
      "Iteration 29, loss = 0.13151257\n",
      "Iteration 30, loss = 0.12932776\n",
      "Iteration 31, loss = 0.12812941\n",
      "Iteration 32, loss = 0.12924596\n",
      "Iteration 33, loss = 0.12591246\n",
      "Iteration 34, loss = 0.12169248\n",
      "Iteration 35, loss = 0.12310255\n",
      "Iteration 36, loss = 0.11892190\n",
      "Iteration 37, loss = 0.12145258\n",
      "Iteration 38, loss = 0.11675366\n",
      "Iteration 39, loss = 0.11689659\n",
      "Iteration 40, loss = 0.11529379\n",
      "Iteration 41, loss = 0.11947577\n",
      "Iteration 42, loss = 0.11417094\n",
      "Iteration 43, loss = 0.11215659\n",
      "Iteration 44, loss = 0.11159535\n",
      "Iteration 45, loss = 0.11408417\n",
      "Iteration 46, loss = 0.11238297\n",
      "Iteration 47, loss = 0.10986600\n",
      "Iteration 48, loss = 0.10939612\n",
      "Iteration 49, loss = 0.10998866\n",
      "Iteration 50, loss = 0.10851730\n",
      "Iteration 51, loss = 0.10876386\n",
      "Iteration 52, loss = 0.10794998\n",
      "Iteration 53, loss = 0.10570979\n",
      "Iteration 54, loss = 0.10603867\n",
      "Iteration 55, loss = 0.10659256\n",
      "Iteration 56, loss = 0.10679533\n",
      "Iteration 57, loss = 0.10365312\n",
      "Iteration 58, loss = 0.10585303\n",
      "Iteration 59, loss = 0.10391812\n",
      "Iteration 60, loss = 0.10437027\n",
      "Iteration 61, loss = 0.10754719\n",
      "Iteration 62, loss = 0.10172431\n",
      "Iteration 63, loss = 0.10190850\n",
      "Iteration 64, loss = 0.10105322\n",
      "Iteration 65, loss = 0.10209199\n",
      "Iteration 66, loss = 0.10188718\n",
      "Iteration 67, loss = 0.10248065\n",
      "Iteration 68, loss = 0.10146889\n",
      "Iteration 69, loss = 0.10061162\n",
      "Iteration 70, loss = 0.10042604\n",
      "Iteration 71, loss = 0.09900922\n",
      "Iteration 72, loss = 0.10084689\n",
      "Iteration 73, loss = 0.09769134\n",
      "Iteration 74, loss = 0.09943000\n",
      "Iteration 75, loss = 0.09715785\n",
      "Iteration 76, loss = 0.09746215\n",
      "Iteration 77, loss = 0.09640787\n",
      "Iteration 78, loss = 0.09902508\n",
      "Iteration 79, loss = 0.09713241\n",
      "Iteration 80, loss = 0.09678748\n",
      "Iteration 81, loss = 0.09786190\n",
      "Iteration 82, loss = 0.09402564\n",
      "Iteration 83, loss = 0.09763573\n",
      "Iteration 84, loss = 0.09724111\n",
      "Iteration 85, loss = 0.09582293\n",
      "Iteration 86, loss = 0.09474450\n",
      "Iteration 87, loss = 0.09562231\n",
      "Iteration 88, loss = 0.09590602\n",
      "Iteration 89, loss = 0.09354726\n",
      "Iteration 90, loss = 0.09247104\n",
      "Iteration 91, loss = 0.09694007\n",
      "Iteration 92, loss = 0.09376177\n",
      "Iteration 93, loss = 0.09533974\n",
      "Iteration 94, loss = 0.09166855\n",
      "Iteration 95, loss = 0.09347854\n",
      "Iteration 96, loss = 0.09462972\n",
      "Iteration 97, loss = 0.09182904\n",
      "Iteration 98, loss = 0.09143512\n",
      "Iteration 99, loss = 0.09172267\n",
      "Iteration 100, loss = 0.09291215\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kileymack/opt/anaconda3/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 1.74118647\n",
      "Iteration 2, loss = 0.37346328\n",
      "Iteration 3, loss = 0.31744032\n",
      "Iteration 4, loss = 0.28624224\n",
      "Iteration 5, loss = 0.26069538\n",
      "Iteration 6, loss = 0.24696759\n",
      "Iteration 7, loss = 0.23067661\n",
      "Iteration 8, loss = 0.21830264\n",
      "Iteration 9, loss = 0.20815838\n",
      "Iteration 10, loss = 0.20184503\n",
      "Iteration 11, loss = 0.19484426\n",
      "Iteration 12, loss = 0.18572701\n",
      "Iteration 13, loss = 0.18070792\n",
      "Iteration 14, loss = 0.17683941\n",
      "Iteration 15, loss = 0.17122998\n",
      "Iteration 16, loss = 0.16639477\n",
      "Iteration 17, loss = 0.16410691\n",
      "Iteration 18, loss = 0.16003912\n",
      "Iteration 19, loss = 0.15776427\n",
      "Iteration 20, loss = 0.15466355\n",
      "Iteration 21, loss = 0.14972406\n",
      "Iteration 22, loss = 0.14974723\n",
      "Iteration 23, loss = 0.14842463\n",
      "Iteration 24, loss = 0.14525655\n",
      "Iteration 25, loss = 0.14281596\n",
      "Iteration 26, loss = 0.14151314\n",
      "Iteration 27, loss = 0.13833748\n",
      "Iteration 28, loss = 0.13796811\n",
      "Iteration 29, loss = 0.13654175\n",
      "Iteration 30, loss = 0.13419818\n",
      "Iteration 31, loss = 0.13373789\n",
      "Iteration 32, loss = 0.13105485\n",
      "Iteration 33, loss = 0.13044854\n",
      "Iteration 34, loss = 0.13042268\n",
      "Iteration 35, loss = 0.12912243\n",
      "Iteration 36, loss = 0.12601125\n",
      "Iteration 37, loss = 0.12606218\n",
      "Iteration 38, loss = 0.12570914\n",
      "Iteration 39, loss = 0.12559008\n",
      "Iteration 40, loss = 0.12430057\n",
      "Iteration 41, loss = 0.12406255\n",
      "Iteration 42, loss = 0.12311675\n",
      "Iteration 43, loss = 0.12077518\n",
      "Iteration 44, loss = 0.12096347\n",
      "Iteration 45, loss = 0.12112998\n",
      "Iteration 46, loss = 0.11943321\n",
      "Iteration 47, loss = 0.11866679\n",
      "Iteration 48, loss = 0.11907931\n",
      "Iteration 49, loss = 0.11663937\n",
      "Iteration 50, loss = 0.11882034\n",
      "Iteration 51, loss = 0.11638867\n",
      "Iteration 52, loss = 0.11698529\n",
      "Iteration 53, loss = 0.11732050\n",
      "Iteration 54, loss = 0.11466869\n",
      "Iteration 55, loss = 0.11505154\n",
      "Iteration 56, loss = 0.11426684\n",
      "Iteration 57, loss = 0.11395603\n",
      "Iteration 58, loss = 0.11329393\n",
      "Iteration 59, loss = 0.11356173\n",
      "Iteration 60, loss = 0.11355365\n",
      "Iteration 61, loss = 0.11027543\n",
      "Iteration 62, loss = 0.11199928\n",
      "Iteration 63, loss = 0.11160752\n",
      "Iteration 64, loss = 0.11243251\n",
      "Iteration 65, loss = 0.10992905\n",
      "Iteration 66, loss = 0.11063020\n",
      "Iteration 67, loss = 0.11077747\n",
      "Iteration 68, loss = 0.11008915\n",
      "Iteration 69, loss = 0.11085177\n",
      "Iteration 70, loss = 0.10893044\n",
      "Iteration 71, loss = 0.10957467\n",
      "Iteration 72, loss = 0.10766492\n",
      "Iteration 73, loss = 0.10862409\n",
      "Iteration 74, loss = 0.10974297\n",
      "Iteration 75, loss = 0.10740849\n",
      "Iteration 76, loss = 0.10826660\n",
      "Iteration 77, loss = 0.10768752\n",
      "Iteration 78, loss = 0.10719273\n",
      "Iteration 79, loss = 0.10666771\n",
      "Iteration 80, loss = 0.10662075\n",
      "Iteration 81, loss = 0.10568341\n",
      "Iteration 82, loss = 0.10727333\n",
      "Iteration 83, loss = 0.10666030\n",
      "Iteration 84, loss = 0.10575713\n",
      "Iteration 85, loss = 0.10574702\n",
      "Iteration 86, loss = 0.10524766\n",
      "Iteration 87, loss = 0.10563374\n",
      "Iteration 88, loss = 0.10685063\n",
      "Iteration 89, loss = 0.10493245\n",
      "Iteration 90, loss = 0.10313256\n",
      "Iteration 91, loss = 0.10419775\n",
      "Iteration 92, loss = 0.10503940\n",
      "Iteration 93, loss = 0.10379157\n",
      "Iteration 94, loss = 0.10329629\n",
      "Iteration 95, loss = 0.10345229\n",
      "Iteration 96, loss = 0.10436617\n",
      "Iteration 97, loss = 0.10360320\n",
      "Iteration 98, loss = 0.10507953\n",
      "Iteration 99, loss = 0.10351619\n",
      "Iteration 100, loss = 0.10446143\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kileymack/opt/anaconda3/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 1.64322664\n",
      "Iteration 2, loss = 0.44746643\n",
      "Iteration 3, loss = 0.37450110\n",
      "Iteration 4, loss = 0.33467731\n",
      "Iteration 5, loss = 0.30922241\n",
      "Iteration 6, loss = 0.28992445\n",
      "Iteration 7, loss = 0.27602028\n",
      "Iteration 8, loss = 0.26471115\n",
      "Iteration 9, loss = 0.25566856\n",
      "Iteration 10, loss = 0.24799532\n",
      "Iteration 11, loss = 0.24082625\n",
      "Iteration 12, loss = 0.23490535\n",
      "Iteration 13, loss = 0.22912135\n",
      "Iteration 14, loss = 0.22442114\n",
      "Iteration 15, loss = 0.22076153\n",
      "Iteration 16, loss = 0.21502665\n",
      "Iteration 17, loss = 0.21174468\n",
      "Iteration 18, loss = 0.20783571\n",
      "Iteration 19, loss = 0.20421765\n",
      "Iteration 20, loss = 0.20075690\n",
      "Iteration 21, loss = 0.19766248\n",
      "Iteration 22, loss = 0.19496926\n",
      "Iteration 23, loss = 0.19175108\n",
      "Iteration 24, loss = 0.18949907\n",
      "Iteration 25, loss = 0.18666581\n",
      "Iteration 26, loss = 0.18351921\n",
      "Iteration 27, loss = 0.18155440\n",
      "Iteration 28, loss = 0.18001129\n",
      "Iteration 29, loss = 0.17754644\n",
      "Iteration 30, loss = 0.17528988\n",
      "Iteration 31, loss = 0.17319418\n",
      "Iteration 32, loss = 0.17103927\n",
      "Iteration 33, loss = 0.16929301\n",
      "Iteration 34, loss = 0.16739986\n",
      "Iteration 35, loss = 0.16578732\n",
      "Iteration 36, loss = 0.16367016\n",
      "Iteration 37, loss = 0.16177293\n",
      "Iteration 38, loss = 0.16032338\n",
      "Iteration 39, loss = 0.15863906\n",
      "Iteration 40, loss = 0.15746635\n",
      "Iteration 41, loss = 0.15599692\n",
      "Iteration 42, loss = 0.15416761\n",
      "Iteration 43, loss = 0.15303841\n",
      "Iteration 44, loss = 0.15180767\n",
      "Iteration 45, loss = 0.15095752\n",
      "Iteration 46, loss = 0.14922837\n",
      "Iteration 47, loss = 0.14788922\n",
      "Iteration 48, loss = 0.14664290\n",
      "Iteration 49, loss = 0.14557348\n",
      "Iteration 50, loss = 0.14448091\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kileymack/opt/anaconda3/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (50) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 1.26619637\n",
      "Iteration 2, loss = 0.43010658\n",
      "Iteration 3, loss = 0.37237972\n",
      "Iteration 4, loss = 0.33948552\n",
      "Iteration 5, loss = 0.31537574\n",
      "Iteration 6, loss = 0.29737138\n",
      "Iteration 7, loss = 0.28240031\n",
      "Iteration 8, loss = 0.27019941\n",
      "Iteration 9, loss = 0.25956831\n",
      "Iteration 10, loss = 0.25018250\n",
      "Iteration 11, loss = 0.24255834\n",
      "Iteration 12, loss = 0.23539079\n",
      "Iteration 13, loss = 0.22892956\n",
      "Iteration 14, loss = 0.22345561\n",
      "Iteration 15, loss = 0.21762290\n",
      "Iteration 16, loss = 0.21276581\n",
      "Iteration 17, loss = 0.20813587\n",
      "Iteration 18, loss = 0.20395972\n",
      "Iteration 19, loss = 0.19969025\n",
      "Iteration 20, loss = 0.19625815\n",
      "Iteration 21, loss = 0.19291887\n",
      "Iteration 22, loss = 0.18952097\n",
      "Iteration 23, loss = 0.18642931\n",
      "Iteration 24, loss = 0.18348038\n",
      "Iteration 25, loss = 0.18083983\n",
      "Iteration 26, loss = 0.17845766\n",
      "Iteration 27, loss = 0.17581545\n",
      "Iteration 28, loss = 0.17335249\n",
      "Iteration 29, loss = 0.17094619\n",
      "Iteration 30, loss = 0.16887728\n",
      "Iteration 31, loss = 0.16677904\n",
      "Iteration 32, loss = 0.16457697\n",
      "Iteration 33, loss = 0.16290385\n",
      "Iteration 34, loss = 0.16092416\n",
      "Iteration 35, loss = 0.15897972\n",
      "Iteration 36, loss = 0.15742261\n",
      "Iteration 37, loss = 0.15612651\n",
      "Iteration 38, loss = 0.15438482\n",
      "Iteration 39, loss = 0.15301872\n",
      "Iteration 40, loss = 0.15132209\n",
      "Iteration 41, loss = 0.14963545\n",
      "Iteration 42, loss = 0.14847591\n",
      "Iteration 43, loss = 0.14732351\n",
      "Iteration 44, loss = 0.14538429\n",
      "Iteration 45, loss = 0.14413536\n",
      "Iteration 46, loss = 0.14282435\n",
      "Iteration 47, loss = 0.14169314\n",
      "Iteration 48, loss = 0.14048913\n",
      "Iteration 49, loss = 0.13948997\n",
      "Iteration 50, loss = 0.13819137\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kileymack/opt/anaconda3/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (50) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 1.90801609\n",
      "Iteration 2, loss = 0.37494441\n",
      "Iteration 3, loss = 0.30816777\n",
      "Iteration 4, loss = 0.27855064\n",
      "Iteration 5, loss = 0.24904203\n",
      "Iteration 6, loss = 0.23433588\n",
      "Iteration 7, loss = 0.22536205\n",
      "Iteration 8, loss = 0.21173230\n",
      "Iteration 9, loss = 0.20416988\n",
      "Iteration 10, loss = 0.19667186\n",
      "Iteration 11, loss = 0.19017231\n",
      "Iteration 12, loss = 0.18234959\n",
      "Iteration 13, loss = 0.17893628\n",
      "Iteration 14, loss = 0.17316408\n",
      "Iteration 15, loss = 0.16880705\n",
      "Iteration 16, loss = 0.16214421\n",
      "Iteration 17, loss = 0.16283087\n",
      "Iteration 18, loss = 0.15724344\n",
      "Iteration 19, loss = 0.15450249\n",
      "Iteration 20, loss = 0.15072334\n",
      "Iteration 21, loss = 0.14853088\n",
      "Iteration 22, loss = 0.14708482\n",
      "Iteration 23, loss = 0.14538225\n",
      "Iteration 24, loss = 0.14344123\n",
      "Iteration 25, loss = 0.14005907\n",
      "Iteration 26, loss = 0.13677439\n",
      "Iteration 27, loss = 0.13779788\n",
      "Iteration 28, loss = 0.13575139\n",
      "Iteration 29, loss = 0.13447395\n",
      "Iteration 30, loss = 0.13154300\n",
      "Iteration 31, loss = 0.12980352\n",
      "Iteration 32, loss = 0.12768152\n",
      "Iteration 33, loss = 0.12947904\n",
      "Iteration 34, loss = 0.12541091\n",
      "Iteration 35, loss = 0.12542418\n",
      "Iteration 36, loss = 0.12206853\n",
      "Iteration 37, loss = 0.12200497\n",
      "Iteration 38, loss = 0.12215906\n",
      "Iteration 39, loss = 0.12052731\n",
      "Iteration 40, loss = 0.12248923\n",
      "Iteration 41, loss = 0.11889548\n",
      "Iteration 42, loss = 0.11747579\n",
      "Iteration 43, loss = 0.11566241\n",
      "Iteration 44, loss = 0.11687125\n",
      "Iteration 45, loss = 0.11501880\n",
      "Iteration 46, loss = 0.11625048\n",
      "Iteration 47, loss = 0.11454541\n",
      "Iteration 48, loss = 0.11273017\n",
      "Iteration 49, loss = 0.11163151\n",
      "Iteration 50, loss = 0.11476994\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kileymack/opt/anaconda3/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (50) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 1.98476072\n",
      "Iteration 2, loss = 0.38426120\n",
      "Iteration 3, loss = 0.31914079\n",
      "Iteration 4, loss = 0.28544621\n",
      "Iteration 5, loss = 0.25674064\n",
      "Iteration 6, loss = 0.23719356\n",
      "Iteration 7, loss = 0.22404787\n",
      "Iteration 8, loss = 0.21262287\n",
      "Iteration 9, loss = 0.20459224\n",
      "Iteration 10, loss = 0.20039339\n",
      "Iteration 11, loss = 0.18981523\n",
      "Iteration 12, loss = 0.18702486\n",
      "Iteration 13, loss = 0.17995987\n",
      "Iteration 14, loss = 0.17523674\n",
      "Iteration 15, loss = 0.17249480\n",
      "Iteration 16, loss = 0.16759119\n",
      "Iteration 17, loss = 0.16440300\n",
      "Iteration 18, loss = 0.16031837\n",
      "Iteration 19, loss = 0.15861235\n",
      "Iteration 20, loss = 0.15683644\n",
      "Iteration 21, loss = 0.15214493\n",
      "Iteration 22, loss = 0.15036911\n",
      "Iteration 23, loss = 0.14791825\n",
      "Iteration 24, loss = 0.14850451\n",
      "Iteration 25, loss = 0.14411924\n",
      "Iteration 26, loss = 0.14393850\n",
      "Iteration 27, loss = 0.14085959\n",
      "Iteration 28, loss = 0.14012863\n",
      "Iteration 29, loss = 0.13780677\n",
      "Iteration 30, loss = 0.13793248\n",
      "Iteration 31, loss = 0.13483363\n",
      "Iteration 32, loss = 0.13366160\n",
      "Iteration 33, loss = 0.13386033\n",
      "Iteration 34, loss = 0.13082299\n",
      "Iteration 35, loss = 0.13129747\n",
      "Iteration 36, loss = 0.12982309\n",
      "Iteration 37, loss = 0.13043302\n",
      "Iteration 38, loss = 0.12802628\n",
      "Iteration 39, loss = 0.12681994\n",
      "Iteration 40, loss = 0.12644513\n",
      "Iteration 41, loss = 0.12611327\n",
      "Iteration 42, loss = 0.12448570\n",
      "Iteration 43, loss = 0.12625916\n",
      "Iteration 44, loss = 0.12438938\n",
      "Iteration 45, loss = 0.12255480\n",
      "Iteration 46, loss = 0.12286736\n",
      "Iteration 47, loss = 0.12111008\n",
      "Iteration 48, loss = 0.12112961\n",
      "Iteration 49, loss = 0.12021411\n",
      "Iteration 50, loss = 0.12009079\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kileymack/opt/anaconda3/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (50) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 1.64322664\n",
      "Iteration 2, loss = 0.44746643\n",
      "Iteration 3, loss = 0.37450110\n",
      "Iteration 4, loss = 0.33467731\n",
      "Iteration 5, loss = 0.30922241\n",
      "Iteration 6, loss = 0.28992445\n",
      "Iteration 7, loss = 0.27602028\n",
      "Iteration 8, loss = 0.26471115\n",
      "Iteration 9, loss = 0.25566856\n",
      "Iteration 10, loss = 0.24799532\n",
      "Iteration 11, loss = 0.24082625\n",
      "Iteration 12, loss = 0.23490535\n",
      "Iteration 13, loss = 0.22912135\n",
      "Iteration 14, loss = 0.22442114\n",
      "Iteration 15, loss = 0.22076153\n",
      "Iteration 16, loss = 0.21502665\n",
      "Iteration 17, loss = 0.21174468\n",
      "Iteration 18, loss = 0.20783571\n",
      "Iteration 19, loss = 0.20421765\n",
      "Iteration 20, loss = 0.20075690\n",
      "Iteration 21, loss = 0.19766248\n",
      "Iteration 22, loss = 0.19496926\n",
      "Iteration 23, loss = 0.19175108\n",
      "Iteration 24, loss = 0.18949907\n",
      "Iteration 25, loss = 0.18666581\n",
      "Iteration 26, loss = 0.18351921\n",
      "Iteration 27, loss = 0.18155440\n",
      "Iteration 28, loss = 0.18001129\n",
      "Iteration 29, loss = 0.17754644\n",
      "Iteration 30, loss = 0.17528988\n",
      "Iteration 31, loss = 0.17319418\n",
      "Iteration 32, loss = 0.17103927\n",
      "Iteration 33, loss = 0.16929301\n",
      "Iteration 34, loss = 0.16739986\n",
      "Iteration 35, loss = 0.16578732\n",
      "Iteration 36, loss = 0.16367016\n",
      "Iteration 37, loss = 0.16177293\n",
      "Iteration 38, loss = 0.16032338\n",
      "Iteration 39, loss = 0.15863906\n",
      "Iteration 40, loss = 0.15746635\n",
      "Iteration 41, loss = 0.15599692\n",
      "Iteration 42, loss = 0.15416761\n",
      "Iteration 43, loss = 0.15303841\n",
      "Iteration 44, loss = 0.15180767\n",
      "Iteration 45, loss = 0.15095752\n",
      "Iteration 46, loss = 0.14922837\n",
      "Iteration 47, loss = 0.14788922\n",
      "Iteration 48, loss = 0.14664290\n",
      "Iteration 49, loss = 0.14557348\n",
      "Iteration 50, loss = 0.14448091\n",
      "Iteration 51, loss = 0.14302138\n",
      "Iteration 52, loss = 0.14176147\n",
      "Iteration 53, loss = 0.14105746\n",
      "Iteration 54, loss = 0.13990190\n",
      "Iteration 55, loss = 0.13894293\n",
      "Iteration 56, loss = 0.13745161\n",
      "Iteration 57, loss = 0.13658078\n",
      "Iteration 58, loss = 0.13590397\n",
      "Iteration 59, loss = 0.13510442\n",
      "Iteration 60, loss = 0.13392564\n",
      "Iteration 61, loss = 0.13276611\n",
      "Iteration 62, loss = 0.13217636\n",
      "Iteration 63, loss = 0.13115416\n",
      "Iteration 64, loss = 0.13055297\n",
      "Iteration 65, loss = 0.12902752\n",
      "Iteration 66, loss = 0.12822503\n",
      "Iteration 67, loss = 0.12759875\n",
      "Iteration 68, loss = 0.12737123\n",
      "Iteration 69, loss = 0.12646941\n",
      "Iteration 70, loss = 0.12579975\n",
      "Iteration 71, loss = 0.12488459\n",
      "Iteration 72, loss = 0.12400456\n",
      "Iteration 73, loss = 0.12320072\n",
      "Iteration 74, loss = 0.12232627\n",
      "Iteration 75, loss = 0.12218563\n",
      "Iteration 76, loss = 0.12162755\n",
      "Iteration 77, loss = 0.12049422\n",
      "Iteration 78, loss = 0.12040819\n",
      "Iteration 79, loss = 0.11933545\n",
      "Iteration 80, loss = 0.11858780\n",
      "Iteration 81, loss = 0.11806873\n",
      "Iteration 82, loss = 0.11755653\n",
      "Iteration 83, loss = 0.11724612\n",
      "Iteration 84, loss = 0.11629744\n",
      "Iteration 85, loss = 0.11554210\n",
      "Iteration 86, loss = 0.11517589\n",
      "Iteration 87, loss = 0.11467441\n",
      "Iteration 88, loss = 0.11404625\n",
      "Iteration 89, loss = 0.11288524\n",
      "Iteration 90, loss = 0.11297298\n",
      "Iteration 91, loss = 0.11266455\n",
      "Iteration 92, loss = 0.11246941\n",
      "Iteration 93, loss = 0.11154873\n",
      "Iteration 94, loss = 0.11100713\n",
      "Iteration 95, loss = 0.11046631\n",
      "Iteration 96, loss = 0.11033158\n",
      "Iteration 97, loss = 0.10940870\n",
      "Iteration 98, loss = 0.10910531\n",
      "Iteration 99, loss = 0.10864226\n",
      "Iteration 100, loss = 0.10808699\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kileymack/opt/anaconda3/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 1.26619637\n",
      "Iteration 2, loss = 0.43010658\n",
      "Iteration 3, loss = 0.37237972\n",
      "Iteration 4, loss = 0.33948552\n",
      "Iteration 5, loss = 0.31537574\n",
      "Iteration 6, loss = 0.29737138\n",
      "Iteration 7, loss = 0.28240031\n",
      "Iteration 8, loss = 0.27019941\n",
      "Iteration 9, loss = 0.25956831\n",
      "Iteration 10, loss = 0.25018250\n",
      "Iteration 11, loss = 0.24255834\n",
      "Iteration 12, loss = 0.23539079\n",
      "Iteration 13, loss = 0.22892956\n",
      "Iteration 14, loss = 0.22345561\n",
      "Iteration 15, loss = 0.21762290\n",
      "Iteration 16, loss = 0.21276581\n",
      "Iteration 17, loss = 0.20813587\n",
      "Iteration 18, loss = 0.20395972\n",
      "Iteration 19, loss = 0.19969025\n",
      "Iteration 20, loss = 0.19625815\n",
      "Iteration 21, loss = 0.19291887\n",
      "Iteration 22, loss = 0.18952097\n",
      "Iteration 23, loss = 0.18642931\n",
      "Iteration 24, loss = 0.18348038\n",
      "Iteration 25, loss = 0.18083983\n",
      "Iteration 26, loss = 0.17845766\n",
      "Iteration 27, loss = 0.17581545\n",
      "Iteration 28, loss = 0.17335249\n",
      "Iteration 29, loss = 0.17094619\n",
      "Iteration 30, loss = 0.16887728\n",
      "Iteration 31, loss = 0.16677904\n",
      "Iteration 32, loss = 0.16457697\n",
      "Iteration 33, loss = 0.16290385\n",
      "Iteration 34, loss = 0.16092416\n",
      "Iteration 35, loss = 0.15897972\n",
      "Iteration 36, loss = 0.15742261\n",
      "Iteration 37, loss = 0.15612651\n",
      "Iteration 38, loss = 0.15438482\n",
      "Iteration 39, loss = 0.15301872\n",
      "Iteration 40, loss = 0.15132209\n",
      "Iteration 41, loss = 0.14963545\n",
      "Iteration 42, loss = 0.14847591\n",
      "Iteration 43, loss = 0.14732351\n",
      "Iteration 44, loss = 0.14538429\n",
      "Iteration 45, loss = 0.14413536\n",
      "Iteration 46, loss = 0.14282435\n",
      "Iteration 47, loss = 0.14169314\n",
      "Iteration 48, loss = 0.14048913\n",
      "Iteration 49, loss = 0.13948997\n",
      "Iteration 50, loss = 0.13819137\n",
      "Iteration 51, loss = 0.13718053\n",
      "Iteration 52, loss = 0.13605140\n",
      "Iteration 53, loss = 0.13498371\n",
      "Iteration 54, loss = 0.13406271\n",
      "Iteration 55, loss = 0.13303224\n",
      "Iteration 56, loss = 0.13215417\n",
      "Iteration 57, loss = 0.13102389\n",
      "Iteration 58, loss = 0.13033154\n",
      "Iteration 59, loss = 0.12932909\n",
      "Iteration 60, loss = 0.12809987\n",
      "Iteration 61, loss = 0.12716237\n",
      "Iteration 62, loss = 0.12648895\n",
      "Iteration 63, loss = 0.12550313\n",
      "Iteration 64, loss = 0.12470549\n",
      "Iteration 65, loss = 0.12399793\n",
      "Iteration 66, loss = 0.12327571\n",
      "Iteration 67, loss = 0.12231244\n",
      "Iteration 68, loss = 0.12172784\n",
      "Iteration 69, loss = 0.12103439\n",
      "Iteration 70, loss = 0.12013207\n",
      "Iteration 71, loss = 0.11939127\n",
      "Iteration 72, loss = 0.11890658\n",
      "Iteration 73, loss = 0.11806159\n",
      "Iteration 74, loss = 0.11709756\n",
      "Iteration 75, loss = 0.11689132\n",
      "Iteration 76, loss = 0.11594818\n",
      "Iteration 77, loss = 0.11570395\n",
      "Iteration 78, loss = 0.11463737\n",
      "Iteration 79, loss = 0.11429667\n",
      "Iteration 80, loss = 0.11370933\n",
      "Iteration 81, loss = 0.11295103\n",
      "Iteration 82, loss = 0.11239042\n",
      "Iteration 83, loss = 0.11182893\n",
      "Iteration 84, loss = 0.11139229\n",
      "Iteration 85, loss = 0.11079560\n",
      "Iteration 86, loss = 0.11008215\n",
      "Iteration 87, loss = 0.10939575\n",
      "Iteration 88, loss = 0.10910441\n",
      "Iteration 89, loss = 0.10870534\n",
      "Iteration 90, loss = 0.10862158\n",
      "Iteration 91, loss = 0.10757372\n",
      "Iteration 92, loss = 0.10698764\n",
      "Iteration 93, loss = 0.10640478\n",
      "Iteration 94, loss = 0.10585966\n",
      "Iteration 95, loss = 0.10560933\n",
      "Iteration 96, loss = 0.10506565\n",
      "Iteration 97, loss = 0.10436416\n",
      "Iteration 98, loss = 0.10436289\n",
      "Iteration 99, loss = 0.10358135\n",
      "Iteration 100, loss = 0.10282363\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kileymack/opt/anaconda3/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 1.90801609\n",
      "Iteration 2, loss = 0.37494441\n",
      "Iteration 3, loss = 0.30816777\n",
      "Iteration 4, loss = 0.27855064\n",
      "Iteration 5, loss = 0.24904203\n",
      "Iteration 6, loss = 0.23433588\n",
      "Iteration 7, loss = 0.22536205\n",
      "Iteration 8, loss = 0.21173230\n",
      "Iteration 9, loss = 0.20416988\n",
      "Iteration 10, loss = 0.19667186\n",
      "Iteration 11, loss = 0.19017231\n",
      "Iteration 12, loss = 0.18234959\n",
      "Iteration 13, loss = 0.17893628\n",
      "Iteration 14, loss = 0.17316408\n",
      "Iteration 15, loss = 0.16880705\n",
      "Iteration 16, loss = 0.16214421\n",
      "Iteration 17, loss = 0.16283087\n",
      "Iteration 18, loss = 0.15724344\n",
      "Iteration 19, loss = 0.15450249\n",
      "Iteration 20, loss = 0.15072334\n",
      "Iteration 21, loss = 0.14853088\n",
      "Iteration 22, loss = 0.14708482\n",
      "Iteration 23, loss = 0.14538225\n",
      "Iteration 24, loss = 0.14344123\n",
      "Iteration 25, loss = 0.14005907\n",
      "Iteration 26, loss = 0.13677439\n",
      "Iteration 27, loss = 0.13779788\n",
      "Iteration 28, loss = 0.13575139\n",
      "Iteration 29, loss = 0.13447395\n",
      "Iteration 30, loss = 0.13154300\n",
      "Iteration 31, loss = 0.12980352\n",
      "Iteration 32, loss = 0.12768152\n",
      "Iteration 33, loss = 0.12947904\n",
      "Iteration 34, loss = 0.12541091\n",
      "Iteration 35, loss = 0.12542418\n",
      "Iteration 36, loss = 0.12206853\n",
      "Iteration 37, loss = 0.12200497\n",
      "Iteration 38, loss = 0.12215906\n",
      "Iteration 39, loss = 0.12052731\n",
      "Iteration 40, loss = 0.12248923\n",
      "Iteration 41, loss = 0.11889548\n",
      "Iteration 42, loss = 0.11747579\n",
      "Iteration 43, loss = 0.11566241\n",
      "Iteration 44, loss = 0.11687125\n",
      "Iteration 45, loss = 0.11501880\n",
      "Iteration 46, loss = 0.11625048\n",
      "Iteration 47, loss = 0.11454541\n",
      "Iteration 48, loss = 0.11273017\n",
      "Iteration 49, loss = 0.11163151\n",
      "Iteration 50, loss = 0.11476994\n",
      "Iteration 51, loss = 0.10896834\n",
      "Iteration 52, loss = 0.10951608\n",
      "Iteration 53, loss = 0.11252522\n",
      "Iteration 54, loss = 0.10893265\n",
      "Iteration 55, loss = 0.10812136\n",
      "Iteration 56, loss = 0.10753050\n",
      "Iteration 57, loss = 0.10656736\n",
      "Iteration 58, loss = 0.10759224\n",
      "Iteration 59, loss = 0.10728891\n",
      "Iteration 60, loss = 0.10667561\n",
      "Iteration 61, loss = 0.10371165\n",
      "Iteration 62, loss = 0.10504883\n",
      "Iteration 63, loss = 0.10482636\n",
      "Iteration 64, loss = 0.10583068\n",
      "Iteration 65, loss = 0.10261932\n",
      "Iteration 66, loss = 0.10238438\n",
      "Iteration 67, loss = 0.10434651\n",
      "Iteration 68, loss = 0.10276170\n",
      "Iteration 69, loss = 0.10488954\n",
      "Iteration 70, loss = 0.10252818\n",
      "Iteration 71, loss = 0.10182407\n",
      "Iteration 72, loss = 0.10252680\n",
      "Iteration 73, loss = 0.10008744\n",
      "Iteration 74, loss = 0.10115802\n",
      "Iteration 75, loss = 0.10185894\n",
      "Iteration 76, loss = 0.10075259\n",
      "Iteration 77, loss = 0.10171722\n",
      "Iteration 78, loss = 0.10054277\n",
      "Iteration 79, loss = 0.09808049\n",
      "Iteration 80, loss = 0.09834304\n",
      "Iteration 81, loss = 0.09930483\n",
      "Iteration 82, loss = 0.09910096\n",
      "Iteration 83, loss = 0.09796687\n",
      "Iteration 84, loss = 0.09645470\n",
      "Iteration 85, loss = 0.09743753\n",
      "Iteration 86, loss = 0.09818368\n",
      "Iteration 87, loss = 0.09512213\n",
      "Iteration 88, loss = 0.10061014\n",
      "Iteration 89, loss = 0.09562873\n",
      "Iteration 90, loss = 0.09669678\n",
      "Iteration 91, loss = 0.09593262\n",
      "Iteration 92, loss = 0.09865441\n",
      "Iteration 93, loss = 0.09567753\n",
      "Iteration 94, loss = 0.09506909\n",
      "Iteration 95, loss = 0.09571548\n",
      "Iteration 96, loss = 0.09430691\n",
      "Iteration 97, loss = 0.09460998\n",
      "Iteration 98, loss = 0.09402299\n",
      "Iteration 99, loss = 0.09599777\n",
      "Iteration 100, loss = 0.09456427\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kileymack/opt/anaconda3/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 1.98476072\n",
      "Iteration 2, loss = 0.38426120\n",
      "Iteration 3, loss = 0.31914079\n",
      "Iteration 4, loss = 0.28544621\n",
      "Iteration 5, loss = 0.25674064\n",
      "Iteration 6, loss = 0.23719356\n",
      "Iteration 7, loss = 0.22404787\n",
      "Iteration 8, loss = 0.21262287\n",
      "Iteration 9, loss = 0.20459224\n",
      "Iteration 10, loss = 0.20039339\n",
      "Iteration 11, loss = 0.18981523\n",
      "Iteration 12, loss = 0.18702486\n",
      "Iteration 13, loss = 0.17995987\n",
      "Iteration 14, loss = 0.17523674\n",
      "Iteration 15, loss = 0.17249480\n",
      "Iteration 16, loss = 0.16759119\n",
      "Iteration 17, loss = 0.16440300\n",
      "Iteration 18, loss = 0.16031837\n",
      "Iteration 19, loss = 0.15861235\n",
      "Iteration 20, loss = 0.15683644\n",
      "Iteration 21, loss = 0.15214493\n",
      "Iteration 22, loss = 0.15036911\n",
      "Iteration 23, loss = 0.14791825\n",
      "Iteration 24, loss = 0.14850451\n",
      "Iteration 25, loss = 0.14411924\n",
      "Iteration 26, loss = 0.14393850\n",
      "Iteration 27, loss = 0.14085959\n",
      "Iteration 28, loss = 0.14012863\n",
      "Iteration 29, loss = 0.13780677\n",
      "Iteration 30, loss = 0.13793248\n",
      "Iteration 31, loss = 0.13483363\n",
      "Iteration 32, loss = 0.13366160\n",
      "Iteration 33, loss = 0.13386033\n",
      "Iteration 34, loss = 0.13082299\n",
      "Iteration 35, loss = 0.13129747\n",
      "Iteration 36, loss = 0.12982309\n",
      "Iteration 37, loss = 0.13043302\n",
      "Iteration 38, loss = 0.12802628\n",
      "Iteration 39, loss = 0.12681994\n",
      "Iteration 40, loss = 0.12644513\n",
      "Iteration 41, loss = 0.12611327\n",
      "Iteration 42, loss = 0.12448570\n",
      "Iteration 43, loss = 0.12625916\n",
      "Iteration 44, loss = 0.12438938\n",
      "Iteration 45, loss = 0.12255480\n",
      "Iteration 46, loss = 0.12286736\n",
      "Iteration 47, loss = 0.12111008\n",
      "Iteration 48, loss = 0.12112961\n",
      "Iteration 49, loss = 0.12021411\n",
      "Iteration 50, loss = 0.12009079\n",
      "Iteration 51, loss = 0.12020951\n",
      "Iteration 52, loss = 0.11742806\n",
      "Iteration 53, loss = 0.11861445\n",
      "Iteration 54, loss = 0.11858688\n",
      "Iteration 55, loss = 0.11535820\n",
      "Iteration 56, loss = 0.11886686\n",
      "Iteration 57, loss = 0.11778581\n",
      "Iteration 58, loss = 0.11611208\n",
      "Iteration 59, loss = 0.11678855\n",
      "Iteration 60, loss = 0.11572783\n",
      "Iteration 61, loss = 0.11394884\n",
      "Iteration 62, loss = 0.11433674\n",
      "Iteration 63, loss = 0.11352874\n",
      "Iteration 64, loss = 0.11349246\n",
      "Iteration 65, loss = 0.11359046\n",
      "Iteration 66, loss = 0.11391299\n",
      "Iteration 67, loss = 0.11346332\n",
      "Iteration 68, loss = 0.11222118\n",
      "Iteration 69, loss = 0.11368919\n",
      "Iteration 70, loss = 0.11374283\n",
      "Iteration 71, loss = 0.11092536\n",
      "Iteration 72, loss = 0.11019194\n",
      "Iteration 73, loss = 0.11159372\n",
      "Iteration 74, loss = 0.11098454\n",
      "Iteration 75, loss = 0.11186504\n",
      "Iteration 76, loss = 0.11047805\n",
      "Iteration 77, loss = 0.11038625\n",
      "Iteration 78, loss = 0.10898690\n",
      "Iteration 79, loss = 0.10865193\n",
      "Iteration 80, loss = 0.10898945\n",
      "Iteration 81, loss = 0.10855803\n",
      "Iteration 82, loss = 0.10741799\n",
      "Iteration 83, loss = 0.10875423\n",
      "Iteration 84, loss = 0.10839775\n",
      "Iteration 85, loss = 0.10803744\n",
      "Iteration 86, loss = 0.10718458\n",
      "Iteration 87, loss = 0.10822601\n",
      "Iteration 88, loss = 0.10647723\n",
      "Iteration 89, loss = 0.10750565\n",
      "Iteration 90, loss = 0.10693157\n",
      "Iteration 91, loss = 0.10674938\n",
      "Iteration 92, loss = 0.10532782\n",
      "Iteration 93, loss = 0.10646073\n",
      "Iteration 94, loss = 0.10509963\n",
      "Iteration 95, loss = 0.10653921\n",
      "Iteration 96, loss = 0.10473669\n",
      "Iteration 97, loss = 0.10365089\n",
      "Iteration 98, loss = 0.10664144\n",
      "Iteration 99, loss = 0.10476883\n",
      "Iteration 100, loss = 0.10522463\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kileymack/opt/anaconda3/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 1.64322664\n",
      "Iteration 2, loss = 0.44746643\n",
      "Iteration 3, loss = 0.37450110\n",
      "Iteration 4, loss = 0.33467731\n",
      "Iteration 5, loss = 0.30922241\n",
      "Iteration 6, loss = 0.28992445\n",
      "Iteration 7, loss = 0.27602028\n",
      "Iteration 8, loss = 0.26471115\n",
      "Iteration 9, loss = 0.25566856\n",
      "Iteration 10, loss = 0.24799532\n",
      "Iteration 11, loss = 0.24082625\n",
      "Iteration 12, loss = 0.23490535\n",
      "Iteration 13, loss = 0.22912135\n",
      "Iteration 14, loss = 0.22442114\n",
      "Iteration 15, loss = 0.22076153\n",
      "Iteration 16, loss = 0.21502665\n",
      "Iteration 17, loss = 0.21174468\n",
      "Iteration 18, loss = 0.20783571\n",
      "Iteration 19, loss = 0.20421765\n",
      "Iteration 20, loss = 0.20075690\n",
      "Iteration 21, loss = 0.19766248\n",
      "Iteration 22, loss = 0.19496926\n",
      "Iteration 23, loss = 0.19175108\n",
      "Iteration 24, loss = 0.18949907\n",
      "Iteration 25, loss = 0.18666581\n",
      "Iteration 26, loss = 0.18351921\n",
      "Iteration 27, loss = 0.18155440\n",
      "Iteration 28, loss = 0.18001129\n",
      "Iteration 29, loss = 0.17754644\n",
      "Iteration 30, loss = 0.17528988\n",
      "Iteration 31, loss = 0.17319418\n",
      "Iteration 32, loss = 0.17103927\n",
      "Iteration 33, loss = 0.16929301\n",
      "Iteration 34, loss = 0.16739986\n",
      "Iteration 35, loss = 0.16578732\n",
      "Iteration 36, loss = 0.16367016\n",
      "Iteration 37, loss = 0.16177293\n",
      "Iteration 38, loss = 0.16032338\n",
      "Iteration 39, loss = 0.15863906\n",
      "Iteration 40, loss = 0.15746635\n",
      "Iteration 41, loss = 0.15599692\n",
      "Iteration 42, loss = 0.15416761\n",
      "Iteration 43, loss = 0.15303841\n",
      "Iteration 44, loss = 0.15180767\n",
      "Iteration 45, loss = 0.15095752\n",
      "Iteration 46, loss = 0.14922837\n",
      "Iteration 47, loss = 0.14788922\n",
      "Iteration 48, loss = 0.14664290\n",
      "Iteration 49, loss = 0.14557348\n",
      "Iteration 50, loss = 0.14448091\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kileymack/opt/anaconda3/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (50) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 1.26619637\n",
      "Iteration 2, loss = 0.43010658\n",
      "Iteration 3, loss = 0.37237972\n",
      "Iteration 4, loss = 0.33948552\n",
      "Iteration 5, loss = 0.31537574\n",
      "Iteration 6, loss = 0.29737138\n",
      "Iteration 7, loss = 0.28240031\n",
      "Iteration 8, loss = 0.27019941\n",
      "Iteration 9, loss = 0.25956831\n",
      "Iteration 10, loss = 0.25018250\n",
      "Iteration 11, loss = 0.24255834\n",
      "Iteration 12, loss = 0.23539079\n",
      "Iteration 13, loss = 0.22892956\n",
      "Iteration 14, loss = 0.22345561\n",
      "Iteration 15, loss = 0.21762290\n",
      "Iteration 16, loss = 0.21276581\n",
      "Iteration 17, loss = 0.20813587\n",
      "Iteration 18, loss = 0.20395972\n",
      "Iteration 19, loss = 0.19969025\n",
      "Iteration 20, loss = 0.19625815\n",
      "Iteration 21, loss = 0.19291887\n",
      "Iteration 22, loss = 0.18952097\n",
      "Iteration 23, loss = 0.18642931\n",
      "Iteration 24, loss = 0.18348038\n",
      "Iteration 25, loss = 0.18083983\n",
      "Iteration 26, loss = 0.17845766\n",
      "Iteration 27, loss = 0.17581545\n",
      "Iteration 28, loss = 0.17335249\n",
      "Iteration 29, loss = 0.17094619\n",
      "Iteration 30, loss = 0.16887728\n",
      "Iteration 31, loss = 0.16677904\n",
      "Iteration 32, loss = 0.16457697\n",
      "Iteration 33, loss = 0.16290385\n",
      "Iteration 34, loss = 0.16092416\n",
      "Iteration 35, loss = 0.15897972\n",
      "Iteration 36, loss = 0.15742261\n",
      "Iteration 37, loss = 0.15612651\n",
      "Iteration 38, loss = 0.15438482\n",
      "Iteration 39, loss = 0.15301872\n",
      "Iteration 40, loss = 0.15132209\n",
      "Iteration 41, loss = 0.14963545\n",
      "Iteration 42, loss = 0.14847591\n",
      "Iteration 43, loss = 0.14732351\n",
      "Iteration 44, loss = 0.14538429\n",
      "Iteration 45, loss = 0.14413536\n",
      "Iteration 46, loss = 0.14282435\n",
      "Iteration 47, loss = 0.14169314\n",
      "Iteration 48, loss = 0.14048913\n",
      "Iteration 49, loss = 0.13948997\n",
      "Iteration 50, loss = 0.13819137\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kileymack/opt/anaconda3/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (50) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 1.90801609\n",
      "Iteration 2, loss = 0.37494441\n",
      "Iteration 3, loss = 0.30816777\n",
      "Iteration 4, loss = 0.27855064\n",
      "Iteration 5, loss = 0.24904203\n",
      "Iteration 6, loss = 0.23433588\n",
      "Iteration 7, loss = 0.22536205\n",
      "Iteration 8, loss = 0.21173230\n",
      "Iteration 9, loss = 0.20416988\n",
      "Iteration 10, loss = 0.19667186\n",
      "Iteration 11, loss = 0.19017231\n",
      "Iteration 12, loss = 0.18234959\n",
      "Iteration 13, loss = 0.17893628\n",
      "Iteration 14, loss = 0.17316408\n",
      "Iteration 15, loss = 0.16880705\n",
      "Iteration 16, loss = 0.16214421\n",
      "Iteration 17, loss = 0.16283087\n",
      "Iteration 18, loss = 0.15724344\n",
      "Iteration 19, loss = 0.15450249\n",
      "Iteration 20, loss = 0.15072334\n",
      "Iteration 21, loss = 0.14853088\n",
      "Iteration 22, loss = 0.14708482\n",
      "Iteration 23, loss = 0.14538225\n",
      "Iteration 24, loss = 0.14344123\n",
      "Iteration 25, loss = 0.14005907\n",
      "Iteration 26, loss = 0.13677439\n",
      "Iteration 27, loss = 0.13779788\n",
      "Iteration 28, loss = 0.13575139\n",
      "Iteration 29, loss = 0.13447395\n",
      "Iteration 30, loss = 0.13154300\n",
      "Iteration 31, loss = 0.12980352\n",
      "Iteration 32, loss = 0.12768152\n",
      "Iteration 33, loss = 0.12947904\n",
      "Iteration 34, loss = 0.12541091\n",
      "Iteration 35, loss = 0.12542418\n",
      "Iteration 36, loss = 0.12206853\n",
      "Iteration 37, loss = 0.12200497\n",
      "Iteration 38, loss = 0.12215906\n",
      "Iteration 39, loss = 0.12052731\n",
      "Iteration 40, loss = 0.12248923\n",
      "Iteration 41, loss = 0.11889548\n",
      "Iteration 42, loss = 0.11747579\n",
      "Iteration 43, loss = 0.11566241\n",
      "Iteration 44, loss = 0.11687125\n",
      "Iteration 45, loss = 0.11501880\n",
      "Iteration 46, loss = 0.11625048\n",
      "Iteration 47, loss = 0.11454541\n",
      "Iteration 48, loss = 0.11273017\n",
      "Iteration 49, loss = 0.11163151\n",
      "Iteration 50, loss = 0.11476994\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kileymack/opt/anaconda3/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (50) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 1.98476072\n",
      "Iteration 2, loss = 0.38426120\n",
      "Iteration 3, loss = 0.31914079\n",
      "Iteration 4, loss = 0.28544621\n",
      "Iteration 5, loss = 0.25674064\n",
      "Iteration 6, loss = 0.23719356\n",
      "Iteration 7, loss = 0.22404787\n",
      "Iteration 8, loss = 0.21262287\n",
      "Iteration 9, loss = 0.20459224\n",
      "Iteration 10, loss = 0.20039339\n",
      "Iteration 11, loss = 0.18981523\n",
      "Iteration 12, loss = 0.18702486\n",
      "Iteration 13, loss = 0.17995987\n",
      "Iteration 14, loss = 0.17523674\n",
      "Iteration 15, loss = 0.17249480\n",
      "Iteration 16, loss = 0.16759119\n",
      "Iteration 17, loss = 0.16440300\n",
      "Iteration 18, loss = 0.16031837\n",
      "Iteration 19, loss = 0.15861235\n",
      "Iteration 20, loss = 0.15683644\n",
      "Iteration 21, loss = 0.15214493\n",
      "Iteration 22, loss = 0.15036911\n",
      "Iteration 23, loss = 0.14791825\n",
      "Iteration 24, loss = 0.14850451\n",
      "Iteration 25, loss = 0.14411924\n",
      "Iteration 26, loss = 0.14393850\n",
      "Iteration 27, loss = 0.14085959\n",
      "Iteration 28, loss = 0.14012863\n",
      "Iteration 29, loss = 0.13780677\n",
      "Iteration 30, loss = 0.13793248\n",
      "Iteration 31, loss = 0.13483363\n",
      "Iteration 32, loss = 0.13366160\n",
      "Iteration 33, loss = 0.13386033\n",
      "Iteration 34, loss = 0.13082299\n",
      "Iteration 35, loss = 0.13129747\n",
      "Iteration 36, loss = 0.12982309\n",
      "Iteration 37, loss = 0.13043302\n",
      "Iteration 38, loss = 0.12802628\n",
      "Iteration 39, loss = 0.12681994\n",
      "Iteration 40, loss = 0.12644513\n",
      "Iteration 41, loss = 0.12611327\n",
      "Iteration 42, loss = 0.12448570\n",
      "Iteration 43, loss = 0.12625916\n",
      "Iteration 44, loss = 0.12438938\n",
      "Iteration 45, loss = 0.12255480\n",
      "Iteration 46, loss = 0.12286736\n",
      "Iteration 47, loss = 0.12111008\n",
      "Iteration 48, loss = 0.12112961\n",
      "Iteration 49, loss = 0.12021411\n",
      "Iteration 50, loss = 0.12009079\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kileymack/opt/anaconda3/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (50) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 1.64322664\n",
      "Iteration 2, loss = 0.44746643\n",
      "Iteration 3, loss = 0.37450110\n",
      "Iteration 4, loss = 0.33467731\n",
      "Iteration 5, loss = 0.30922241\n",
      "Iteration 6, loss = 0.28992445\n",
      "Iteration 7, loss = 0.27602028\n",
      "Iteration 8, loss = 0.26471115\n",
      "Iteration 9, loss = 0.25566856\n",
      "Iteration 10, loss = 0.24799532\n",
      "Iteration 11, loss = 0.24082625\n",
      "Iteration 12, loss = 0.23490535\n",
      "Iteration 13, loss = 0.22912135\n",
      "Iteration 14, loss = 0.22442114\n",
      "Iteration 15, loss = 0.22076153\n",
      "Iteration 16, loss = 0.21502665\n",
      "Iteration 17, loss = 0.21174468\n",
      "Iteration 18, loss = 0.20783571\n",
      "Iteration 19, loss = 0.20421765\n",
      "Iteration 20, loss = 0.20075690\n",
      "Iteration 21, loss = 0.19766248\n",
      "Iteration 22, loss = 0.19496926\n",
      "Iteration 23, loss = 0.19175108\n",
      "Iteration 24, loss = 0.18949907\n",
      "Iteration 25, loss = 0.18666581\n",
      "Iteration 26, loss = 0.18351921\n",
      "Iteration 27, loss = 0.18155440\n",
      "Iteration 28, loss = 0.18001129\n",
      "Iteration 29, loss = 0.17754644\n",
      "Iteration 30, loss = 0.17528988\n",
      "Iteration 31, loss = 0.17319418\n",
      "Iteration 32, loss = 0.17103927\n",
      "Iteration 33, loss = 0.16929301\n",
      "Iteration 34, loss = 0.16739986\n",
      "Iteration 35, loss = 0.16578732\n",
      "Iteration 36, loss = 0.16367016\n",
      "Iteration 37, loss = 0.16177293\n",
      "Iteration 38, loss = 0.16032338\n",
      "Iteration 39, loss = 0.15863906\n",
      "Iteration 40, loss = 0.15746635\n",
      "Iteration 41, loss = 0.15599692\n",
      "Iteration 42, loss = 0.15416761\n",
      "Iteration 43, loss = 0.15303841\n",
      "Iteration 44, loss = 0.15180767\n",
      "Iteration 45, loss = 0.15095752\n",
      "Iteration 46, loss = 0.14922837\n",
      "Iteration 47, loss = 0.14788922\n",
      "Iteration 48, loss = 0.14664290\n",
      "Iteration 49, loss = 0.14557348\n",
      "Iteration 50, loss = 0.14448091\n",
      "Iteration 51, loss = 0.14302138\n",
      "Iteration 52, loss = 0.14176147\n",
      "Iteration 53, loss = 0.14105746\n",
      "Iteration 54, loss = 0.13990190\n",
      "Iteration 55, loss = 0.13894293\n",
      "Iteration 56, loss = 0.13745161\n",
      "Iteration 57, loss = 0.13658078\n",
      "Iteration 58, loss = 0.13590397\n",
      "Iteration 59, loss = 0.13510442\n",
      "Iteration 60, loss = 0.13392564\n",
      "Iteration 61, loss = 0.13276611\n",
      "Iteration 62, loss = 0.13217636\n",
      "Iteration 63, loss = 0.13115416\n",
      "Iteration 64, loss = 0.13055297\n",
      "Iteration 65, loss = 0.12902752\n",
      "Iteration 66, loss = 0.12822503\n",
      "Iteration 67, loss = 0.12759875\n",
      "Iteration 68, loss = 0.12737123\n",
      "Iteration 69, loss = 0.12646941\n",
      "Iteration 70, loss = 0.12579975\n",
      "Iteration 71, loss = 0.12488459\n",
      "Iteration 72, loss = 0.12400456\n",
      "Iteration 73, loss = 0.12320072\n",
      "Iteration 74, loss = 0.12232627\n",
      "Iteration 75, loss = 0.12218563\n",
      "Iteration 76, loss = 0.12162755\n",
      "Iteration 77, loss = 0.12049422\n",
      "Iteration 78, loss = 0.12040819\n",
      "Iteration 79, loss = 0.11933545\n",
      "Iteration 80, loss = 0.11858780\n",
      "Iteration 81, loss = 0.11806873\n",
      "Iteration 82, loss = 0.11755653\n",
      "Iteration 83, loss = 0.11724612\n",
      "Iteration 84, loss = 0.11629744\n",
      "Iteration 85, loss = 0.11554210\n",
      "Iteration 86, loss = 0.11517589\n",
      "Iteration 87, loss = 0.11467441\n",
      "Iteration 88, loss = 0.11404625\n",
      "Iteration 89, loss = 0.11288524\n",
      "Iteration 90, loss = 0.11297298\n",
      "Iteration 91, loss = 0.11266455\n",
      "Iteration 92, loss = 0.11246941\n",
      "Iteration 93, loss = 0.11154873\n",
      "Iteration 94, loss = 0.11100713\n",
      "Iteration 95, loss = 0.11046631\n",
      "Iteration 96, loss = 0.11033158\n",
      "Iteration 97, loss = 0.10940870\n",
      "Iteration 98, loss = 0.10910531\n",
      "Iteration 99, loss = 0.10864226\n",
      "Iteration 100, loss = 0.10808699\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kileymack/opt/anaconda3/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 1.26619637\n",
      "Iteration 2, loss = 0.43010658\n",
      "Iteration 3, loss = 0.37237972\n",
      "Iteration 4, loss = 0.33948552\n",
      "Iteration 5, loss = 0.31537574\n",
      "Iteration 6, loss = 0.29737138\n",
      "Iteration 7, loss = 0.28240031\n",
      "Iteration 8, loss = 0.27019941\n",
      "Iteration 9, loss = 0.25956831\n",
      "Iteration 10, loss = 0.25018250\n",
      "Iteration 11, loss = 0.24255834\n",
      "Iteration 12, loss = 0.23539079\n",
      "Iteration 13, loss = 0.22892956\n",
      "Iteration 14, loss = 0.22345561\n",
      "Iteration 15, loss = 0.21762290\n",
      "Iteration 16, loss = 0.21276581\n",
      "Iteration 17, loss = 0.20813587\n",
      "Iteration 18, loss = 0.20395972\n",
      "Iteration 19, loss = 0.19969025\n",
      "Iteration 20, loss = 0.19625815\n",
      "Iteration 21, loss = 0.19291887\n",
      "Iteration 22, loss = 0.18952097\n",
      "Iteration 23, loss = 0.18642931\n",
      "Iteration 24, loss = 0.18348038\n",
      "Iteration 25, loss = 0.18083983\n",
      "Iteration 26, loss = 0.17845766\n",
      "Iteration 27, loss = 0.17581545\n",
      "Iteration 28, loss = 0.17335249\n",
      "Iteration 29, loss = 0.17094619\n",
      "Iteration 30, loss = 0.16887728\n",
      "Iteration 31, loss = 0.16677904\n",
      "Iteration 32, loss = 0.16457697\n",
      "Iteration 33, loss = 0.16290385\n",
      "Iteration 34, loss = 0.16092416\n",
      "Iteration 35, loss = 0.15897972\n",
      "Iteration 36, loss = 0.15742261\n",
      "Iteration 37, loss = 0.15612651\n",
      "Iteration 38, loss = 0.15438482\n",
      "Iteration 39, loss = 0.15301872\n",
      "Iteration 40, loss = 0.15132209\n",
      "Iteration 41, loss = 0.14963545\n",
      "Iteration 42, loss = 0.14847591\n",
      "Iteration 43, loss = 0.14732351\n",
      "Iteration 44, loss = 0.14538429\n",
      "Iteration 45, loss = 0.14413536\n",
      "Iteration 46, loss = 0.14282435\n",
      "Iteration 47, loss = 0.14169314\n",
      "Iteration 48, loss = 0.14048913\n",
      "Iteration 49, loss = 0.13948997\n",
      "Iteration 50, loss = 0.13819137\n",
      "Iteration 51, loss = 0.13718053\n",
      "Iteration 52, loss = 0.13605140\n",
      "Iteration 53, loss = 0.13498371\n",
      "Iteration 54, loss = 0.13406271\n",
      "Iteration 55, loss = 0.13303224\n",
      "Iteration 56, loss = 0.13215417\n",
      "Iteration 57, loss = 0.13102389\n",
      "Iteration 58, loss = 0.13033154\n",
      "Iteration 59, loss = 0.12932909\n",
      "Iteration 60, loss = 0.12809987\n",
      "Iteration 61, loss = 0.12716237\n",
      "Iteration 62, loss = 0.12648895\n",
      "Iteration 63, loss = 0.12550313\n",
      "Iteration 64, loss = 0.12470549\n",
      "Iteration 65, loss = 0.12399793\n",
      "Iteration 66, loss = 0.12327571\n",
      "Iteration 67, loss = 0.12231244\n",
      "Iteration 68, loss = 0.12172784\n",
      "Iteration 69, loss = 0.12103439\n",
      "Iteration 70, loss = 0.12013207\n",
      "Iteration 71, loss = 0.11939127\n",
      "Iteration 72, loss = 0.11890658\n",
      "Iteration 73, loss = 0.11806159\n",
      "Iteration 74, loss = 0.11709756\n",
      "Iteration 75, loss = 0.11689132\n",
      "Iteration 76, loss = 0.11594818\n",
      "Iteration 77, loss = 0.11570395\n",
      "Iteration 78, loss = 0.11463737\n",
      "Iteration 79, loss = 0.11429667\n",
      "Iteration 80, loss = 0.11370933\n",
      "Iteration 81, loss = 0.11295103\n",
      "Iteration 82, loss = 0.11239042\n",
      "Iteration 83, loss = 0.11182893\n",
      "Iteration 84, loss = 0.11139229\n",
      "Iteration 85, loss = 0.11079560\n",
      "Iteration 86, loss = 0.11008215\n",
      "Iteration 87, loss = 0.10939575\n",
      "Iteration 88, loss = 0.10910441\n",
      "Iteration 89, loss = 0.10870534\n",
      "Iteration 90, loss = 0.10862158\n",
      "Iteration 91, loss = 0.10757372\n",
      "Iteration 92, loss = 0.10698764\n",
      "Iteration 93, loss = 0.10640478\n",
      "Iteration 94, loss = 0.10585966\n",
      "Iteration 95, loss = 0.10560933\n",
      "Iteration 96, loss = 0.10506565\n",
      "Iteration 97, loss = 0.10436416\n",
      "Iteration 98, loss = 0.10436289\n",
      "Iteration 99, loss = 0.10358135\n",
      "Iteration 100, loss = 0.10282363\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kileymack/opt/anaconda3/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 1.90801609\n",
      "Iteration 2, loss = 0.37494441\n",
      "Iteration 3, loss = 0.30816777\n",
      "Iteration 4, loss = 0.27855064\n",
      "Iteration 5, loss = 0.24904203\n",
      "Iteration 6, loss = 0.23433588\n",
      "Iteration 7, loss = 0.22536205\n",
      "Iteration 8, loss = 0.21173230\n",
      "Iteration 9, loss = 0.20416988\n",
      "Iteration 10, loss = 0.19667186\n",
      "Iteration 11, loss = 0.19017231\n",
      "Iteration 12, loss = 0.18234959\n",
      "Iteration 13, loss = 0.17893628\n",
      "Iteration 14, loss = 0.17316408\n",
      "Iteration 15, loss = 0.16880705\n",
      "Iteration 16, loss = 0.16214421\n",
      "Iteration 17, loss = 0.16283087\n",
      "Iteration 18, loss = 0.15724344\n",
      "Iteration 19, loss = 0.15450249\n",
      "Iteration 20, loss = 0.15072334\n",
      "Iteration 21, loss = 0.14853088\n",
      "Iteration 22, loss = 0.14708482\n",
      "Iteration 23, loss = 0.14538225\n",
      "Iteration 24, loss = 0.14344123\n",
      "Iteration 25, loss = 0.14005907\n",
      "Iteration 26, loss = 0.13677439\n",
      "Iteration 27, loss = 0.13779788\n",
      "Iteration 28, loss = 0.13575139\n",
      "Iteration 29, loss = 0.13447395\n",
      "Iteration 30, loss = 0.13154300\n",
      "Iteration 31, loss = 0.12980352\n",
      "Iteration 32, loss = 0.12768152\n",
      "Iteration 33, loss = 0.12947904\n",
      "Iteration 34, loss = 0.12541091\n",
      "Iteration 35, loss = 0.12542418\n",
      "Iteration 36, loss = 0.12206853\n",
      "Iteration 37, loss = 0.12200497\n",
      "Iteration 38, loss = 0.12215906\n",
      "Iteration 39, loss = 0.12052731\n",
      "Iteration 40, loss = 0.12248923\n",
      "Iteration 41, loss = 0.11889548\n",
      "Iteration 42, loss = 0.11747579\n",
      "Iteration 43, loss = 0.11566241\n",
      "Iteration 44, loss = 0.11687125\n",
      "Iteration 45, loss = 0.11501880\n",
      "Iteration 46, loss = 0.11625048\n",
      "Iteration 47, loss = 0.11454541\n",
      "Iteration 48, loss = 0.11273017\n",
      "Iteration 49, loss = 0.11163151\n",
      "Iteration 50, loss = 0.11476994\n",
      "Iteration 51, loss = 0.10896834\n",
      "Iteration 52, loss = 0.10951608\n",
      "Iteration 53, loss = 0.11252522\n",
      "Iteration 54, loss = 0.10893265\n",
      "Iteration 55, loss = 0.10812136\n",
      "Iteration 56, loss = 0.10753050\n",
      "Iteration 57, loss = 0.10656736\n",
      "Iteration 58, loss = 0.10759224\n",
      "Iteration 59, loss = 0.10728891\n",
      "Iteration 60, loss = 0.10667561\n",
      "Iteration 61, loss = 0.10371165\n",
      "Iteration 62, loss = 0.10504883\n",
      "Iteration 63, loss = 0.10482636\n",
      "Iteration 64, loss = 0.10583068\n",
      "Iteration 65, loss = 0.10261932\n",
      "Iteration 66, loss = 0.10238438\n",
      "Iteration 67, loss = 0.10434651\n",
      "Iteration 68, loss = 0.10276170\n",
      "Iteration 69, loss = 0.10488954\n",
      "Iteration 70, loss = 0.10252818\n",
      "Iteration 71, loss = 0.10182407\n",
      "Iteration 72, loss = 0.10252680\n",
      "Iteration 73, loss = 0.10008744\n",
      "Iteration 74, loss = 0.10115802\n",
      "Iteration 75, loss = 0.10185894\n",
      "Iteration 76, loss = 0.10075259\n",
      "Iteration 77, loss = 0.10171722\n",
      "Iteration 78, loss = 0.10054277\n",
      "Iteration 79, loss = 0.09808049\n",
      "Iteration 80, loss = 0.09834304\n",
      "Iteration 81, loss = 0.09930483\n",
      "Iteration 82, loss = 0.09910096\n",
      "Iteration 83, loss = 0.09796687\n",
      "Iteration 84, loss = 0.09645470\n",
      "Iteration 85, loss = 0.09743753\n",
      "Iteration 86, loss = 0.09818368\n",
      "Iteration 87, loss = 0.09512213\n",
      "Iteration 88, loss = 0.10061014\n",
      "Iteration 89, loss = 0.09562873\n",
      "Iteration 90, loss = 0.09669678\n",
      "Iteration 91, loss = 0.09593262\n",
      "Iteration 92, loss = 0.09865441\n",
      "Iteration 93, loss = 0.09567753\n",
      "Iteration 94, loss = 0.09506909\n",
      "Iteration 95, loss = 0.09571548\n",
      "Iteration 96, loss = 0.09430691\n",
      "Iteration 97, loss = 0.09460998\n",
      "Iteration 98, loss = 0.09402299\n",
      "Iteration 99, loss = 0.09599777\n",
      "Iteration 100, loss = 0.09456427\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kileymack/opt/anaconda3/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 1.98476072\n",
      "Iteration 2, loss = 0.38426120\n",
      "Iteration 3, loss = 0.31914079\n",
      "Iteration 4, loss = 0.28544621\n",
      "Iteration 5, loss = 0.25674064\n",
      "Iteration 6, loss = 0.23719356\n",
      "Iteration 7, loss = 0.22404787\n",
      "Iteration 8, loss = 0.21262287\n",
      "Iteration 9, loss = 0.20459224\n",
      "Iteration 10, loss = 0.20039339\n",
      "Iteration 11, loss = 0.18981523\n",
      "Iteration 12, loss = 0.18702486\n",
      "Iteration 13, loss = 0.17995987\n",
      "Iteration 14, loss = 0.17523674\n",
      "Iteration 15, loss = 0.17249480\n",
      "Iteration 16, loss = 0.16759119\n",
      "Iteration 17, loss = 0.16440300\n",
      "Iteration 18, loss = 0.16031837\n",
      "Iteration 19, loss = 0.15861235\n",
      "Iteration 20, loss = 0.15683644\n",
      "Iteration 21, loss = 0.15214493\n",
      "Iteration 22, loss = 0.15036911\n",
      "Iteration 23, loss = 0.14791825\n",
      "Iteration 24, loss = 0.14850451\n",
      "Iteration 25, loss = 0.14411924\n",
      "Iteration 26, loss = 0.14393850\n",
      "Iteration 27, loss = 0.14085959\n",
      "Iteration 28, loss = 0.14012863\n",
      "Iteration 29, loss = 0.13780677\n",
      "Iteration 30, loss = 0.13793248\n",
      "Iteration 31, loss = 0.13483363\n",
      "Iteration 32, loss = 0.13366160\n",
      "Iteration 33, loss = 0.13386033\n",
      "Iteration 34, loss = 0.13082299\n",
      "Iteration 35, loss = 0.13129747\n",
      "Iteration 36, loss = 0.12982309\n",
      "Iteration 37, loss = 0.13043302\n",
      "Iteration 38, loss = 0.12802628\n",
      "Iteration 39, loss = 0.12681994\n",
      "Iteration 40, loss = 0.12644513\n",
      "Iteration 41, loss = 0.12611327\n",
      "Iteration 42, loss = 0.12448570\n",
      "Iteration 43, loss = 0.12625916\n",
      "Iteration 44, loss = 0.12438938\n",
      "Iteration 45, loss = 0.12255480\n",
      "Iteration 46, loss = 0.12286736\n",
      "Iteration 47, loss = 0.12111008\n",
      "Iteration 48, loss = 0.12112961\n",
      "Iteration 49, loss = 0.12021411\n",
      "Iteration 50, loss = 0.12009079\n",
      "Iteration 51, loss = 0.12020951\n",
      "Iteration 52, loss = 0.11742806\n",
      "Iteration 53, loss = 0.11861445\n",
      "Iteration 54, loss = 0.11858688\n",
      "Iteration 55, loss = 0.11535820\n",
      "Iteration 56, loss = 0.11886686\n",
      "Iteration 57, loss = 0.11778581\n",
      "Iteration 58, loss = 0.11611208\n",
      "Iteration 59, loss = 0.11678855\n",
      "Iteration 60, loss = 0.11572783\n",
      "Iteration 61, loss = 0.11394884\n",
      "Iteration 62, loss = 0.11433674\n",
      "Iteration 63, loss = 0.11352874\n",
      "Iteration 64, loss = 0.11349246\n",
      "Iteration 65, loss = 0.11359046\n",
      "Iteration 66, loss = 0.11391299\n",
      "Iteration 67, loss = 0.11346332\n",
      "Iteration 68, loss = 0.11222118\n",
      "Iteration 69, loss = 0.11368919\n",
      "Iteration 70, loss = 0.11374283\n",
      "Iteration 71, loss = 0.11092536\n",
      "Iteration 72, loss = 0.11019194\n",
      "Iteration 73, loss = 0.11159372\n",
      "Iteration 74, loss = 0.11098454\n",
      "Iteration 75, loss = 0.11186504\n",
      "Iteration 76, loss = 0.11047805\n",
      "Iteration 77, loss = 0.11038625\n",
      "Iteration 78, loss = 0.10898690\n",
      "Iteration 79, loss = 0.10865193\n",
      "Iteration 80, loss = 0.10898945\n",
      "Iteration 81, loss = 0.10855803\n",
      "Iteration 82, loss = 0.10741799\n",
      "Iteration 83, loss = 0.10875423\n",
      "Iteration 84, loss = 0.10839775\n",
      "Iteration 85, loss = 0.10803744\n",
      "Iteration 86, loss = 0.10718458\n",
      "Iteration 87, loss = 0.10822601\n",
      "Iteration 88, loss = 0.10647723\n",
      "Iteration 89, loss = 0.10750565\n",
      "Iteration 90, loss = 0.10693157\n",
      "Iteration 91, loss = 0.10674938\n",
      "Iteration 92, loss = 0.10532782\n",
      "Iteration 93, loss = 0.10646073\n",
      "Iteration 94, loss = 0.10509963\n",
      "Iteration 95, loss = 0.10653921\n",
      "Iteration 96, loss = 0.10473669\n",
      "Iteration 97, loss = 0.10365089\n",
      "Iteration 98, loss = 0.10664144\n",
      "Iteration 99, loss = 0.10476883\n",
      "Iteration 100, loss = 0.10522463\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kileymack/opt/anaconda3/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 1.55520877\n",
      "Iteration 2, loss = 0.45153631\n",
      "Iteration 3, loss = 0.37597383\n",
      "Iteration 4, loss = 0.33895131\n",
      "Iteration 5, loss = 0.31459111\n",
      "Iteration 6, loss = 0.29660611\n",
      "Iteration 7, loss = 0.28251802\n",
      "Iteration 8, loss = 0.27048592\n",
      "Iteration 9, loss = 0.26073982\n",
      "Iteration 10, loss = 0.25215779\n",
      "Iteration 11, loss = 0.24532254\n",
      "Iteration 12, loss = 0.23849721\n",
      "Iteration 13, loss = 0.23253743\n",
      "Iteration 14, loss = 0.22801788\n",
      "Iteration 15, loss = 0.22242953\n",
      "Iteration 16, loss = 0.21830013\n",
      "Iteration 17, loss = 0.21502738\n",
      "Iteration 18, loss = 0.21132621\n",
      "Iteration 19, loss = 0.20718891\n",
      "Iteration 20, loss = 0.20436733\n",
      "Iteration 21, loss = 0.20115835\n",
      "Iteration 22, loss = 0.19825876\n",
      "Iteration 23, loss = 0.19555682\n",
      "Iteration 24, loss = 0.19278328\n",
      "Iteration 25, loss = 0.19106485\n",
      "Iteration 26, loss = 0.18800234\n",
      "Iteration 27, loss = 0.18556278\n",
      "Iteration 28, loss = 0.18365760\n",
      "Iteration 29, loss = 0.18129077\n",
      "Iteration 30, loss = 0.17948847\n",
      "Iteration 31, loss = 0.17711758\n",
      "Iteration 32, loss = 0.17600897\n",
      "Iteration 33, loss = 0.17378269\n",
      "Iteration 34, loss = 0.17220199\n",
      "Iteration 35, loss = 0.17034956\n",
      "Iteration 36, loss = 0.16905691\n",
      "Iteration 37, loss = 0.16771959\n",
      "Iteration 38, loss = 0.16588095\n",
      "Iteration 39, loss = 0.16522234\n",
      "Iteration 40, loss = 0.16347085\n",
      "Iteration 41, loss = 0.16138039\n",
      "Iteration 42, loss = 0.16021938\n",
      "Iteration 43, loss = 0.15878747\n",
      "Iteration 44, loss = 0.15803007\n",
      "Iteration 45, loss = 0.15678759\n",
      "Iteration 46, loss = 0.15555646\n",
      "Iteration 47, loss = 0.15398052\n",
      "Iteration 48, loss = 0.15238866\n",
      "Iteration 49, loss = 0.15158716\n",
      "Iteration 50, loss = 0.15028156\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kileymack/opt/anaconda3/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (50) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 1.18446270\n",
      "Iteration 2, loss = 0.43440252\n",
      "Iteration 3, loss = 0.37597126\n",
      "Iteration 4, loss = 0.34324044\n",
      "Iteration 5, loss = 0.31993371\n",
      "Iteration 6, loss = 0.30265833\n",
      "Iteration 7, loss = 0.28857308\n",
      "Iteration 8, loss = 0.27574162\n",
      "Iteration 9, loss = 0.26503788\n",
      "Iteration 10, loss = 0.25571917\n",
      "Iteration 11, loss = 0.24765400\n",
      "Iteration 12, loss = 0.24018936\n",
      "Iteration 13, loss = 0.23344656\n",
      "Iteration 14, loss = 0.22758198\n",
      "Iteration 15, loss = 0.22254878\n",
      "Iteration 16, loss = 0.21782889\n",
      "Iteration 17, loss = 0.21347235\n",
      "Iteration 18, loss = 0.20927169\n",
      "Iteration 19, loss = 0.20558477\n",
      "Iteration 20, loss = 0.20167788\n",
      "Iteration 21, loss = 0.19877371\n",
      "Iteration 22, loss = 0.19574245\n",
      "Iteration 23, loss = 0.19289674\n",
      "Iteration 24, loss = 0.19013027\n",
      "Iteration 25, loss = 0.18756741\n",
      "Iteration 26, loss = 0.18540903\n",
      "Iteration 27, loss = 0.18309951\n",
      "Iteration 28, loss = 0.18044146\n",
      "Iteration 29, loss = 0.17851433\n",
      "Iteration 30, loss = 0.17630124\n",
      "Iteration 31, loss = 0.17428966\n",
      "Iteration 32, loss = 0.17274904\n",
      "Iteration 33, loss = 0.17051523\n",
      "Iteration 34, loss = 0.16871222\n",
      "Iteration 35, loss = 0.16685536\n",
      "Iteration 36, loss = 0.16547939\n",
      "Iteration 37, loss = 0.16382840\n",
      "Iteration 38, loss = 0.16220581\n",
      "Iteration 39, loss = 0.16076547\n",
      "Iteration 40, loss = 0.15915559\n",
      "Iteration 41, loss = 0.15811441\n",
      "Iteration 42, loss = 0.15643836\n",
      "Iteration 43, loss = 0.15509720\n",
      "Iteration 44, loss = 0.15370473\n",
      "Iteration 45, loss = 0.15251920\n",
      "Iteration 46, loss = 0.15141485\n",
      "Iteration 47, loss = 0.15015645\n",
      "Iteration 48, loss = 0.14911739\n",
      "Iteration 49, loss = 0.14778458\n",
      "Iteration 50, loss = 0.14655624\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kileymack/opt/anaconda3/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (50) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 1.83016396\n",
      "Iteration 2, loss = 0.39187184\n",
      "Iteration 3, loss = 0.31382096\n",
      "Iteration 4, loss = 0.27864189\n",
      "Iteration 5, loss = 0.26196771\n",
      "Iteration 6, loss = 0.24810105\n",
      "Iteration 7, loss = 0.23341762\n",
      "Iteration 8, loss = 0.22134124\n",
      "Iteration 9, loss = 0.21100152\n",
      "Iteration 10, loss = 0.20447277\n",
      "Iteration 11, loss = 0.19815122\n",
      "Iteration 12, loss = 0.19287148\n",
      "Iteration 13, loss = 0.18453294\n",
      "Iteration 14, loss = 0.18191539\n",
      "Iteration 15, loss = 0.17677003\n",
      "Iteration 16, loss = 0.17177320\n",
      "Iteration 17, loss = 0.17191890\n",
      "Iteration 18, loss = 0.16370683\n",
      "Iteration 19, loss = 0.16294258\n",
      "Iteration 20, loss = 0.16012759\n",
      "Iteration 21, loss = 0.15491247\n",
      "Iteration 22, loss = 0.15397076\n",
      "Iteration 23, loss = 0.14801935\n",
      "Iteration 24, loss = 0.14850437\n",
      "Iteration 25, loss = 0.14540434\n",
      "Iteration 26, loss = 0.14636679\n",
      "Iteration 27, loss = 0.14103024\n",
      "Iteration 28, loss = 0.13938007\n",
      "Iteration 29, loss = 0.13808698\n",
      "Iteration 30, loss = 0.13711884\n",
      "Iteration 31, loss = 0.13572909\n",
      "Iteration 32, loss = 0.13447188\n",
      "Iteration 33, loss = 0.13298500\n",
      "Iteration 34, loss = 0.13054258\n",
      "Iteration 35, loss = 0.12763910\n",
      "Iteration 36, loss = 0.13050691\n",
      "Iteration 37, loss = 0.12516453\n",
      "Iteration 38, loss = 0.12440973\n",
      "Iteration 39, loss = 0.12644672\n",
      "Iteration 40, loss = 0.12475196\n",
      "Iteration 41, loss = 0.12301953\n",
      "Iteration 42, loss = 0.12456237\n",
      "Iteration 43, loss = 0.11895292\n",
      "Iteration 44, loss = 0.12260095\n",
      "Iteration 45, loss = 0.11961652\n",
      "Iteration 46, loss = 0.11847605\n",
      "Iteration 47, loss = 0.11934114\n",
      "Iteration 48, loss = 0.11585294\n",
      "Iteration 49, loss = 0.11617471\n",
      "Iteration 50, loss = 0.11723837\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kileymack/opt/anaconda3/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (50) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 1.85483652\n",
      "Iteration 2, loss = 0.38828490\n",
      "Iteration 3, loss = 0.32313285\n",
      "Iteration 4, loss = 0.29032141\n",
      "Iteration 5, loss = 0.26490751\n",
      "Iteration 6, loss = 0.24618751\n",
      "Iteration 7, loss = 0.23547346\n",
      "Iteration 8, loss = 0.22140848\n",
      "Iteration 9, loss = 0.21422447\n",
      "Iteration 10, loss = 0.20713531\n",
      "Iteration 11, loss = 0.20023228\n",
      "Iteration 12, loss = 0.19176431\n",
      "Iteration 13, loss = 0.18949049\n",
      "Iteration 14, loss = 0.18367251\n",
      "Iteration 15, loss = 0.17910822\n",
      "Iteration 16, loss = 0.17489221\n",
      "Iteration 17, loss = 0.17398324\n",
      "Iteration 18, loss = 0.17072590\n",
      "Iteration 19, loss = 0.16652264\n",
      "Iteration 20, loss = 0.16549070\n",
      "Iteration 21, loss = 0.16233017\n",
      "Iteration 22, loss = 0.16053522\n",
      "Iteration 23, loss = 0.15686104\n",
      "Iteration 24, loss = 0.15423817\n",
      "Iteration 25, loss = 0.15384192\n",
      "Iteration 26, loss = 0.15315142\n",
      "Iteration 27, loss = 0.15092553\n",
      "Iteration 28, loss = 0.14953008\n",
      "Iteration 29, loss = 0.14683047\n",
      "Iteration 30, loss = 0.14631482\n",
      "Iteration 31, loss = 0.14536189\n",
      "Iteration 32, loss = 0.14343077\n",
      "Iteration 33, loss = 0.14309345\n",
      "Iteration 34, loss = 0.14108443\n",
      "Iteration 35, loss = 0.13981096\n",
      "Iteration 36, loss = 0.13879040\n",
      "Iteration 37, loss = 0.13765456\n",
      "Iteration 38, loss = 0.13747752\n",
      "Iteration 39, loss = 0.13661612\n",
      "Iteration 40, loss = 0.13389711\n",
      "Iteration 41, loss = 0.13600064\n",
      "Iteration 42, loss = 0.13255333\n",
      "Iteration 43, loss = 0.13335499\n",
      "Iteration 44, loss = 0.13227518\n",
      "Iteration 45, loss = 0.13118556\n",
      "Iteration 46, loss = 0.13027505\n",
      "Iteration 47, loss = 0.12936939\n",
      "Iteration 48, loss = 0.12996099\n",
      "Iteration 49, loss = 0.12787750\n",
      "Iteration 50, loss = 0.12715985\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kileymack/opt/anaconda3/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (50) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 1.55520877\n",
      "Iteration 2, loss = 0.45153631\n",
      "Iteration 3, loss = 0.37597383\n",
      "Iteration 4, loss = 0.33895131\n",
      "Iteration 5, loss = 0.31459111\n",
      "Iteration 6, loss = 0.29660611\n",
      "Iteration 7, loss = 0.28251802\n",
      "Iteration 8, loss = 0.27048592\n",
      "Iteration 9, loss = 0.26073982\n",
      "Iteration 10, loss = 0.25215779\n",
      "Iteration 11, loss = 0.24532254\n",
      "Iteration 12, loss = 0.23849721\n",
      "Iteration 13, loss = 0.23253743\n",
      "Iteration 14, loss = 0.22801788\n",
      "Iteration 15, loss = 0.22242953\n",
      "Iteration 16, loss = 0.21830013\n",
      "Iteration 17, loss = 0.21502738\n",
      "Iteration 18, loss = 0.21132621\n",
      "Iteration 19, loss = 0.20718891\n",
      "Iteration 20, loss = 0.20436733\n",
      "Iteration 21, loss = 0.20115835\n",
      "Iteration 22, loss = 0.19825876\n",
      "Iteration 23, loss = 0.19555682\n",
      "Iteration 24, loss = 0.19278328\n",
      "Iteration 25, loss = 0.19106485\n",
      "Iteration 26, loss = 0.18800234\n",
      "Iteration 27, loss = 0.18556278\n",
      "Iteration 28, loss = 0.18365760\n",
      "Iteration 29, loss = 0.18129077\n",
      "Iteration 30, loss = 0.17948847\n",
      "Iteration 31, loss = 0.17711758\n",
      "Iteration 32, loss = 0.17600897\n",
      "Iteration 33, loss = 0.17378269\n",
      "Iteration 34, loss = 0.17220199\n",
      "Iteration 35, loss = 0.17034956\n",
      "Iteration 36, loss = 0.16905691\n",
      "Iteration 37, loss = 0.16771959\n",
      "Iteration 38, loss = 0.16588095\n",
      "Iteration 39, loss = 0.16522234\n",
      "Iteration 40, loss = 0.16347085\n",
      "Iteration 41, loss = 0.16138039\n",
      "Iteration 42, loss = 0.16021938\n",
      "Iteration 43, loss = 0.15878747\n",
      "Iteration 44, loss = 0.15803007\n",
      "Iteration 45, loss = 0.15678759\n",
      "Iteration 46, loss = 0.15555646\n",
      "Iteration 47, loss = 0.15398052\n",
      "Iteration 48, loss = 0.15238866\n",
      "Iteration 49, loss = 0.15158716\n",
      "Iteration 50, loss = 0.15028156\n",
      "Iteration 51, loss = 0.14995747\n",
      "Iteration 52, loss = 0.14793493\n",
      "Iteration 53, loss = 0.14743229\n",
      "Iteration 54, loss = 0.14620606\n",
      "Iteration 55, loss = 0.14604837\n",
      "Iteration 56, loss = 0.14505930\n",
      "Iteration 57, loss = 0.14403168\n",
      "Iteration 58, loss = 0.14277950\n",
      "Iteration 59, loss = 0.14284331\n",
      "Iteration 60, loss = 0.14120745\n",
      "Iteration 61, loss = 0.14007551\n",
      "Iteration 62, loss = 0.13983238\n",
      "Iteration 63, loss = 0.13875617\n",
      "Iteration 64, loss = 0.13746717\n",
      "Iteration 65, loss = 0.13675122\n",
      "Iteration 66, loss = 0.13655941\n",
      "Iteration 67, loss = 0.13544912\n",
      "Iteration 68, loss = 0.13480329\n",
      "Iteration 69, loss = 0.13406781\n",
      "Iteration 70, loss = 0.13364151\n",
      "Iteration 71, loss = 0.13272817\n",
      "Iteration 72, loss = 0.13192051\n",
      "Iteration 73, loss = 0.13146398\n",
      "Iteration 74, loss = 0.13092709\n",
      "Iteration 75, loss = 0.12951912\n",
      "Iteration 76, loss = 0.12902495\n",
      "Iteration 77, loss = 0.12832728\n",
      "Iteration 78, loss = 0.12814569\n",
      "Iteration 79, loss = 0.12760737\n",
      "Iteration 80, loss = 0.12683845\n",
      "Iteration 81, loss = 0.12625905\n",
      "Iteration 82, loss = 0.12583044\n",
      "Iteration 83, loss = 0.12557327\n",
      "Iteration 84, loss = 0.12468138\n",
      "Iteration 85, loss = 0.12379146\n",
      "Iteration 86, loss = 0.12317361\n",
      "Iteration 87, loss = 0.12243240\n",
      "Iteration 88, loss = 0.12248064\n",
      "Iteration 89, loss = 0.12218826\n",
      "Iteration 90, loss = 0.12145821\n",
      "Iteration 91, loss = 0.12122165\n",
      "Iteration 92, loss = 0.12001836\n",
      "Iteration 93, loss = 0.11981187\n",
      "Iteration 94, loss = 0.11950840\n",
      "Iteration 95, loss = 0.11837660\n",
      "Iteration 96, loss = 0.11831481\n",
      "Iteration 97, loss = 0.11852025\n",
      "Iteration 98, loss = 0.11737017\n",
      "Iteration 99, loss = 0.11672990\n",
      "Iteration 100, loss = 0.11599236\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kileymack/opt/anaconda3/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 1.18446270\n",
      "Iteration 2, loss = 0.43440252\n",
      "Iteration 3, loss = 0.37597126\n",
      "Iteration 4, loss = 0.34324044\n",
      "Iteration 5, loss = 0.31993371\n",
      "Iteration 6, loss = 0.30265833\n",
      "Iteration 7, loss = 0.28857308\n",
      "Iteration 8, loss = 0.27574162\n",
      "Iteration 9, loss = 0.26503788\n",
      "Iteration 10, loss = 0.25571917\n",
      "Iteration 11, loss = 0.24765400\n",
      "Iteration 12, loss = 0.24018936\n",
      "Iteration 13, loss = 0.23344656\n",
      "Iteration 14, loss = 0.22758198\n",
      "Iteration 15, loss = 0.22254878\n",
      "Iteration 16, loss = 0.21782889\n",
      "Iteration 17, loss = 0.21347235\n",
      "Iteration 18, loss = 0.20927169\n",
      "Iteration 19, loss = 0.20558477\n",
      "Iteration 20, loss = 0.20167788\n",
      "Iteration 21, loss = 0.19877371\n",
      "Iteration 22, loss = 0.19574245\n",
      "Iteration 23, loss = 0.19289674\n",
      "Iteration 24, loss = 0.19013027\n",
      "Iteration 25, loss = 0.18756741\n",
      "Iteration 26, loss = 0.18540903\n",
      "Iteration 27, loss = 0.18309951\n",
      "Iteration 28, loss = 0.18044146\n",
      "Iteration 29, loss = 0.17851433\n",
      "Iteration 30, loss = 0.17630124\n",
      "Iteration 31, loss = 0.17428966\n",
      "Iteration 32, loss = 0.17274904\n",
      "Iteration 33, loss = 0.17051523\n",
      "Iteration 34, loss = 0.16871222\n",
      "Iteration 35, loss = 0.16685536\n",
      "Iteration 36, loss = 0.16547939\n",
      "Iteration 37, loss = 0.16382840\n",
      "Iteration 38, loss = 0.16220581\n",
      "Iteration 39, loss = 0.16076547\n",
      "Iteration 40, loss = 0.15915559\n",
      "Iteration 41, loss = 0.15811441\n",
      "Iteration 42, loss = 0.15643836\n",
      "Iteration 43, loss = 0.15509720\n",
      "Iteration 44, loss = 0.15370473\n",
      "Iteration 45, loss = 0.15251920\n",
      "Iteration 46, loss = 0.15141485\n",
      "Iteration 47, loss = 0.15015645\n",
      "Iteration 48, loss = 0.14911739\n",
      "Iteration 49, loss = 0.14778458\n",
      "Iteration 50, loss = 0.14655624\n",
      "Iteration 51, loss = 0.14551076\n",
      "Iteration 52, loss = 0.14431147\n",
      "Iteration 53, loss = 0.14354065\n",
      "Iteration 54, loss = 0.14235198\n",
      "Iteration 55, loss = 0.14121427\n",
      "Iteration 56, loss = 0.14081735\n",
      "Iteration 57, loss = 0.13963580\n",
      "Iteration 58, loss = 0.13883788\n",
      "Iteration 59, loss = 0.13762190\n",
      "Iteration 60, loss = 0.13673800\n",
      "Iteration 61, loss = 0.13622766\n",
      "Iteration 62, loss = 0.13545964\n",
      "Iteration 63, loss = 0.13438046\n",
      "Iteration 64, loss = 0.13348285\n",
      "Iteration 65, loss = 0.13249386\n",
      "Iteration 66, loss = 0.13198602\n",
      "Iteration 67, loss = 0.13102710\n",
      "Iteration 68, loss = 0.13048873\n",
      "Iteration 69, loss = 0.13002494\n",
      "Iteration 70, loss = 0.12890924\n",
      "Iteration 71, loss = 0.12825126\n",
      "Iteration 72, loss = 0.12758023\n",
      "Iteration 73, loss = 0.12699828\n",
      "Iteration 74, loss = 0.12600189\n",
      "Iteration 75, loss = 0.12511301\n",
      "Iteration 76, loss = 0.12481296\n",
      "Iteration 77, loss = 0.12418866\n",
      "Iteration 78, loss = 0.12367908\n",
      "Iteration 79, loss = 0.12306286\n",
      "Iteration 80, loss = 0.12255170\n",
      "Iteration 81, loss = 0.12158655\n",
      "Iteration 82, loss = 0.12114072\n",
      "Iteration 83, loss = 0.12048730\n",
      "Iteration 84, loss = 0.11962847\n",
      "Iteration 85, loss = 0.11938874\n",
      "Iteration 86, loss = 0.11861154\n",
      "Iteration 87, loss = 0.11784213\n",
      "Iteration 88, loss = 0.11782427\n",
      "Iteration 89, loss = 0.11728654\n",
      "Iteration 90, loss = 0.11676280\n",
      "Iteration 91, loss = 0.11617852\n",
      "Iteration 92, loss = 0.11554679\n",
      "Iteration 93, loss = 0.11494701\n",
      "Iteration 94, loss = 0.11451707\n",
      "Iteration 95, loss = 0.11403911\n",
      "Iteration 96, loss = 0.11356972\n",
      "Iteration 97, loss = 0.11298385\n",
      "Iteration 98, loss = 0.11269306\n",
      "Iteration 99, loss = 0.11197519\n",
      "Iteration 100, loss = 0.11159417\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kileymack/opt/anaconda3/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 1.83016396\n",
      "Iteration 2, loss = 0.39187184\n",
      "Iteration 3, loss = 0.31382096\n",
      "Iteration 4, loss = 0.27864189\n",
      "Iteration 5, loss = 0.26196771\n",
      "Iteration 6, loss = 0.24810105\n",
      "Iteration 7, loss = 0.23341762\n",
      "Iteration 8, loss = 0.22134124\n",
      "Iteration 9, loss = 0.21100152\n",
      "Iteration 10, loss = 0.20447277\n",
      "Iteration 11, loss = 0.19815122\n",
      "Iteration 12, loss = 0.19287148\n",
      "Iteration 13, loss = 0.18453294\n",
      "Iteration 14, loss = 0.18191539\n",
      "Iteration 15, loss = 0.17677003\n",
      "Iteration 16, loss = 0.17177320\n",
      "Iteration 17, loss = 0.17191890\n",
      "Iteration 18, loss = 0.16370683\n",
      "Iteration 19, loss = 0.16294258\n",
      "Iteration 20, loss = 0.16012759\n",
      "Iteration 21, loss = 0.15491247\n",
      "Iteration 22, loss = 0.15397076\n",
      "Iteration 23, loss = 0.14801935\n",
      "Iteration 24, loss = 0.14850437\n",
      "Iteration 25, loss = 0.14540434\n",
      "Iteration 26, loss = 0.14636679\n",
      "Iteration 27, loss = 0.14103024\n",
      "Iteration 28, loss = 0.13938007\n",
      "Iteration 29, loss = 0.13808698\n",
      "Iteration 30, loss = 0.13711884\n",
      "Iteration 31, loss = 0.13572909\n",
      "Iteration 32, loss = 0.13447188\n",
      "Iteration 33, loss = 0.13298500\n",
      "Iteration 34, loss = 0.13054258\n",
      "Iteration 35, loss = 0.12763910\n",
      "Iteration 36, loss = 0.13050691\n",
      "Iteration 37, loss = 0.12516453\n",
      "Iteration 38, loss = 0.12440973\n",
      "Iteration 39, loss = 0.12644672\n",
      "Iteration 40, loss = 0.12475196\n",
      "Iteration 41, loss = 0.12301953\n",
      "Iteration 42, loss = 0.12456237\n",
      "Iteration 43, loss = 0.11895292\n",
      "Iteration 44, loss = 0.12260095\n",
      "Iteration 45, loss = 0.11961652\n",
      "Iteration 46, loss = 0.11847605\n",
      "Iteration 47, loss = 0.11934114\n",
      "Iteration 48, loss = 0.11585294\n",
      "Iteration 49, loss = 0.11617471\n",
      "Iteration 50, loss = 0.11723837\n",
      "Iteration 51, loss = 0.11683749\n",
      "Iteration 52, loss = 0.11308768\n",
      "Iteration 53, loss = 0.11484638\n",
      "Iteration 54, loss = 0.11304556\n",
      "Iteration 55, loss = 0.11248175\n",
      "Iteration 56, loss = 0.11307641\n",
      "Iteration 57, loss = 0.11122721\n",
      "Iteration 58, loss = 0.11417510\n",
      "Iteration 59, loss = 0.11254530\n",
      "Iteration 60, loss = 0.10905721\n",
      "Iteration 61, loss = 0.11133357\n",
      "Iteration 62, loss = 0.10936983\n",
      "Iteration 63, loss = 0.10961700\n",
      "Iteration 64, loss = 0.10742665\n",
      "Iteration 65, loss = 0.11064126\n",
      "Iteration 66, loss = 0.10856279\n",
      "Iteration 67, loss = 0.10691096\n",
      "Iteration 68, loss = 0.10557789\n",
      "Iteration 69, loss = 0.10824367\n",
      "Iteration 70, loss = 0.10676646\n",
      "Iteration 71, loss = 0.10542800\n",
      "Iteration 72, loss = 0.10678433\n",
      "Iteration 73, loss = 0.10554140\n",
      "Iteration 74, loss = 0.10584077\n",
      "Iteration 75, loss = 0.10481631\n",
      "Iteration 76, loss = 0.10471429\n",
      "Iteration 77, loss = 0.10361627\n",
      "Iteration 78, loss = 0.10310737\n",
      "Iteration 79, loss = 0.10355810\n",
      "Iteration 80, loss = 0.10295109\n",
      "Iteration 81, loss = 0.10376244\n",
      "Iteration 82, loss = 0.10307433\n",
      "Iteration 83, loss = 0.10371219\n",
      "Iteration 84, loss = 0.10041267\n",
      "Iteration 85, loss = 0.10505241\n",
      "Iteration 86, loss = 0.10296131\n",
      "Iteration 87, loss = 0.10026205\n",
      "Iteration 88, loss = 0.10056689\n",
      "Iteration 89, loss = 0.10226085\n",
      "Iteration 90, loss = 0.10040277\n",
      "Iteration 91, loss = 0.09963332\n",
      "Iteration 92, loss = 0.10088711\n",
      "Iteration 93, loss = 0.10100509\n",
      "Iteration 94, loss = 0.09975172\n",
      "Iteration 95, loss = 0.09934424\n",
      "Iteration 96, loss = 0.09800460\n",
      "Iteration 97, loss = 0.10264492\n",
      "Iteration 98, loss = 0.09901449\n",
      "Iteration 99, loss = 0.09766312\n",
      "Iteration 100, loss = 0.09715486\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kileymack/opt/anaconda3/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 1.85483652\n",
      "Iteration 2, loss = 0.38828490\n",
      "Iteration 3, loss = 0.32313285\n",
      "Iteration 4, loss = 0.29032141\n",
      "Iteration 5, loss = 0.26490751\n",
      "Iteration 6, loss = 0.24618751\n",
      "Iteration 7, loss = 0.23547346\n",
      "Iteration 8, loss = 0.22140848\n",
      "Iteration 9, loss = 0.21422447\n",
      "Iteration 10, loss = 0.20713531\n",
      "Iteration 11, loss = 0.20023228\n",
      "Iteration 12, loss = 0.19176431\n",
      "Iteration 13, loss = 0.18949049\n",
      "Iteration 14, loss = 0.18367251\n",
      "Iteration 15, loss = 0.17910822\n",
      "Iteration 16, loss = 0.17489221\n",
      "Iteration 17, loss = 0.17398324\n",
      "Iteration 18, loss = 0.17072590\n",
      "Iteration 19, loss = 0.16652264\n",
      "Iteration 20, loss = 0.16549070\n",
      "Iteration 21, loss = 0.16233017\n",
      "Iteration 22, loss = 0.16053522\n",
      "Iteration 23, loss = 0.15686104\n",
      "Iteration 24, loss = 0.15423817\n",
      "Iteration 25, loss = 0.15384192\n",
      "Iteration 26, loss = 0.15315142\n",
      "Iteration 27, loss = 0.15092553\n",
      "Iteration 28, loss = 0.14953008\n",
      "Iteration 29, loss = 0.14683047\n",
      "Iteration 30, loss = 0.14631482\n",
      "Iteration 31, loss = 0.14536189\n",
      "Iteration 32, loss = 0.14343077\n",
      "Iteration 33, loss = 0.14309345\n",
      "Iteration 34, loss = 0.14108443\n",
      "Iteration 35, loss = 0.13981096\n",
      "Iteration 36, loss = 0.13879040\n",
      "Iteration 37, loss = 0.13765456\n",
      "Iteration 38, loss = 0.13747752\n",
      "Iteration 39, loss = 0.13661612\n",
      "Iteration 40, loss = 0.13389711\n",
      "Iteration 41, loss = 0.13600064\n",
      "Iteration 42, loss = 0.13255333\n",
      "Iteration 43, loss = 0.13335499\n",
      "Iteration 44, loss = 0.13227518\n",
      "Iteration 45, loss = 0.13118556\n",
      "Iteration 46, loss = 0.13027505\n",
      "Iteration 47, loss = 0.12936939\n",
      "Iteration 48, loss = 0.12996099\n",
      "Iteration 49, loss = 0.12787750\n",
      "Iteration 50, loss = 0.12715985\n",
      "Iteration 51, loss = 0.12836914\n",
      "Iteration 52, loss = 0.12718527\n",
      "Iteration 53, loss = 0.12573180\n",
      "Iteration 54, loss = 0.12667544\n",
      "Iteration 55, loss = 0.12449885\n",
      "Iteration 56, loss = 0.12610125\n",
      "Iteration 57, loss = 0.12411438\n",
      "Iteration 58, loss = 0.12386353\n",
      "Iteration 59, loss = 0.12422909\n",
      "Iteration 60, loss = 0.12306038\n",
      "Iteration 61, loss = 0.12210395\n",
      "Iteration 62, loss = 0.12068688\n",
      "Iteration 63, loss = 0.12128870\n",
      "Iteration 64, loss = 0.12181274\n",
      "Iteration 65, loss = 0.12240890\n",
      "Iteration 66, loss = 0.11942675\n",
      "Iteration 67, loss = 0.11997152\n",
      "Iteration 68, loss = 0.12021689\n",
      "Iteration 69, loss = 0.12018874\n",
      "Iteration 70, loss = 0.11846816\n",
      "Iteration 71, loss = 0.12023189\n",
      "Iteration 72, loss = 0.11792768\n",
      "Iteration 73, loss = 0.12010913\n",
      "Iteration 74, loss = 0.11714590\n",
      "Iteration 75, loss = 0.11798797\n",
      "Iteration 76, loss = 0.11611934\n",
      "Iteration 77, loss = 0.11631966\n",
      "Iteration 78, loss = 0.11709276\n",
      "Iteration 79, loss = 0.11728862\n",
      "Iteration 80, loss = 0.11679253\n",
      "Iteration 81, loss = 0.11555371\n",
      "Iteration 82, loss = 0.11649673\n",
      "Iteration 83, loss = 0.11567849\n",
      "Iteration 84, loss = 0.11586297\n",
      "Iteration 85, loss = 0.11475662\n",
      "Iteration 86, loss = 0.11662176\n",
      "Iteration 87, loss = 0.11313622\n",
      "Iteration 88, loss = 0.11485808\n",
      "Iteration 89, loss = 0.11363688\n",
      "Iteration 90, loss = 0.11492364\n",
      "Iteration 91, loss = 0.11392307\n",
      "Iteration 92, loss = 0.11356734\n",
      "Iteration 93, loss = 0.11404613\n",
      "Iteration 94, loss = 0.11306863\n",
      "Iteration 95, loss = 0.11252615\n",
      "Iteration 96, loss = 0.11388088\n",
      "Iteration 97, loss = 0.11295816\n",
      "Iteration 98, loss = 0.11284609\n",
      "Iteration 99, loss = 0.11190334\n",
      "Iteration 100, loss = 0.11230997\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kileymack/opt/anaconda3/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 1.55520877\n",
      "Iteration 2, loss = 0.45153631\n",
      "Iteration 3, loss = 0.37597383\n",
      "Iteration 4, loss = 0.33895131\n",
      "Iteration 5, loss = 0.31459111\n",
      "Iteration 6, loss = 0.29660611\n",
      "Iteration 7, loss = 0.28251802\n",
      "Iteration 8, loss = 0.27048592\n",
      "Iteration 9, loss = 0.26073982\n",
      "Iteration 10, loss = 0.25215779\n",
      "Iteration 11, loss = 0.24532254\n",
      "Iteration 12, loss = 0.23849721\n",
      "Iteration 13, loss = 0.23253743\n",
      "Iteration 14, loss = 0.22801788\n",
      "Iteration 15, loss = 0.22242953\n",
      "Iteration 16, loss = 0.21830013\n",
      "Iteration 17, loss = 0.21502738\n",
      "Iteration 18, loss = 0.21132621\n",
      "Iteration 19, loss = 0.20718891\n",
      "Iteration 20, loss = 0.20436733\n",
      "Iteration 21, loss = 0.20115835\n",
      "Iteration 22, loss = 0.19825876\n",
      "Iteration 23, loss = 0.19555682\n",
      "Iteration 24, loss = 0.19278328\n",
      "Iteration 25, loss = 0.19106485\n",
      "Iteration 26, loss = 0.18800234\n",
      "Iteration 27, loss = 0.18556278\n",
      "Iteration 28, loss = 0.18365760\n",
      "Iteration 29, loss = 0.18129077\n",
      "Iteration 30, loss = 0.17948847\n",
      "Iteration 31, loss = 0.17711758\n",
      "Iteration 32, loss = 0.17600897\n",
      "Iteration 33, loss = 0.17378269\n",
      "Iteration 34, loss = 0.17220199\n",
      "Iteration 35, loss = 0.17034956\n",
      "Iteration 36, loss = 0.16905691\n",
      "Iteration 37, loss = 0.16771959\n",
      "Iteration 38, loss = 0.16588095\n",
      "Iteration 39, loss = 0.16522234\n",
      "Iteration 40, loss = 0.16347085\n",
      "Iteration 41, loss = 0.16138039\n",
      "Iteration 42, loss = 0.16021938\n",
      "Iteration 43, loss = 0.15878747\n",
      "Iteration 44, loss = 0.15803007\n",
      "Iteration 45, loss = 0.15678759\n",
      "Iteration 46, loss = 0.15555646\n",
      "Iteration 47, loss = 0.15398052\n",
      "Iteration 48, loss = 0.15238866\n",
      "Iteration 49, loss = 0.15158716\n",
      "Iteration 50, loss = 0.15028156\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kileymack/opt/anaconda3/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (50) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 1.18446270\n",
      "Iteration 2, loss = 0.43440252\n",
      "Iteration 3, loss = 0.37597126\n",
      "Iteration 4, loss = 0.34324044\n",
      "Iteration 5, loss = 0.31993371\n",
      "Iteration 6, loss = 0.30265833\n",
      "Iteration 7, loss = 0.28857308\n",
      "Iteration 8, loss = 0.27574162\n",
      "Iteration 9, loss = 0.26503788\n",
      "Iteration 10, loss = 0.25571917\n",
      "Iteration 11, loss = 0.24765400\n",
      "Iteration 12, loss = 0.24018936\n",
      "Iteration 13, loss = 0.23344656\n",
      "Iteration 14, loss = 0.22758198\n",
      "Iteration 15, loss = 0.22254878\n",
      "Iteration 16, loss = 0.21782889\n",
      "Iteration 17, loss = 0.21347235\n",
      "Iteration 18, loss = 0.20927169\n",
      "Iteration 19, loss = 0.20558477\n",
      "Iteration 20, loss = 0.20167788\n",
      "Iteration 21, loss = 0.19877371\n",
      "Iteration 22, loss = 0.19574245\n",
      "Iteration 23, loss = 0.19289674\n",
      "Iteration 24, loss = 0.19013027\n",
      "Iteration 25, loss = 0.18756741\n",
      "Iteration 26, loss = 0.18540903\n",
      "Iteration 27, loss = 0.18309951\n",
      "Iteration 28, loss = 0.18044146\n",
      "Iteration 29, loss = 0.17851433\n",
      "Iteration 30, loss = 0.17630124\n",
      "Iteration 31, loss = 0.17428966\n",
      "Iteration 32, loss = 0.17274904\n",
      "Iteration 33, loss = 0.17051523\n",
      "Iteration 34, loss = 0.16871222\n",
      "Iteration 35, loss = 0.16685536\n",
      "Iteration 36, loss = 0.16547939\n",
      "Iteration 37, loss = 0.16382840\n",
      "Iteration 38, loss = 0.16220581\n",
      "Iteration 39, loss = 0.16076547\n",
      "Iteration 40, loss = 0.15915559\n",
      "Iteration 41, loss = 0.15811441\n",
      "Iteration 42, loss = 0.15643836\n",
      "Iteration 43, loss = 0.15509720\n",
      "Iteration 44, loss = 0.15370473\n",
      "Iteration 45, loss = 0.15251920\n",
      "Iteration 46, loss = 0.15141485\n",
      "Iteration 47, loss = 0.15015645\n",
      "Iteration 48, loss = 0.14911739\n",
      "Iteration 49, loss = 0.14778458\n",
      "Iteration 50, loss = 0.14655624\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kileymack/opt/anaconda3/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (50) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 1.83016396\n",
      "Iteration 2, loss = 0.39187184\n",
      "Iteration 3, loss = 0.31382096\n",
      "Iteration 4, loss = 0.27864189\n",
      "Iteration 5, loss = 0.26196771\n",
      "Iteration 6, loss = 0.24810105\n",
      "Iteration 7, loss = 0.23341762\n",
      "Iteration 8, loss = 0.22134124\n",
      "Iteration 9, loss = 0.21100152\n",
      "Iteration 10, loss = 0.20447277\n",
      "Iteration 11, loss = 0.19815122\n",
      "Iteration 12, loss = 0.19287148\n",
      "Iteration 13, loss = 0.18453294\n",
      "Iteration 14, loss = 0.18191539\n",
      "Iteration 15, loss = 0.17677003\n",
      "Iteration 16, loss = 0.17177320\n",
      "Iteration 17, loss = 0.17191890\n",
      "Iteration 18, loss = 0.16370683\n",
      "Iteration 19, loss = 0.16294258\n",
      "Iteration 20, loss = 0.16012759\n",
      "Iteration 21, loss = 0.15491247\n",
      "Iteration 22, loss = 0.15397076\n",
      "Iteration 23, loss = 0.14801935\n",
      "Iteration 24, loss = 0.14850437\n",
      "Iteration 25, loss = 0.14540434\n",
      "Iteration 26, loss = 0.14636679\n",
      "Iteration 27, loss = 0.14103024\n",
      "Iteration 28, loss = 0.13938007\n",
      "Iteration 29, loss = 0.13808698\n",
      "Iteration 30, loss = 0.13711884\n",
      "Iteration 31, loss = 0.13572909\n",
      "Iteration 32, loss = 0.13447188\n",
      "Iteration 33, loss = 0.13298500\n",
      "Iteration 34, loss = 0.13054258\n",
      "Iteration 35, loss = 0.12763910\n",
      "Iteration 36, loss = 0.13050691\n",
      "Iteration 37, loss = 0.12516453\n",
      "Iteration 38, loss = 0.12440973\n",
      "Iteration 39, loss = 0.12644672\n",
      "Iteration 40, loss = 0.12475196\n",
      "Iteration 41, loss = 0.12301953\n",
      "Iteration 42, loss = 0.12456237\n",
      "Iteration 43, loss = 0.11895292\n",
      "Iteration 44, loss = 0.12260095\n",
      "Iteration 45, loss = 0.11961652\n",
      "Iteration 46, loss = 0.11847605\n",
      "Iteration 47, loss = 0.11934114\n",
      "Iteration 48, loss = 0.11585294\n",
      "Iteration 49, loss = 0.11617471\n",
      "Iteration 50, loss = 0.11723837\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kileymack/opt/anaconda3/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (50) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 1.85483652\n",
      "Iteration 2, loss = 0.38828490\n",
      "Iteration 3, loss = 0.32313285\n",
      "Iteration 4, loss = 0.29032141\n",
      "Iteration 5, loss = 0.26490751\n",
      "Iteration 6, loss = 0.24618751\n",
      "Iteration 7, loss = 0.23547346\n",
      "Iteration 8, loss = 0.22140848\n",
      "Iteration 9, loss = 0.21422447\n",
      "Iteration 10, loss = 0.20713531\n",
      "Iteration 11, loss = 0.20023228\n",
      "Iteration 12, loss = 0.19176431\n",
      "Iteration 13, loss = 0.18949049\n",
      "Iteration 14, loss = 0.18367251\n",
      "Iteration 15, loss = 0.17910822\n",
      "Iteration 16, loss = 0.17489221\n",
      "Iteration 17, loss = 0.17398324\n",
      "Iteration 18, loss = 0.17072590\n",
      "Iteration 19, loss = 0.16652264\n",
      "Iteration 20, loss = 0.16549070\n",
      "Iteration 21, loss = 0.16233017\n",
      "Iteration 22, loss = 0.16053522\n",
      "Iteration 23, loss = 0.15686104\n",
      "Iteration 24, loss = 0.15423817\n",
      "Iteration 25, loss = 0.15384192\n",
      "Iteration 26, loss = 0.15315142\n",
      "Iteration 27, loss = 0.15092553\n",
      "Iteration 28, loss = 0.14953008\n",
      "Iteration 29, loss = 0.14683047\n",
      "Iteration 30, loss = 0.14631482\n",
      "Iteration 31, loss = 0.14536189\n",
      "Iteration 32, loss = 0.14343077\n",
      "Iteration 33, loss = 0.14309345\n",
      "Iteration 34, loss = 0.14108443\n",
      "Iteration 35, loss = 0.13981096\n",
      "Iteration 36, loss = 0.13879040\n",
      "Iteration 37, loss = 0.13765456\n",
      "Iteration 38, loss = 0.13747752\n",
      "Iteration 39, loss = 0.13661612\n",
      "Iteration 40, loss = 0.13389711\n",
      "Iteration 41, loss = 0.13600064\n",
      "Iteration 42, loss = 0.13255333\n",
      "Iteration 43, loss = 0.13335499\n",
      "Iteration 44, loss = 0.13227518\n",
      "Iteration 45, loss = 0.13118556\n",
      "Iteration 46, loss = 0.13027505\n",
      "Iteration 47, loss = 0.12936939\n",
      "Iteration 48, loss = 0.12996099\n",
      "Iteration 49, loss = 0.12787750\n",
      "Iteration 50, loss = 0.12715985\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kileymack/opt/anaconda3/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (50) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 1.55520877\n",
      "Iteration 2, loss = 0.45153631\n",
      "Iteration 3, loss = 0.37597383\n",
      "Iteration 4, loss = 0.33895131\n",
      "Iteration 5, loss = 0.31459111\n",
      "Iteration 6, loss = 0.29660611\n",
      "Iteration 7, loss = 0.28251802\n",
      "Iteration 8, loss = 0.27048592\n",
      "Iteration 9, loss = 0.26073982\n",
      "Iteration 10, loss = 0.25215779\n",
      "Iteration 11, loss = 0.24532254\n",
      "Iteration 12, loss = 0.23849721\n",
      "Iteration 13, loss = 0.23253743\n",
      "Iteration 14, loss = 0.22801788\n",
      "Iteration 15, loss = 0.22242953\n",
      "Iteration 16, loss = 0.21830013\n",
      "Iteration 17, loss = 0.21502738\n",
      "Iteration 18, loss = 0.21132621\n",
      "Iteration 19, loss = 0.20718891\n",
      "Iteration 20, loss = 0.20436733\n",
      "Iteration 21, loss = 0.20115835\n",
      "Iteration 22, loss = 0.19825876\n",
      "Iteration 23, loss = 0.19555682\n",
      "Iteration 24, loss = 0.19278328\n",
      "Iteration 25, loss = 0.19106485\n",
      "Iteration 26, loss = 0.18800234\n",
      "Iteration 27, loss = 0.18556278\n",
      "Iteration 28, loss = 0.18365760\n",
      "Iteration 29, loss = 0.18129077\n",
      "Iteration 30, loss = 0.17948847\n",
      "Iteration 31, loss = 0.17711758\n",
      "Iteration 32, loss = 0.17600897\n",
      "Iteration 33, loss = 0.17378269\n",
      "Iteration 34, loss = 0.17220199\n",
      "Iteration 35, loss = 0.17034956\n",
      "Iteration 36, loss = 0.16905691\n",
      "Iteration 37, loss = 0.16771959\n",
      "Iteration 38, loss = 0.16588095\n",
      "Iteration 39, loss = 0.16522234\n",
      "Iteration 40, loss = 0.16347085\n",
      "Iteration 41, loss = 0.16138039\n",
      "Iteration 42, loss = 0.16021938\n",
      "Iteration 43, loss = 0.15878747\n",
      "Iteration 44, loss = 0.15803007\n",
      "Iteration 45, loss = 0.15678759\n",
      "Iteration 46, loss = 0.15555646\n",
      "Iteration 47, loss = 0.15398052\n",
      "Iteration 48, loss = 0.15238866\n",
      "Iteration 49, loss = 0.15158716\n",
      "Iteration 50, loss = 0.15028156\n",
      "Iteration 51, loss = 0.14995747\n",
      "Iteration 52, loss = 0.14793493\n",
      "Iteration 53, loss = 0.14743229\n",
      "Iteration 54, loss = 0.14620606\n",
      "Iteration 55, loss = 0.14604837\n",
      "Iteration 56, loss = 0.14505930\n",
      "Iteration 57, loss = 0.14403168\n",
      "Iteration 58, loss = 0.14277950\n",
      "Iteration 59, loss = 0.14284331\n",
      "Iteration 60, loss = 0.14120745\n",
      "Iteration 61, loss = 0.14007551\n",
      "Iteration 62, loss = 0.13983238\n",
      "Iteration 63, loss = 0.13875617\n",
      "Iteration 64, loss = 0.13746717\n",
      "Iteration 65, loss = 0.13675122\n",
      "Iteration 66, loss = 0.13655941\n",
      "Iteration 67, loss = 0.13544912\n",
      "Iteration 68, loss = 0.13480329\n",
      "Iteration 69, loss = 0.13406781\n",
      "Iteration 70, loss = 0.13364151\n",
      "Iteration 71, loss = 0.13272817\n",
      "Iteration 72, loss = 0.13192051\n",
      "Iteration 73, loss = 0.13146398\n",
      "Iteration 74, loss = 0.13092709\n",
      "Iteration 75, loss = 0.12951912\n",
      "Iteration 76, loss = 0.12902495\n",
      "Iteration 77, loss = 0.12832728\n",
      "Iteration 78, loss = 0.12814569\n",
      "Iteration 79, loss = 0.12760737\n",
      "Iteration 80, loss = 0.12683845\n",
      "Iteration 81, loss = 0.12625905\n",
      "Iteration 82, loss = 0.12583044\n",
      "Iteration 83, loss = 0.12557327\n",
      "Iteration 84, loss = 0.12468138\n",
      "Iteration 85, loss = 0.12379146\n",
      "Iteration 86, loss = 0.12317361\n",
      "Iteration 87, loss = 0.12243240\n",
      "Iteration 88, loss = 0.12248064\n",
      "Iteration 89, loss = 0.12218826\n",
      "Iteration 90, loss = 0.12145821\n",
      "Iteration 91, loss = 0.12122165\n",
      "Iteration 92, loss = 0.12001836\n",
      "Iteration 93, loss = 0.11981187\n",
      "Iteration 94, loss = 0.11950840\n",
      "Iteration 95, loss = 0.11837660\n",
      "Iteration 96, loss = 0.11831481\n",
      "Iteration 97, loss = 0.11852025\n",
      "Iteration 98, loss = 0.11737017\n",
      "Iteration 99, loss = 0.11672990\n",
      "Iteration 100, loss = 0.11599236\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kileymack/opt/anaconda3/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 1.18446270\n",
      "Iteration 2, loss = 0.43440252\n",
      "Iteration 3, loss = 0.37597126\n",
      "Iteration 4, loss = 0.34324044\n",
      "Iteration 5, loss = 0.31993371\n",
      "Iteration 6, loss = 0.30265833\n",
      "Iteration 7, loss = 0.28857308\n",
      "Iteration 8, loss = 0.27574162\n",
      "Iteration 9, loss = 0.26503788\n",
      "Iteration 10, loss = 0.25571917\n",
      "Iteration 11, loss = 0.24765400\n",
      "Iteration 12, loss = 0.24018936\n",
      "Iteration 13, loss = 0.23344656\n",
      "Iteration 14, loss = 0.22758198\n",
      "Iteration 15, loss = 0.22254878\n",
      "Iteration 16, loss = 0.21782889\n",
      "Iteration 17, loss = 0.21347235\n",
      "Iteration 18, loss = 0.20927169\n",
      "Iteration 19, loss = 0.20558477\n",
      "Iteration 20, loss = 0.20167788\n",
      "Iteration 21, loss = 0.19877371\n",
      "Iteration 22, loss = 0.19574245\n",
      "Iteration 23, loss = 0.19289674\n",
      "Iteration 24, loss = 0.19013027\n",
      "Iteration 25, loss = 0.18756741\n",
      "Iteration 26, loss = 0.18540903\n",
      "Iteration 27, loss = 0.18309951\n",
      "Iteration 28, loss = 0.18044146\n",
      "Iteration 29, loss = 0.17851433\n",
      "Iteration 30, loss = 0.17630124\n",
      "Iteration 31, loss = 0.17428966\n",
      "Iteration 32, loss = 0.17274904\n",
      "Iteration 33, loss = 0.17051523\n",
      "Iteration 34, loss = 0.16871222\n",
      "Iteration 35, loss = 0.16685536\n",
      "Iteration 36, loss = 0.16547939\n",
      "Iteration 37, loss = 0.16382840\n",
      "Iteration 38, loss = 0.16220581\n",
      "Iteration 39, loss = 0.16076547\n",
      "Iteration 40, loss = 0.15915559\n",
      "Iteration 41, loss = 0.15811441\n",
      "Iteration 42, loss = 0.15643836\n",
      "Iteration 43, loss = 0.15509720\n",
      "Iteration 44, loss = 0.15370473\n",
      "Iteration 45, loss = 0.15251920\n",
      "Iteration 46, loss = 0.15141485\n",
      "Iteration 47, loss = 0.15015645\n",
      "Iteration 48, loss = 0.14911739\n",
      "Iteration 49, loss = 0.14778458\n",
      "Iteration 50, loss = 0.14655624\n",
      "Iteration 51, loss = 0.14551076\n",
      "Iteration 52, loss = 0.14431147\n",
      "Iteration 53, loss = 0.14354065\n",
      "Iteration 54, loss = 0.14235198\n",
      "Iteration 55, loss = 0.14121427\n",
      "Iteration 56, loss = 0.14081735\n",
      "Iteration 57, loss = 0.13963580\n",
      "Iteration 58, loss = 0.13883788\n",
      "Iteration 59, loss = 0.13762190\n",
      "Iteration 60, loss = 0.13673800\n",
      "Iteration 61, loss = 0.13622766\n",
      "Iteration 62, loss = 0.13545964\n",
      "Iteration 63, loss = 0.13438046\n",
      "Iteration 64, loss = 0.13348285\n",
      "Iteration 65, loss = 0.13249386\n",
      "Iteration 66, loss = 0.13198602\n",
      "Iteration 67, loss = 0.13102710\n",
      "Iteration 68, loss = 0.13048873\n",
      "Iteration 69, loss = 0.13002494\n",
      "Iteration 70, loss = 0.12890924\n",
      "Iteration 71, loss = 0.12825126\n",
      "Iteration 72, loss = 0.12758023\n",
      "Iteration 73, loss = 0.12699828\n",
      "Iteration 74, loss = 0.12600189\n",
      "Iteration 75, loss = 0.12511301\n",
      "Iteration 76, loss = 0.12481296\n",
      "Iteration 77, loss = 0.12418866\n",
      "Iteration 78, loss = 0.12367908\n",
      "Iteration 79, loss = 0.12306286\n",
      "Iteration 80, loss = 0.12255170\n",
      "Iteration 81, loss = 0.12158655\n",
      "Iteration 82, loss = 0.12114072\n",
      "Iteration 83, loss = 0.12048730\n",
      "Iteration 84, loss = 0.11962847\n",
      "Iteration 85, loss = 0.11938874\n",
      "Iteration 86, loss = 0.11861154\n",
      "Iteration 87, loss = 0.11784213\n",
      "Iteration 88, loss = 0.11782427\n",
      "Iteration 89, loss = 0.11728654\n",
      "Iteration 90, loss = 0.11676280\n",
      "Iteration 91, loss = 0.11617852\n",
      "Iteration 92, loss = 0.11554679\n",
      "Iteration 93, loss = 0.11494701\n",
      "Iteration 94, loss = 0.11451707\n",
      "Iteration 95, loss = 0.11403911\n",
      "Iteration 96, loss = 0.11356972\n",
      "Iteration 97, loss = 0.11298385\n",
      "Iteration 98, loss = 0.11269306\n",
      "Iteration 99, loss = 0.11197519\n",
      "Iteration 100, loss = 0.11159417\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kileymack/opt/anaconda3/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 1.83016396\n",
      "Iteration 2, loss = 0.39187184\n",
      "Iteration 3, loss = 0.31382096\n",
      "Iteration 4, loss = 0.27864189\n",
      "Iteration 5, loss = 0.26196771\n",
      "Iteration 6, loss = 0.24810105\n",
      "Iteration 7, loss = 0.23341762\n",
      "Iteration 8, loss = 0.22134124\n",
      "Iteration 9, loss = 0.21100152\n",
      "Iteration 10, loss = 0.20447277\n",
      "Iteration 11, loss = 0.19815122\n",
      "Iteration 12, loss = 0.19287148\n",
      "Iteration 13, loss = 0.18453294\n",
      "Iteration 14, loss = 0.18191539\n",
      "Iteration 15, loss = 0.17677003\n",
      "Iteration 16, loss = 0.17177320\n",
      "Iteration 17, loss = 0.17191890\n",
      "Iteration 18, loss = 0.16370683\n",
      "Iteration 19, loss = 0.16294258\n",
      "Iteration 20, loss = 0.16012759\n",
      "Iteration 21, loss = 0.15491247\n",
      "Iteration 22, loss = 0.15397076\n",
      "Iteration 23, loss = 0.14801935\n",
      "Iteration 24, loss = 0.14850437\n",
      "Iteration 25, loss = 0.14540434\n",
      "Iteration 26, loss = 0.14636679\n",
      "Iteration 27, loss = 0.14103024\n",
      "Iteration 28, loss = 0.13938007\n",
      "Iteration 29, loss = 0.13808698\n",
      "Iteration 30, loss = 0.13711884\n",
      "Iteration 31, loss = 0.13572909\n",
      "Iteration 32, loss = 0.13447188\n",
      "Iteration 33, loss = 0.13298500\n",
      "Iteration 34, loss = 0.13054258\n",
      "Iteration 35, loss = 0.12763910\n",
      "Iteration 36, loss = 0.13050691\n",
      "Iteration 37, loss = 0.12516453\n",
      "Iteration 38, loss = 0.12440973\n",
      "Iteration 39, loss = 0.12644672\n",
      "Iteration 40, loss = 0.12475196\n",
      "Iteration 41, loss = 0.12301953\n",
      "Iteration 42, loss = 0.12456237\n",
      "Iteration 43, loss = 0.11895292\n",
      "Iteration 44, loss = 0.12260095\n",
      "Iteration 45, loss = 0.11961652\n",
      "Iteration 46, loss = 0.11847605\n",
      "Iteration 47, loss = 0.11934114\n",
      "Iteration 48, loss = 0.11585294\n",
      "Iteration 49, loss = 0.11617471\n",
      "Iteration 50, loss = 0.11723837\n",
      "Iteration 51, loss = 0.11683749\n",
      "Iteration 52, loss = 0.11308768\n",
      "Iteration 53, loss = 0.11484638\n",
      "Iteration 54, loss = 0.11304556\n",
      "Iteration 55, loss = 0.11248175\n",
      "Iteration 56, loss = 0.11307641\n",
      "Iteration 57, loss = 0.11122721\n",
      "Iteration 58, loss = 0.11417510\n",
      "Iteration 59, loss = 0.11254530\n",
      "Iteration 60, loss = 0.10905721\n",
      "Iteration 61, loss = 0.11133357\n",
      "Iteration 62, loss = 0.10936983\n",
      "Iteration 63, loss = 0.10961700\n",
      "Iteration 64, loss = 0.10742665\n",
      "Iteration 65, loss = 0.11064126\n",
      "Iteration 66, loss = 0.10856279\n",
      "Iteration 67, loss = 0.10691096\n",
      "Iteration 68, loss = 0.10557789\n",
      "Iteration 69, loss = 0.10824367\n",
      "Iteration 70, loss = 0.10676646\n",
      "Iteration 71, loss = 0.10542800\n",
      "Iteration 72, loss = 0.10678433\n",
      "Iteration 73, loss = 0.10554140\n",
      "Iteration 74, loss = 0.10584077\n",
      "Iteration 75, loss = 0.10481631\n",
      "Iteration 76, loss = 0.10471429\n",
      "Iteration 77, loss = 0.10361627\n",
      "Iteration 78, loss = 0.10310737\n",
      "Iteration 79, loss = 0.10355810\n",
      "Iteration 80, loss = 0.10295109\n",
      "Iteration 81, loss = 0.10376244\n",
      "Iteration 82, loss = 0.10307433\n",
      "Iteration 83, loss = 0.10371219\n",
      "Iteration 84, loss = 0.10041267\n",
      "Iteration 85, loss = 0.10505241\n",
      "Iteration 86, loss = 0.10296131\n",
      "Iteration 87, loss = 0.10026205\n",
      "Iteration 88, loss = 0.10056689\n",
      "Iteration 89, loss = 0.10226085\n",
      "Iteration 90, loss = 0.10040277\n",
      "Iteration 91, loss = 0.09963332\n",
      "Iteration 92, loss = 0.10088711\n",
      "Iteration 93, loss = 0.10100509\n",
      "Iteration 94, loss = 0.09975172\n",
      "Iteration 95, loss = 0.09934424\n",
      "Iteration 96, loss = 0.09800460\n",
      "Iteration 97, loss = 0.10264492\n",
      "Iteration 98, loss = 0.09901449\n",
      "Iteration 99, loss = 0.09766312\n",
      "Iteration 100, loss = 0.09715486\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kileymack/opt/anaconda3/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 1.85483652\n",
      "Iteration 2, loss = 0.38828490\n",
      "Iteration 3, loss = 0.32313285\n",
      "Iteration 4, loss = 0.29032141\n",
      "Iteration 5, loss = 0.26490751\n",
      "Iteration 6, loss = 0.24618751\n",
      "Iteration 7, loss = 0.23547346\n",
      "Iteration 8, loss = 0.22140848\n",
      "Iteration 9, loss = 0.21422447\n",
      "Iteration 10, loss = 0.20713531\n",
      "Iteration 11, loss = 0.20023228\n",
      "Iteration 12, loss = 0.19176431\n",
      "Iteration 13, loss = 0.18949049\n",
      "Iteration 14, loss = 0.18367251\n",
      "Iteration 15, loss = 0.17910822\n",
      "Iteration 16, loss = 0.17489221\n",
      "Iteration 17, loss = 0.17398324\n",
      "Iteration 18, loss = 0.17072590\n",
      "Iteration 19, loss = 0.16652264\n",
      "Iteration 20, loss = 0.16549070\n",
      "Iteration 21, loss = 0.16233017\n",
      "Iteration 22, loss = 0.16053522\n",
      "Iteration 23, loss = 0.15686104\n",
      "Iteration 24, loss = 0.15423817\n",
      "Iteration 25, loss = 0.15384192\n",
      "Iteration 26, loss = 0.15315142\n",
      "Iteration 27, loss = 0.15092553\n",
      "Iteration 28, loss = 0.14953008\n",
      "Iteration 29, loss = 0.14683047\n",
      "Iteration 30, loss = 0.14631482\n",
      "Iteration 31, loss = 0.14536189\n",
      "Iteration 32, loss = 0.14343077\n",
      "Iteration 33, loss = 0.14309345\n",
      "Iteration 34, loss = 0.14108443\n",
      "Iteration 35, loss = 0.13981096\n",
      "Iteration 36, loss = 0.13879040\n",
      "Iteration 37, loss = 0.13765456\n",
      "Iteration 38, loss = 0.13747752\n",
      "Iteration 39, loss = 0.13661612\n",
      "Iteration 40, loss = 0.13389711\n",
      "Iteration 41, loss = 0.13600064\n",
      "Iteration 42, loss = 0.13255333\n",
      "Iteration 43, loss = 0.13335499\n",
      "Iteration 44, loss = 0.13227518\n",
      "Iteration 45, loss = 0.13118556\n",
      "Iteration 46, loss = 0.13027505\n",
      "Iteration 47, loss = 0.12936939\n",
      "Iteration 48, loss = 0.12996099\n",
      "Iteration 49, loss = 0.12787750\n",
      "Iteration 50, loss = 0.12715985\n",
      "Iteration 51, loss = 0.12836914\n",
      "Iteration 52, loss = 0.12718527\n",
      "Iteration 53, loss = 0.12573180\n",
      "Iteration 54, loss = 0.12667544\n",
      "Iteration 55, loss = 0.12449885\n",
      "Iteration 56, loss = 0.12610125\n",
      "Iteration 57, loss = 0.12411438\n",
      "Iteration 58, loss = 0.12386353\n",
      "Iteration 59, loss = 0.12422909\n",
      "Iteration 60, loss = 0.12306038\n",
      "Iteration 61, loss = 0.12210395\n",
      "Iteration 62, loss = 0.12068688\n",
      "Iteration 63, loss = 0.12128870\n",
      "Iteration 64, loss = 0.12181274\n",
      "Iteration 65, loss = 0.12240890\n",
      "Iteration 66, loss = 0.11942675\n",
      "Iteration 67, loss = 0.11997152\n",
      "Iteration 68, loss = 0.12021689\n",
      "Iteration 69, loss = 0.12018874\n",
      "Iteration 70, loss = 0.11846816\n",
      "Iteration 71, loss = 0.12023189\n",
      "Iteration 72, loss = 0.11792768\n",
      "Iteration 73, loss = 0.12010913\n",
      "Iteration 74, loss = 0.11714590\n",
      "Iteration 75, loss = 0.11798797\n",
      "Iteration 76, loss = 0.11611934\n",
      "Iteration 77, loss = 0.11631966\n",
      "Iteration 78, loss = 0.11709276\n",
      "Iteration 79, loss = 0.11728862\n",
      "Iteration 80, loss = 0.11679253\n",
      "Iteration 81, loss = 0.11555371\n",
      "Iteration 82, loss = 0.11649673\n",
      "Iteration 83, loss = 0.11567849\n",
      "Iteration 84, loss = 0.11586297\n",
      "Iteration 85, loss = 0.11475662\n",
      "Iteration 86, loss = 0.11662176\n",
      "Iteration 87, loss = 0.11313622\n",
      "Iteration 88, loss = 0.11485808\n",
      "Iteration 89, loss = 0.11363688\n",
      "Iteration 90, loss = 0.11492364\n",
      "Iteration 91, loss = 0.11392307\n",
      "Iteration 92, loss = 0.11356734\n",
      "Iteration 93, loss = 0.11404613\n",
      "Iteration 94, loss = 0.11306863\n",
      "Iteration 95, loss = 0.11252615\n",
      "Iteration 96, loss = 0.11388088\n",
      "Iteration 97, loss = 0.11295816\n",
      "Iteration 98, loss = 0.11284609\n",
      "Iteration 99, loss = 0.11190334\n",
      "Iteration 100, loss = 0.11230997\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kileymack/opt/anaconda3/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.93978834\n",
      "Iteration 2, loss = 0.36664269\n",
      "Iteration 3, loss = 0.31068436\n",
      "Iteration 4, loss = 0.27868344\n",
      "Iteration 5, loss = 0.25643965\n",
      "Iteration 6, loss = 0.23894696\n",
      "Iteration 7, loss = 0.22455648\n",
      "Iteration 8, loss = 0.21285328\n",
      "Iteration 9, loss = 0.20333737\n",
      "Iteration 10, loss = 0.19399545\n",
      "Iteration 11, loss = 0.18638647\n",
      "Iteration 12, loss = 0.18001324\n",
      "Iteration 13, loss = 0.17400492\n",
      "Iteration 14, loss = 0.16829215\n",
      "Iteration 15, loss = 0.16307033\n",
      "Iteration 16, loss = 0.15859751\n",
      "Iteration 17, loss = 0.15438820\n",
      "Iteration 18, loss = 0.15019366\n",
      "Iteration 19, loss = 0.14638070\n",
      "Iteration 20, loss = 0.14303218\n",
      "Iteration 21, loss = 0.13965797\n",
      "Iteration 22, loss = 0.13671035\n",
      "Iteration 23, loss = 0.13398846\n",
      "Iteration 24, loss = 0.13132780\n",
      "Iteration 25, loss = 0.12867221\n",
      "Iteration 26, loss = 0.12651217\n",
      "Iteration 27, loss = 0.12380565\n",
      "Iteration 28, loss = 0.12194231\n",
      "Iteration 29, loss = 0.11978015\n",
      "Iteration 30, loss = 0.11794056\n",
      "Iteration 31, loss = 0.11604094\n",
      "Iteration 32, loss = 0.11428387\n",
      "Iteration 33, loss = 0.11237646\n",
      "Iteration 34, loss = 0.11075323\n",
      "Iteration 35, loss = 0.10897822\n",
      "Iteration 36, loss = 0.10764940\n",
      "Iteration 37, loss = 0.10617475\n",
      "Iteration 38, loss = 0.10469851\n",
      "Iteration 39, loss = 0.10333451\n",
      "Iteration 40, loss = 0.10204618\n",
      "Iteration 41, loss = 0.10095883\n",
      "Iteration 42, loss = 0.09963241\n",
      "Iteration 43, loss = 0.09819366\n",
      "Iteration 44, loss = 0.09720834\n",
      "Iteration 45, loss = 0.09595727\n",
      "Iteration 46, loss = 0.09487856\n",
      "Iteration 47, loss = 0.09370193\n",
      "Iteration 48, loss = 0.09307163\n",
      "Iteration 49, loss = 0.09195403\n",
      "Iteration 50, loss = 0.09083477\n",
      "Iteration 51, loss = 0.08992010\n",
      "Iteration 52, loss = 0.08901070\n",
      "Iteration 53, loss = 0.08820012\n",
      "Iteration 54, loss = 0.08722144\n",
      "Iteration 55, loss = 0.08650753\n",
      "Iteration 56, loss = 0.08561910\n",
      "Iteration 57, loss = 0.08497802\n",
      "Iteration 58, loss = 0.08419831\n",
      "Iteration 59, loss = 0.08330165\n",
      "Iteration 60, loss = 0.08260518\n",
      "Iteration 61, loss = 0.08177415\n",
      "Iteration 62, loss = 0.08117502\n",
      "Iteration 63, loss = 0.08046565\n",
      "Iteration 64, loss = 0.07970842\n",
      "Iteration 65, loss = 0.07911283\n",
      "Iteration 66, loss = 0.07833590\n",
      "Iteration 67, loss = 0.07767858\n",
      "Iteration 68, loss = 0.07709969\n",
      "Iteration 69, loss = 0.07646224\n",
      "Iteration 70, loss = 0.07580653\n",
      "Iteration 71, loss = 0.07523470\n",
      "Iteration 72, loss = 0.07474650\n",
      "Iteration 73, loss = 0.07436336\n",
      "Iteration 74, loss = 0.07368287\n",
      "Iteration 75, loss = 0.07305408\n",
      "Iteration 76, loss = 0.07259542\n",
      "Iteration 77, loss = 0.07180740\n",
      "Iteration 78, loss = 0.07135705\n",
      "Iteration 79, loss = 0.07095710\n",
      "Iteration 80, loss = 0.07067529\n",
      "Iteration 81, loss = 0.07002464\n",
      "Iteration 82, loss = 0.06945759\n",
      "Iteration 83, loss = 0.06900549\n",
      "Iteration 84, loss = 0.06864334\n",
      "Iteration 85, loss = 0.06810920\n",
      "Iteration 86, loss = 0.06766026\n",
      "Iteration 87, loss = 0.06732871\n",
      "Iteration 88, loss = 0.06690201\n",
      "Iteration 89, loss = 0.06630075\n",
      "Iteration 90, loss = 0.06601103\n",
      "Iteration 91, loss = 0.06561740\n",
      "Iteration 92, loss = 0.06520549\n",
      "Iteration 93, loss = 0.06488310\n",
      "Iteration 94, loss = 0.06443979\n",
      "Iteration 95, loss = 0.06404998\n",
      "Iteration 96, loss = 0.06363112\n",
      "Iteration 97, loss = 0.06321895\n",
      "Iteration 98, loss = 0.06278369\n",
      "Iteration 99, loss = 0.06269502\n",
      "Iteration 100, loss = 0.06213193\n",
      "CPU times: user 1d 23h 58min 4s, sys: 1h 25min 58s, total: 2d 1h 24min 3s\n",
      "Wall time: 8h 37min 2s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kileymack/opt/anaconda3/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=TimeSeriesSplit(gap=0, max_train_size=None, n_splits=2, test_size=None),\n",
       "             estimator=MLPRegressor(random_state=32, verbose=1),\n",
       "             param_grid={'activation': ['tanh', 'relu'],\n",
       "                         'alpha': [0.0001, 0.05],\n",
       "                         'hidden_layer_sizes': [(150, 100, 50), (120, 80, 40),\n",
       "                                                (100, 50, 30)],\n",
       "                         'learning_rate': ['constant', 'adaptive'],\n",
       "                         'max_iter': [50, 100], 'solver': ['sgd', 'adam']})"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "tscv = TimeSeriesSplit(n_splits=2)\n",
    "\n",
    "MLPgrid = GridSearchCV(MLPmodel, param_grid, cv=tscv)\n",
    "MLPgrid.fit(X_trainscaled, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a5c268a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'activation': 'relu',\n",
       " 'alpha': 0.0001,\n",
       " 'hidden_layer_sizes': (150, 100, 50),\n",
       " 'learning_rate': 'constant',\n",
       " 'max_iter': 100,\n",
       " 'solver': 'sgd'}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MLPgrid.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "31e89b45",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "#Instantiate\n",
    "MLPmodel2 = MLPRegressor(verbose = 1, random_state = 32, activation = 'relu', alpha = 0.0001, \\\n",
    "                       hidden_layer_sizes = (150, 100, 50), learning_rate = 'constant'\\\n",
    "                        max_iter = 200, solver = 'sgd')\n",
    "#Fit Model\n",
    "MLPmodel2.fit(X_trainscaled, y_train)\n",
    "print(MLPmodel2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fefecb5e",
   "metadata": {},
   "source": [
    "### MLPRegressor Grid Search -  Best Parameters and Run Time\n",
    "\n",
    "- Best Parameters\n",
    "{'activation': 'relu',\n",
    " 'alpha': 0.0001,\n",
    " 'hidden_layer_sizes': (150, 100, 50),\n",
    " 'learning_rate': 'constant',\n",
    " 'max_iter': 100,\n",
    " 'solver': 'sgd'}\n",
    "\n",
    "-  Run Time: 8h 37min 2s\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "72b8d100",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gather grid search predictions\n",
    "grid_pred = MLPgrid.predict(X_testscaled) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "067cb619",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gather results in dataframe for visualization of expected vs. predictions\n",
    "results_MLP_grid = pd.DataFrame(data = {'Actual':y_test, \\\n",
    "                                   'Predictions':grid_pred}, index=y_test.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b6de884c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Actual</th>\n",
       "      <th>Predictions</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>startdate</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2016-05-02</th>\n",
       "      <td>21.080032</td>\n",
       "      <td>21.303924</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-05-02</th>\n",
       "      <td>12.921798</td>\n",
       "      <td>13.245692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-05-02</th>\n",
       "      <td>11.742004</td>\n",
       "      <td>11.297715</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-05-02</th>\n",
       "      <td>18.386656</td>\n",
       "      <td>17.989621</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-05-02</th>\n",
       "      <td>10.771266</td>\n",
       "      <td>10.771513</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-08-31</th>\n",
       "      <td>19.772009</td>\n",
       "      <td>16.822504</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-08-31</th>\n",
       "      <td>19.998930</td>\n",
       "      <td>17.165045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-08-31</th>\n",
       "      <td>20.392469</td>\n",
       "      <td>17.283563</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-08-31</th>\n",
       "      <td>10.406187</td>\n",
       "      <td>11.432712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-08-31</th>\n",
       "      <td>15.910995</td>\n",
       "      <td>14.843682</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>62622 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               Actual  Predictions\n",
       "startdate                         \n",
       "2016-05-02  21.080032    21.303924\n",
       "2016-05-02  12.921798    13.245692\n",
       "2016-05-02  11.742004    11.297715\n",
       "2016-05-02  18.386656    17.989621\n",
       "2016-05-02  10.771266    10.771513\n",
       "...               ...          ...\n",
       "2016-08-31  19.772009    16.822504\n",
       "2016-08-31  19.998930    17.165045\n",
       "2016-08-31  20.392469    17.283563\n",
       "2016-08-31  10.406187    11.432712\n",
       "2016-08-31  15.910995    14.843682\n",
       "\n",
       "[62622 rows x 2 columns]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_MLP_grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "65c443f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R-squared = 0.754\n",
      "RMSE = 2.783\n"
     ]
    }
   ],
   "source": [
    "# Error Metrics for MLP Grid Search\n",
    "print('R-squared = {:.3f}'.format(r2_score(results_MLP_grid['Actual'],results_MLP_grid['Predictions'])))\n",
    "print('RMSE = {:.3f}'.format(sqrt(mean_squared_error(results_MLP_grid['Actual'],results_MLP_grid['Predictions'])))) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb50568d",
   "metadata": {},
   "source": [
    "### MLP Grid Search Evaluation \n",
    "R-squared = 0.754\n",
    "RMSE = 2.783\n",
    "\n",
    "The grid search proved to have poorer scores than the default MLPRegresssor.  We will try out the optimized grid search parameters with 200 iterrations and see if the metrics improve. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "54ed8391",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.93978834\n",
      "Iteration 2, loss = 0.36664269\n",
      "Iteration 3, loss = 0.31068436\n",
      "Iteration 4, loss = 0.27868344\n",
      "Iteration 5, loss = 0.25643965\n",
      "Iteration 6, loss = 0.23894696\n",
      "Iteration 7, loss = 0.22455648\n",
      "Iteration 8, loss = 0.21285328\n",
      "Iteration 9, loss = 0.20333737\n",
      "Iteration 10, loss = 0.19399545\n",
      "Iteration 11, loss = 0.18638647\n",
      "Iteration 12, loss = 0.18001324\n",
      "Iteration 13, loss = 0.17400492\n",
      "Iteration 14, loss = 0.16829215\n",
      "Iteration 15, loss = 0.16307033\n",
      "Iteration 16, loss = 0.15859751\n",
      "Iteration 17, loss = 0.15438820\n",
      "Iteration 18, loss = 0.15019366\n",
      "Iteration 19, loss = 0.14638070\n",
      "Iteration 20, loss = 0.14303218\n",
      "Iteration 21, loss = 0.13965797\n",
      "Iteration 22, loss = 0.13671035\n",
      "Iteration 23, loss = 0.13398846\n",
      "Iteration 24, loss = 0.13132780\n",
      "Iteration 25, loss = 0.12867221\n",
      "Iteration 26, loss = 0.12651217\n",
      "Iteration 27, loss = 0.12380565\n",
      "Iteration 28, loss = 0.12194231\n",
      "Iteration 29, loss = 0.11978015\n",
      "Iteration 30, loss = 0.11794056\n",
      "Iteration 31, loss = 0.11604094\n",
      "Iteration 32, loss = 0.11428387\n",
      "Iteration 33, loss = 0.11237646\n",
      "Iteration 34, loss = 0.11075323\n",
      "Iteration 35, loss = 0.10897822\n",
      "Iteration 36, loss = 0.10764940\n",
      "Iteration 37, loss = 0.10617475\n",
      "Iteration 38, loss = 0.10469851\n",
      "Iteration 39, loss = 0.10333451\n",
      "Iteration 40, loss = 0.10204618\n",
      "Iteration 41, loss = 0.10095883\n",
      "Iteration 42, loss = 0.09963241\n",
      "Iteration 43, loss = 0.09819366\n",
      "Iteration 44, loss = 0.09720834\n",
      "Iteration 45, loss = 0.09595727\n",
      "Iteration 46, loss = 0.09487856\n",
      "Iteration 47, loss = 0.09370193\n",
      "Iteration 48, loss = 0.09307163\n",
      "Iteration 49, loss = 0.09195403\n",
      "Iteration 50, loss = 0.09083477\n",
      "Iteration 51, loss = 0.08992010\n",
      "Iteration 52, loss = 0.08901070\n",
      "Iteration 53, loss = 0.08820012\n",
      "Iteration 54, loss = 0.08722144\n",
      "Iteration 55, loss = 0.08650753\n",
      "Iteration 56, loss = 0.08561910\n",
      "Iteration 57, loss = 0.08497802\n",
      "Iteration 58, loss = 0.08419831\n",
      "Iteration 59, loss = 0.08330165\n",
      "Iteration 60, loss = 0.08260518\n",
      "Iteration 61, loss = 0.08177415\n",
      "Iteration 62, loss = 0.08117502\n",
      "Iteration 63, loss = 0.08046565\n",
      "Iteration 64, loss = 0.07970842\n",
      "Iteration 65, loss = 0.07911283\n",
      "Iteration 66, loss = 0.07833590\n",
      "Iteration 67, loss = 0.07767858\n",
      "Iteration 68, loss = 0.07709969\n",
      "Iteration 69, loss = 0.07646224\n",
      "Iteration 70, loss = 0.07580653\n",
      "Iteration 71, loss = 0.07523470\n",
      "Iteration 72, loss = 0.07474650\n",
      "Iteration 73, loss = 0.07436336\n",
      "Iteration 74, loss = 0.07368287\n",
      "Iteration 75, loss = 0.07305408\n",
      "Iteration 76, loss = 0.07259542\n",
      "Iteration 77, loss = 0.07180740\n",
      "Iteration 78, loss = 0.07135705\n",
      "Iteration 79, loss = 0.07095710\n",
      "Iteration 80, loss = 0.07067529\n",
      "Iteration 81, loss = 0.07002464\n",
      "Iteration 82, loss = 0.06945759\n",
      "Iteration 83, loss = 0.06900549\n",
      "Iteration 84, loss = 0.06864334\n",
      "Iteration 85, loss = 0.06810920\n",
      "Iteration 86, loss = 0.06766026\n",
      "Iteration 87, loss = 0.06732871\n",
      "Iteration 88, loss = 0.06690201\n",
      "Iteration 89, loss = 0.06630075\n",
      "Iteration 90, loss = 0.06601103\n",
      "Iteration 91, loss = 0.06561740\n",
      "Iteration 92, loss = 0.06520549\n",
      "Iteration 93, loss = 0.06488310\n",
      "Iteration 94, loss = 0.06443979\n",
      "Iteration 95, loss = 0.06404998\n",
      "Iteration 96, loss = 0.06363112\n",
      "Iteration 97, loss = 0.06321895\n",
      "Iteration 98, loss = 0.06278369\n",
      "Iteration 99, loss = 0.06269502\n",
      "Iteration 100, loss = 0.06213193\n",
      "Iteration 101, loss = 0.06196479\n",
      "Iteration 102, loss = 0.06164666\n",
      "Iteration 103, loss = 0.06113473\n",
      "Iteration 104, loss = 0.06078501\n",
      "Iteration 105, loss = 0.06060282\n",
      "Iteration 106, loss = 0.06032986\n",
      "Iteration 107, loss = 0.05997126\n",
      "Iteration 108, loss = 0.05969640\n",
      "Iteration 109, loss = 0.05941975\n",
      "Iteration 110, loss = 0.05903298\n",
      "Iteration 111, loss = 0.05887884\n",
      "Iteration 112, loss = 0.05849439\n",
      "Iteration 113, loss = 0.05819520\n",
      "Iteration 114, loss = 0.05797428\n",
      "Iteration 115, loss = 0.05759235\n",
      "Iteration 116, loss = 0.05732471\n",
      "Iteration 117, loss = 0.05705141\n",
      "Iteration 118, loss = 0.05680362\n",
      "Iteration 119, loss = 0.05660600\n",
      "Iteration 120, loss = 0.05632556\n",
      "Iteration 121, loss = 0.05592323\n",
      "Iteration 122, loss = 0.05577633\n",
      "Iteration 123, loss = 0.05556058\n",
      "Iteration 124, loss = 0.05535124\n",
      "Iteration 125, loss = 0.05504981\n",
      "Iteration 126, loss = 0.05472987\n",
      "Iteration 127, loss = 0.05465903\n",
      "Iteration 128, loss = 0.05439519\n",
      "Iteration 129, loss = 0.05400848\n",
      "Iteration 130, loss = 0.05391480\n",
      "Iteration 131, loss = 0.05362991\n",
      "Iteration 132, loss = 0.05335481\n",
      "Iteration 133, loss = 0.05336378\n",
      "Iteration 134, loss = 0.05312028\n",
      "Iteration 135, loss = 0.05283715\n",
      "Iteration 136, loss = 0.05268788\n",
      "Iteration 137, loss = 0.05248252\n",
      "Iteration 138, loss = 0.05219867\n",
      "Iteration 139, loss = 0.05197603\n",
      "Iteration 140, loss = 0.05176484\n",
      "Iteration 141, loss = 0.05148551\n",
      "Iteration 142, loss = 0.05136494\n",
      "Iteration 143, loss = 0.05115191\n",
      "Iteration 144, loss = 0.05090323\n",
      "Iteration 145, loss = 0.05090097\n",
      "Iteration 146, loss = 0.05060390\n",
      "Iteration 147, loss = 0.05034267\n",
      "Iteration 148, loss = 0.05029297\n",
      "Iteration 149, loss = 0.04987506\n",
      "Iteration 150, loss = 0.04991937\n",
      "Iteration 151, loss = 0.04959585\n",
      "Iteration 152, loss = 0.04942595\n",
      "Iteration 153, loss = 0.04936190\n",
      "Iteration 154, loss = 0.04916208\n",
      "Iteration 155, loss = 0.04895357\n",
      "Iteration 156, loss = 0.04879786\n",
      "Iteration 157, loss = 0.04862182\n",
      "Iteration 158, loss = 0.04842527\n",
      "Iteration 159, loss = 0.04825594\n",
      "Iteration 160, loss = 0.04836664\n",
      "Iteration 161, loss = 0.04807375\n",
      "Iteration 162, loss = 0.04795424\n",
      "Iteration 163, loss = 0.04773726\n",
      "Iteration 164, loss = 0.04739279\n",
      "Iteration 165, loss = 0.04735320\n",
      "Iteration 166, loss = 0.04718601\n",
      "Iteration 167, loss = 0.04709746\n",
      "Iteration 168, loss = 0.04688906\n",
      "Iteration 169, loss = 0.04671930\n",
      "Iteration 170, loss = 0.04666795\n",
      "Iteration 171, loss = 0.04652074\n",
      "Iteration 172, loss = 0.04623822\n",
      "Iteration 173, loss = 0.04615613\n",
      "Iteration 174, loss = 0.04603813\n",
      "Iteration 175, loss = 0.04597242\n",
      "Iteration 176, loss = 0.04571087\n",
      "Iteration 177, loss = 0.04566112\n",
      "Iteration 178, loss = 0.04546162\n",
      "Iteration 179, loss = 0.04524879\n",
      "Iteration 180, loss = 0.04517729\n",
      "Iteration 181, loss = 0.04506823\n",
      "Iteration 182, loss = 0.04492935\n",
      "Iteration 183, loss = 0.04492001\n",
      "Iteration 184, loss = 0.04468217\n",
      "Iteration 185, loss = 0.04458527\n",
      "Iteration 186, loss = 0.04435923\n",
      "Iteration 187, loss = 0.04436573\n",
      "Iteration 188, loss = 0.04414504\n",
      "Iteration 189, loss = 0.04405233\n",
      "Iteration 190, loss = 0.04385438\n",
      "Iteration 191, loss = 0.04371042\n",
      "Iteration 192, loss = 0.04375990\n",
      "Iteration 193, loss = 0.04356766\n",
      "Iteration 194, loss = 0.04339095\n",
      "Iteration 195, loss = 0.04334318\n",
      "Iteration 196, loss = 0.04324076\n",
      "Iteration 197, loss = 0.04297672\n",
      "Iteration 198, loss = 0.04289550\n",
      "Iteration 199, loss = 0.04292981\n",
      "Iteration 200, loss = 0.04278101\n",
      "MLPRegressor(hidden_layer_sizes=(150, 100, 50), random_state=32, solver='sgd',\n",
      "             verbose=1)\n",
      "CPU times: user 1h 47min 50s, sys: 3min 5s, total: 1h 50min 55s\n",
      "Wall time: 19min 24s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kileymack/opt/anaconda3/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#Instantiate\n",
    "MLPmodel2 = MLPRegressor(verbose = 1, random_state = 32, activation = 'relu', alpha = 0.0001, \\\n",
    "                       hidden_layer_sizes = (150, 100, 50), learning_rate = 'constant',\\\n",
    "                        max_iter = 200, solver = 'sgd')\n",
    "#Fit Model\n",
    "MLPmodel2.fit(X_trainscaled, y_train)\n",
    "print(MLPmodel2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "cbc76d4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gather grid search predictions\n",
    "pred2 = MLPmodel2.predict(X_testscaled) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "0574a924",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gather results in dataframe for visualization of expected vs. predictions\n",
    "results_MLP2 = pd.DataFrame(data = {'Actual':y_test, \\\n",
    "                                   'Predictions':pred2}, index=y_test.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "cd6583bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Actual</th>\n",
       "      <th>Predictions</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>startdate</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2016-05-02</th>\n",
       "      <td>21.080032</td>\n",
       "      <td>21.276929</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-05-02</th>\n",
       "      <td>12.921798</td>\n",
       "      <td>13.328666</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-05-02</th>\n",
       "      <td>11.742004</td>\n",
       "      <td>11.743602</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-05-02</th>\n",
       "      <td>18.386656</td>\n",
       "      <td>17.927709</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-05-02</th>\n",
       "      <td>10.771266</td>\n",
       "      <td>10.915823</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-08-31</th>\n",
       "      <td>19.772009</td>\n",
       "      <td>16.797451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-08-31</th>\n",
       "      <td>19.998930</td>\n",
       "      <td>17.183268</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-08-31</th>\n",
       "      <td>20.392469</td>\n",
       "      <td>17.279610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-08-31</th>\n",
       "      <td>10.406187</td>\n",
       "      <td>10.420368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-08-31</th>\n",
       "      <td>15.910995</td>\n",
       "      <td>14.193742</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>62622 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               Actual  Predictions\n",
       "startdate                         \n",
       "2016-05-02  21.080032    21.276929\n",
       "2016-05-02  12.921798    13.328666\n",
       "2016-05-02  11.742004    11.743602\n",
       "2016-05-02  18.386656    17.927709\n",
       "2016-05-02  10.771266    10.915823\n",
       "...               ...          ...\n",
       "2016-08-31  19.772009    16.797451\n",
       "2016-08-31  19.998930    17.183268\n",
       "2016-08-31  20.392469    17.279610\n",
       "2016-08-31  10.406187    10.420368\n",
       "2016-08-31  15.910995    14.193742\n",
       "\n",
       "[62622 rows x 2 columns]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_MLP2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "fc09db60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R-squared = 0.720\n",
      "RMSE = 2.966\n"
     ]
    }
   ],
   "source": [
    "# Error Metrics for MLP2\n",
    "print('R-squared = {:.3f}'.format(r2_score(results_MLP2['Actual'],results_MLP2['Predictions'])))\n",
    "print('RMSE = {:.3f}'.format(sqrt(mean_squared_error(results_MLP2['Actual'],results_MLP2['Predictions'])))) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf7787fb",
   "metadata": {},
   "source": [
    "### MLPRegressor - 200 Iterations with \"Optimized Parameters\"\n",
    "- R-squared = 0.720\n",
    "\n",
    "- RMSE = 2.966\n",
    "\n",
    "Our metrics did not improve, lending more evidence that the default MLPRegressor to be superior with this dataset. \n",
    "\n",
    "We also ran some different length testing sets to see how our error scores would be affected. With this small sample size,  we see our R-squared score increase with the longer test set but the RMSE score decrease.  The test set for the datathon submission is only two months long, and our RSME could improve upon submission with the shorter testing window. \n",
    "\n",
    "The MLPRegressor is scoring lower than our RandomForestRegressor, and we did not have to scale the data.  Our next step will be to compose a submission for the Datathon with the RFR model since it has consistently performed better than the MLP Regressor.  This submission will give use a baseline to improve from. \n",
    "\n",
    "We will then have to look to feature engineering and/or other regression models to further improve upon our modeling error metrics in subsequent notebooks. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99fea4fb",
   "metadata": {},
   "source": [
    "## RFR - Submission\n",
    "- We will fit our RFR with a train/test split of the WiDS `training_data` with the optimized parameters from the grid search in Notebook 1.  \n",
    "- We evaluate on the test split to make sure things transferred well to this notebook.\n",
    "- Finally, we will then fit the RFR on the complete `training_data` and make predictions on the WiDS `test_data` and reformat for a submission to the datathon."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "35519c17",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>lat</th>\n",
       "      <th>lon</th>\n",
       "      <th>contest-pevpr-sfc-gauss-14d__pevpr</th>\n",
       "      <th>nmme0-tmp2m-34w__cancm30</th>\n",
       "      <th>nmme0-tmp2m-34w__cancm40</th>\n",
       "      <th>nmme0-tmp2m-34w__ccsm30</th>\n",
       "      <th>nmme0-tmp2m-34w__ccsm40</th>\n",
       "      <th>nmme0-tmp2m-34w__cfsv20</th>\n",
       "      <th>nmme0-tmp2m-34w__gfdlflora0</th>\n",
       "      <th>...</th>\n",
       "      <th>Csb</th>\n",
       "      <th>Dfa</th>\n",
       "      <th>Dfb</th>\n",
       "      <th>Dfc</th>\n",
       "      <th>Dsb</th>\n",
       "      <th>Dsc</th>\n",
       "      <th>Dwa</th>\n",
       "      <th>Dwb</th>\n",
       "      <th>month_number</th>\n",
       "      <th>season_number</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>startdate</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2016-05-02</th>\n",
       "      <td>95639</td>\n",
       "      <td>0.409091</td>\n",
       "      <td>0.266667</td>\n",
       "      <td>374.589996</td>\n",
       "      <td>13.440000</td>\n",
       "      <td>14.210000</td>\n",
       "      <td>11.840000</td>\n",
       "      <td>15.240000</td>\n",
       "      <td>16.790001</td>\n",
       "      <td>13.580000</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-05-02</th>\n",
       "      <td>212599</td>\n",
       "      <td>0.636364</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>293.670013</td>\n",
       "      <td>10.670000</td>\n",
       "      <td>12.210000</td>\n",
       "      <td>10.260000</td>\n",
       "      <td>13.070000</td>\n",
       "      <td>9.980000</td>\n",
       "      <td>10.160000</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-05-02</th>\n",
       "      <td>184090</td>\n",
       "      <td>0.590909</td>\n",
       "      <td>0.466667</td>\n",
       "      <td>197.000000</td>\n",
       "      <td>3.580000</td>\n",
       "      <td>6.840000</td>\n",
       "      <td>2.620000</td>\n",
       "      <td>4.690000</td>\n",
       "      <td>7.580000</td>\n",
       "      <td>5.730000</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-05-02</th>\n",
       "      <td>116838</td>\n",
       "      <td>0.454545</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>395.859985</td>\n",
       "      <td>10.430000</td>\n",
       "      <td>11.370000</td>\n",
       "      <td>9.080000</td>\n",
       "      <td>13.540000</td>\n",
       "      <td>13.200000</td>\n",
       "      <td>11.300000</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-05-02</th>\n",
       "      <td>369764</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>221.509995</td>\n",
       "      <td>4.290000</td>\n",
       "      <td>6.690000</td>\n",
       "      <td>6.380000</td>\n",
       "      <td>8.380000</td>\n",
       "      <td>5.980000</td>\n",
       "      <td>5.370000</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-08-31</th>\n",
       "      <td>255118</td>\n",
       "      <td>0.727273</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>302.059998</td>\n",
       "      <td>24.760000</td>\n",
       "      <td>33.759998</td>\n",
       "      <td>25.900000</td>\n",
       "      <td>27.219999</td>\n",
       "      <td>22.330000</td>\n",
       "      <td>27.590000</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-08-31</th>\n",
       "      <td>255849</td>\n",
       "      <td>0.727273</td>\n",
       "      <td>0.866667</td>\n",
       "      <td>324.470001</td>\n",
       "      <td>25.049999</td>\n",
       "      <td>33.889999</td>\n",
       "      <td>26.740000</td>\n",
       "      <td>27.360001</td>\n",
       "      <td>21.520000</td>\n",
       "      <td>27.680000</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-08-31</th>\n",
       "      <td>256580</td>\n",
       "      <td>0.727273</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>326.140015</td>\n",
       "      <td>25.139999</td>\n",
       "      <td>33.590000</td>\n",
       "      <td>27.469999</td>\n",
       "      <td>27.250000</td>\n",
       "      <td>20.770000</td>\n",
       "      <td>27.469999</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-08-31</th>\n",
       "      <td>187135</td>\n",
       "      <td>0.590909</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>494.890015</td>\n",
       "      <td>21.930000</td>\n",
       "      <td>22.770000</td>\n",
       "      <td>21.430000</td>\n",
       "      <td>18.139999</td>\n",
       "      <td>14.920000</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-08-31</th>\n",
       "      <td>375733</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.866667</td>\n",
       "      <td>295.290009</td>\n",
       "      <td>23.129999</td>\n",
       "      <td>27.200001</td>\n",
       "      <td>20.250000</td>\n",
       "      <td>24.430000</td>\n",
       "      <td>18.350000</td>\n",
       "      <td>23.350000</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>62622 rows × 260 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             index       lat       lon  contest-pevpr-sfc-gauss-14d__pevpr  \\\n",
       "startdate                                                                    \n",
       "2016-05-02   95639  0.409091  0.266667                          374.589996   \n",
       "2016-05-02  212599  0.636364  0.800000                          293.670013   \n",
       "2016-05-02  184090  0.590909  0.466667                          197.000000   \n",
       "2016-05-02  116838  0.454545  0.300000                          395.859985   \n",
       "2016-05-02  369764  1.000000  0.600000                          221.509995   \n",
       "...            ...       ...       ...                                 ...   \n",
       "2016-08-31  255118  0.727273  0.833333                          302.059998   \n",
       "2016-08-31  255849  0.727273  0.866667                          324.470001   \n",
       "2016-08-31  256580  0.727273  0.900000                          326.140015   \n",
       "2016-08-31  187135  0.590909  0.600000                          494.890015   \n",
       "2016-08-31  375733  1.000000  0.866667                          295.290009   \n",
       "\n",
       "            nmme0-tmp2m-34w__cancm30  nmme0-tmp2m-34w__cancm40  \\\n",
       "startdate                                                        \n",
       "2016-05-02                 13.440000                 14.210000   \n",
       "2016-05-02                 10.670000                 12.210000   \n",
       "2016-05-02                  3.580000                  6.840000   \n",
       "2016-05-02                 10.430000                 11.370000   \n",
       "2016-05-02                  4.290000                  6.690000   \n",
       "...                              ...                       ...   \n",
       "2016-08-31                 24.760000                 33.759998   \n",
       "2016-08-31                 25.049999                 33.889999   \n",
       "2016-08-31                 25.139999                 33.590000   \n",
       "2016-08-31                 21.930000                 22.770000   \n",
       "2016-08-31                 23.129999                 27.200001   \n",
       "\n",
       "            nmme0-tmp2m-34w__ccsm30  nmme0-tmp2m-34w__ccsm40  \\\n",
       "startdate                                                      \n",
       "2016-05-02                11.840000                15.240000   \n",
       "2016-05-02                10.260000                13.070000   \n",
       "2016-05-02                 2.620000                 4.690000   \n",
       "2016-05-02                 9.080000                13.540000   \n",
       "2016-05-02                 6.380000                 8.380000   \n",
       "...                             ...                      ...   \n",
       "2016-08-31                25.900000                27.219999   \n",
       "2016-08-31                26.740000                27.360001   \n",
       "2016-08-31                27.469999                27.250000   \n",
       "2016-08-31                21.430000                18.139999   \n",
       "2016-08-31                20.250000                24.430000   \n",
       "\n",
       "            nmme0-tmp2m-34w__cfsv20  nmme0-tmp2m-34w__gfdlflora0  ...  Csb  \\\n",
       "startdate                                                         ...        \n",
       "2016-05-02                16.790001                    13.580000  ...    0   \n",
       "2016-05-02                 9.980000                    10.160000  ...    0   \n",
       "2016-05-02                 7.580000                     5.730000  ...    0   \n",
       "2016-05-02                13.200000                    11.300000  ...    0   \n",
       "2016-05-02                 5.980000                     5.370000  ...    0   \n",
       "...                             ...                          ...  ...  ...   \n",
       "2016-08-31                22.330000                    27.590000  ...    0   \n",
       "2016-08-31                21.520000                    27.680000  ...    0   \n",
       "2016-08-31                20.770000                    27.469999  ...    0   \n",
       "2016-08-31                14.920000                    15.000000  ...    0   \n",
       "2016-08-31                18.350000                    23.350000  ...    0   \n",
       "\n",
       "            Dfa  Dfb  Dfc  Dsb  Dsc  Dwa  Dwb  month_number  season_number  \n",
       "startdate                                                                   \n",
       "2016-05-02    0    0    0    0    0    0    0             5              2  \n",
       "2016-05-02    1    0    0    0    0    0    0             5              2  \n",
       "2016-05-02    0    0    0    0    0    0    0             5              2  \n",
       "2016-05-02    0    0    0    0    0    0    0             5              2  \n",
       "2016-05-02    0    0    0    0    0    0    0             5              2  \n",
       "...         ...  ...  ...  ...  ...  ...  ...           ...            ...  \n",
       "2016-08-31    1    0    0    0    0    0    0             8              3  \n",
       "2016-08-31    1    0    0    0    0    0    0             8              3  \n",
       "2016-08-31    1    0    0    0    0    0    0             8              3  \n",
       "2016-08-31    0    0    1    0    0    0    0             8              3  \n",
       "2016-08-31    0    1    0    0    0    0    0             8              3  \n",
       "\n",
       "[62622 rows x 260 columns]"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Check X_test global variable -- Look good\n",
    "X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "920be8ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import model and scoring metric\n",
    "from sklearn.ensemble import RandomForestRegressor "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "3cf03e71",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Instatiate\n",
    "RFRmodel = RandomForestRegressor(n_estimators=100, max_features = None, \\\n",
    "                                 verbose = 1, random_state=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "95485e71",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1h 4min 58s, sys: 19.8 s, total: 1h 5min 18s\n",
      "Wall time: 1h 5min 43s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed: 65.7min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomForestRegressor(max_features=None, random_state=32, verbose=1)"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "#Fit Model \n",
    "RFRmodel.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "f6acd00d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:    4.6s finished\n"
     ]
    }
   ],
   "source": [
    "# Get Predictions from RFR Model\n",
    "pred = RFRmodel.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "8e92945d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gather results in dataframe for visualization of expected vs. predictions\n",
    "results_RFR = pd.DataFrame(data = {'Actual':y_test, \\\n",
    "                                   'Predictions':pred}, index=y_test.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "3a1f673b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Actual</th>\n",
       "      <th>Predictions</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>startdate</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2016-05-02</th>\n",
       "      <td>21.080032</td>\n",
       "      <td>20.437980</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-05-02</th>\n",
       "      <td>12.921798</td>\n",
       "      <td>13.407156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-05-02</th>\n",
       "      <td>11.742004</td>\n",
       "      <td>11.323933</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-05-02</th>\n",
       "      <td>18.386656</td>\n",
       "      <td>17.999544</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-05-02</th>\n",
       "      <td>10.771266</td>\n",
       "      <td>11.368966</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-08-31</th>\n",
       "      <td>19.772009</td>\n",
       "      <td>19.208867</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-08-31</th>\n",
       "      <td>19.998930</td>\n",
       "      <td>19.483500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-08-31</th>\n",
       "      <td>20.392469</td>\n",
       "      <td>19.578225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-08-31</th>\n",
       "      <td>10.406187</td>\n",
       "      <td>11.230049</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-08-31</th>\n",
       "      <td>15.910995</td>\n",
       "      <td>16.410757</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>62622 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               Actual  Predictions\n",
       "startdate                         \n",
       "2016-05-02  21.080032    20.437980\n",
       "2016-05-02  12.921798    13.407156\n",
       "2016-05-02  11.742004    11.323933\n",
       "2016-05-02  18.386656    17.999544\n",
       "2016-05-02  10.771266    11.368966\n",
       "...               ...          ...\n",
       "2016-08-31  19.772009    19.208867\n",
       "2016-08-31  19.998930    19.483500\n",
       "2016-08-31  20.392469    19.578225\n",
       "2016-08-31  10.406187    11.230049\n",
       "2016-08-31  15.910995    16.410757\n",
       "\n",
       "[62622 rows x 2 columns]"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_RFR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "e4694f90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R-squared = 0.908\n",
      "RMSE = 1.696\n"
     ]
    }
   ],
   "source": [
    "# Error Metrics for initial RFR \n",
    "print('R-squared = {:.3f}'.format(r2_score(results_RFR['Actual'],results_RFR['Predictions'])))\n",
    "print('RMSE = {:.3f}'.format(sqrt(mean_squared_error(results_RFR['Actual'],results_RFR['Predictions'])))) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acb35e4d",
   "metadata": {},
   "source": [
    "The RFR is performing at pretty much the same level as in our last notebook after optimized.  We will proceed.   \n",
    "Notebook 1:\n",
    "R-squared = 0.909\n",
    "RMSE = 1.694\n",
    "\n",
    "Notebook 2:\n",
    "R-squared = 0.908\n",
    "RMSE = 1.696\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f998a67",
   "metadata": {},
   "source": [
    "### Transform Test_Data \n",
    "- Set up WiDS `test_data` with a stardate as index as we did for the `training_data` allowing our models fit on our cleaned and transformed data to run the WiDS `WiDS` test data. \n",
    "- The `test_data` from WiDS has no target variable.  Therefore, we will not split it into X and y.\n",
    "- We will predict the target and then upon submission to the WiDS Kaggle site we will know our RMSE metric.  \n",
    "- This will be our baseline score to improve upon. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "8d6e631c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in clean training set with start date as datetime index\n",
    "test_data = pd.read_csv('data/test_data_clean.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "ef4c3b5b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>lat</th>\n",
       "      <th>lon</th>\n",
       "      <th>startdate</th>\n",
       "      <th>contest-pevpr-sfc-gauss-14d__pevpr</th>\n",
       "      <th>nmme0-tmp2m-34w__cancm30</th>\n",
       "      <th>nmme0-tmp2m-34w__cancm40</th>\n",
       "      <th>nmme0-tmp2m-34w__ccsm30</th>\n",
       "      <th>nmme0-tmp2m-34w__ccsm40</th>\n",
       "      <th>nmme0-tmp2m-34w__cfsv20</th>\n",
       "      <th>...</th>\n",
       "      <th>Csb</th>\n",
       "      <th>Dfa</th>\n",
       "      <th>Dfb</th>\n",
       "      <th>Dfc</th>\n",
       "      <th>Dsb</th>\n",
       "      <th>Dsc</th>\n",
       "      <th>Dwa</th>\n",
       "      <th>Dwb</th>\n",
       "      <th>month_number</th>\n",
       "      <th>season_number</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>375734</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>2022-11-01</td>\n",
       "      <td>339.88</td>\n",
       "      <td>30.88</td>\n",
       "      <td>30.92</td>\n",
       "      <td>29.17</td>\n",
       "      <td>31.02</td>\n",
       "      <td>29.47</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>375735</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>2022-11-02</td>\n",
       "      <td>334.63</td>\n",
       "      <td>30.88</td>\n",
       "      <td>30.92</td>\n",
       "      <td>29.17</td>\n",
       "      <td>31.02</td>\n",
       "      <td>29.47</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>375736</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>2022-11-03</td>\n",
       "      <td>337.83</td>\n",
       "      <td>30.88</td>\n",
       "      <td>30.92</td>\n",
       "      <td>29.17</td>\n",
       "      <td>31.02</td>\n",
       "      <td>29.47</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>375737</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>2022-11-04</td>\n",
       "      <td>345.81</td>\n",
       "      <td>30.88</td>\n",
       "      <td>30.92</td>\n",
       "      <td>29.17</td>\n",
       "      <td>31.02</td>\n",
       "      <td>29.47</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>375738</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>2022-11-05</td>\n",
       "      <td>357.39</td>\n",
       "      <td>30.88</td>\n",
       "      <td>30.92</td>\n",
       "      <td>29.17</td>\n",
       "      <td>31.02</td>\n",
       "      <td>29.47</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 261 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    index  lat       lon   startdate  contest-pevpr-sfc-gauss-14d__pevpr  \\\n",
       "0  375734  0.0  0.833333  2022-11-01                              339.88   \n",
       "1  375735  0.0  0.833333  2022-11-02                              334.63   \n",
       "2  375736  0.0  0.833333  2022-11-03                              337.83   \n",
       "3  375737  0.0  0.833333  2022-11-04                              345.81   \n",
       "4  375738  0.0  0.833333  2022-11-05                              357.39   \n",
       "\n",
       "   nmme0-tmp2m-34w__cancm30  nmme0-tmp2m-34w__cancm40  \\\n",
       "0                     30.88                     30.92   \n",
       "1                     30.88                     30.92   \n",
       "2                     30.88                     30.92   \n",
       "3                     30.88                     30.92   \n",
       "4                     30.88                     30.92   \n",
       "\n",
       "   nmme0-tmp2m-34w__ccsm30  nmme0-tmp2m-34w__ccsm40  nmme0-tmp2m-34w__cfsv20  \\\n",
       "0                    29.17                    31.02                    29.47   \n",
       "1                    29.17                    31.02                    29.47   \n",
       "2                    29.17                    31.02                    29.47   \n",
       "3                    29.17                    31.02                    29.47   \n",
       "4                    29.17                    31.02                    29.47   \n",
       "\n",
       "   ...  Csb  Dfa  Dfb  Dfc  Dsb  Dsc  Dwa  Dwb  month_number  season_number  \n",
       "0  ...    0    0    0    0    0    0    0    0            11              4  \n",
       "1  ...    0    0    0    0    0    0    0    0            11              4  \n",
       "2  ...    0    0    0    0    0    0    0    0            11              4  \n",
       "3  ...    0    0    0    0    0    0    0    0            11              4  \n",
       "4  ...    0    0    0    0    0    0    0    0            11              4  \n",
       "\n",
       "[5 rows x 261 columns]"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's view the indices and shape of the original test set. So, we can make sure the index will realign \n",
    "# when we make our submission.\n",
    "test_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "7c2bd6cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>lat</th>\n",
       "      <th>lon</th>\n",
       "      <th>startdate</th>\n",
       "      <th>contest-pevpr-sfc-gauss-14d__pevpr</th>\n",
       "      <th>nmme0-tmp2m-34w__cancm30</th>\n",
       "      <th>nmme0-tmp2m-34w__cancm40</th>\n",
       "      <th>nmme0-tmp2m-34w__ccsm30</th>\n",
       "      <th>nmme0-tmp2m-34w__ccsm40</th>\n",
       "      <th>nmme0-tmp2m-34w__cfsv20</th>\n",
       "      <th>...</th>\n",
       "      <th>Csb</th>\n",
       "      <th>Dfa</th>\n",
       "      <th>Dfb</th>\n",
       "      <th>Dfc</th>\n",
       "      <th>Dsb</th>\n",
       "      <th>Dsc</th>\n",
       "      <th>Dwa</th>\n",
       "      <th>Dwb</th>\n",
       "      <th>month_number</th>\n",
       "      <th>season_number</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>31349</th>\n",
       "      <td>407083</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.866667</td>\n",
       "      <td>2022-12-27</td>\n",
       "      <td>62.72</td>\n",
       "      <td>4.6</td>\n",
       "      <td>8.71</td>\n",
       "      <td>6.05</td>\n",
       "      <td>10.08</td>\n",
       "      <td>6.39</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31350</th>\n",
       "      <td>407084</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.866667</td>\n",
       "      <td>2022-12-28</td>\n",
       "      <td>73.41</td>\n",
       "      <td>4.6</td>\n",
       "      <td>8.71</td>\n",
       "      <td>6.05</td>\n",
       "      <td>10.08</td>\n",
       "      <td>6.39</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31351</th>\n",
       "      <td>407085</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.866667</td>\n",
       "      <td>2022-12-29</td>\n",
       "      <td>70.00</td>\n",
       "      <td>4.6</td>\n",
       "      <td>8.71</td>\n",
       "      <td>6.05</td>\n",
       "      <td>10.08</td>\n",
       "      <td>6.39</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31352</th>\n",
       "      <td>407086</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.866667</td>\n",
       "      <td>2022-12-30</td>\n",
       "      <td>79.81</td>\n",
       "      <td>4.6</td>\n",
       "      <td>8.71</td>\n",
       "      <td>6.05</td>\n",
       "      <td>10.08</td>\n",
       "      <td>6.39</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31353</th>\n",
       "      <td>407087</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.866667</td>\n",
       "      <td>2022-12-31</td>\n",
       "      <td>86.17</td>\n",
       "      <td>4.6</td>\n",
       "      <td>8.71</td>\n",
       "      <td>6.05</td>\n",
       "      <td>10.08</td>\n",
       "      <td>6.39</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 261 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        index  lat       lon   startdate  contest-pevpr-sfc-gauss-14d__pevpr  \\\n",
       "31349  407083  1.0  0.866667  2022-12-27                               62.72   \n",
       "31350  407084  1.0  0.866667  2022-12-28                               73.41   \n",
       "31351  407085  1.0  0.866667  2022-12-29                               70.00   \n",
       "31352  407086  1.0  0.866667  2022-12-30                               79.81   \n",
       "31353  407087  1.0  0.866667  2022-12-31                               86.17   \n",
       "\n",
       "       nmme0-tmp2m-34w__cancm30  nmme0-tmp2m-34w__cancm40  \\\n",
       "31349                       4.6                      8.71   \n",
       "31350                       4.6                      8.71   \n",
       "31351                       4.6                      8.71   \n",
       "31352                       4.6                      8.71   \n",
       "31353                       4.6                      8.71   \n",
       "\n",
       "       nmme0-tmp2m-34w__ccsm30  nmme0-tmp2m-34w__ccsm40  \\\n",
       "31349                     6.05                    10.08   \n",
       "31350                     6.05                    10.08   \n",
       "31351                     6.05                    10.08   \n",
       "31352                     6.05                    10.08   \n",
       "31353                     6.05                    10.08   \n",
       "\n",
       "       nmme0-tmp2m-34w__cfsv20  ...  Csb  Dfa  Dfb  Dfc  Dsb  Dsc  Dwa  Dwb  \\\n",
       "31349                     6.39  ...    0    0    1    0    0    0    0    0   \n",
       "31350                     6.39  ...    0    0    1    0    0    0    0    0   \n",
       "31351                     6.39  ...    0    0    1    0    0    0    0    0   \n",
       "31352                     6.39  ...    0    0    1    0    0    0    0    0   \n",
       "31353                     6.39  ...    0    0    1    0    0    0    0    0   \n",
       "\n",
       "       month_number  season_number  \n",
       "31349            12              1  \n",
       "31350            12              1  \n",
       "31351            12              1  \n",
       "31352            12              1  \n",
       "31353            12              1  \n",
       "\n",
       "[5 rows x 261 columns]"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "a4224312",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(31354, 261)"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "cbc9c97c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index                                         int64\n",
      "lat                                         float64\n",
      "lon                                         float64\n",
      "startdate                                    object\n",
      "contest-pevpr-sfc-gauss-14d__pevpr          float64\n",
      "nmme0-tmp2m-34w__cancm30                    float64\n",
      "nmme0-tmp2m-34w__cancm40                    float64\n",
      "nmme0-tmp2m-34w__ccsm30                     float64\n",
      "nmme0-tmp2m-34w__ccsm40                     float64\n",
      "nmme0-tmp2m-34w__cfsv20                     float64\n",
      "nmme0-tmp2m-34w__gfdlflora0                 float64\n",
      "nmme0-tmp2m-34w__gfdlflorb0                 float64\n",
      "nmme0-tmp2m-34w__gfdl0                      float64\n",
      "nmme0-tmp2m-34w__nasa0                      float64\n",
      "nmme0-tmp2m-34w__nmme0mean                  float64\n",
      "contest-wind-h10-14d__wind-hgt-10           float64\n",
      "nmme-tmp2m-56w__cancm3                      float64\n",
      "nmme-tmp2m-56w__cancm4                      float64\n",
      "nmme-tmp2m-56w__ccsm3                       float64\n",
      "nmme-tmp2m-56w__ccsm4                       float64\n",
      "nmme-tmp2m-56w__cfsv2                       float64\n",
      "nmme-tmp2m-56w__gfdl                        float64\n",
      "nmme-tmp2m-56w__gfdlflora                   float64\n",
      "nmme-tmp2m-56w__gfdlflorb                   float64\n",
      "nmme-tmp2m-56w__nasa                        float64\n",
      "nmme-tmp2m-56w__nmmemean                    float64\n",
      "contest-rhum-sig995-14d__rhum               float64\n",
      "nmme-prate-34w__cancm3                      float64\n",
      "nmme-prate-34w__cancm4                      float64\n",
      "nmme-prate-34w__ccsm3                       float64\n",
      "nmme-prate-34w__ccsm4                       float64\n",
      "nmme-prate-34w__cfsv2                       float64\n",
      "nmme-prate-34w__gfdl                        float64\n",
      "nmme-prate-34w__gfdlflora                   float64\n",
      "nmme-prate-34w__gfdlflorb                   float64\n",
      "nmme-prate-34w__nasa                        float64\n",
      "nmme-prate-34w__nmmemean                    float64\n",
      "contest-wind-h100-14d__wind-hgt-100         float64\n",
      "nmme0-prate-56w__cancm30                    float64\n",
      "nmme0-prate-56w__cancm40                    float64\n",
      "nmme0-prate-56w__ccsm30                     float64\n",
      "nmme0-prate-56w__ccsm40                     float64\n",
      "nmme0-prate-56w__cfsv20                     float64\n",
      "nmme0-prate-56w__gfdlflora0                 float64\n",
      "nmme0-prate-56w__gfdlflorb0                 float64\n",
      "nmme0-prate-56w__gfdl0                      float64\n",
      "nmme0-prate-56w__nasa0                      float64\n",
      "nmme0-prate-56w__nmme0mean                  float64\n",
      "nmme0-prate-34w__cancm30                    float64\n",
      "nmme0-prate-34w__cancm40                    float64\n",
      "nmme0-prate-34w__ccsm30                     float64\n",
      "nmme0-prate-34w__ccsm40                     float64\n",
      "nmme0-prate-34w__cfsv20                     float64\n",
      "nmme0-prate-34w__gfdlflora0                 float64\n",
      "nmme0-prate-34w__gfdlflorb0                 float64\n",
      "nmme0-prate-34w__gfdl0                      float64\n",
      "nmme0-prate-34w__nasa0                      float64\n",
      "nmme0-prate-34w__nmme0mean                  float64\n",
      "contest-slp-14d__slp                        float64\n",
      "contest-wind-vwnd-925-14d__wind-vwnd-925    float64\n",
      "nmme-prate-56w__cancm3                      float64\n",
      "nmme-prate-56w__cancm4                      float64\n",
      "nmme-prate-56w__ccsm3                       float64\n",
      "nmme-prate-56w__ccsm4                       float64\n",
      "nmme-prate-56w__cfsv2                       float64\n",
      "nmme-prate-56w__gfdl                        float64\n",
      "nmme-prate-56w__gfdlflora                   float64\n",
      "nmme-prate-56w__gfdlflorb                   float64\n",
      "nmme-prate-56w__nasa                        float64\n",
      "nmme-prate-56w__nmmemean                    float64\n",
      "contest-pres-sfc-gauss-14d__pres            float64\n",
      "contest-wind-uwnd-250-14d__wind-uwnd-250    float64\n",
      "nmme-tmp2m-34w__cancm3                      float64\n",
      "nmme-tmp2m-34w__cancm4                      float64\n",
      "nmme-tmp2m-34w__ccsm3                       float64\n",
      "nmme-tmp2m-34w__ccsm4                       float64\n",
      "nmme-tmp2m-34w__cfsv2                       float64\n",
      "nmme-tmp2m-34w__gfdl                        float64\n",
      "nmme-tmp2m-34w__gfdlflora                   float64\n",
      "nmme-tmp2m-34w__gfdlflorb                   float64\n",
      "nmme-tmp2m-34w__nasa                        float64\n",
      "nmme-tmp2m-34w__nmmemean                    float64\n",
      "contest-prwtr-eatm-14d__prwtr               float64\n",
      "contest-wind-vwnd-250-14d__wind-vwnd-250    float64\n",
      "contest-precip-14d__precip                  float64\n",
      "contest-wind-h850-14d__wind-hgt-850         float64\n",
      "contest-wind-uwnd-925-14d__wind-uwnd-925    float64\n",
      "contest-wind-h500-14d__wind-hgt-500         float64\n",
      "cancm30                                     float64\n",
      "cancm40                                     float64\n",
      "ccsm30                                      float64\n",
      "ccsm40                                      float64\n",
      "cfsv20                                      float64\n",
      "gfdlflora0                                  float64\n",
      "gfdlflorb0                                  float64\n",
      "gfdl0                                       float64\n",
      "nasa0                                       float64\n",
      "nmme0mean                                   float64\n",
      "elevation__elevation                          int64\n",
      "wind-vwnd-250-2010-1                        float64\n",
      "wind-vwnd-250-2010-2                        float64\n",
      "wind-vwnd-250-2010-3                        float64\n",
      "wind-vwnd-250-2010-4                        float64\n",
      "wind-vwnd-250-2010-5                        float64\n",
      "wind-vwnd-250-2010-6                        float64\n",
      "wind-vwnd-250-2010-7                        float64\n",
      "wind-vwnd-250-2010-8                        float64\n",
      "wind-vwnd-250-2010-9                        float64\n",
      "wind-vwnd-250-2010-10                       float64\n",
      "wind-vwnd-250-2010-11                       float64\n",
      "wind-vwnd-250-2010-12                       float64\n",
      "wind-vwnd-250-2010-13                       float64\n",
      "wind-vwnd-250-2010-14                       float64\n",
      "wind-vwnd-250-2010-15                       float64\n",
      "wind-vwnd-250-2010-16                       float64\n",
      "wind-vwnd-250-2010-17                       float64\n",
      "wind-vwnd-250-2010-18                       float64\n",
      "wind-vwnd-250-2010-19                       float64\n",
      "wind-vwnd-250-2010-20                       float64\n",
      "wind-uwnd-250-2010-1                        float64\n",
      "wind-uwnd-250-2010-2                        float64\n",
      "wind-uwnd-250-2010-3                        float64\n",
      "wind-uwnd-250-2010-4                        float64\n",
      "wind-uwnd-250-2010-5                        float64\n",
      "wind-uwnd-250-2010-6                        float64\n",
      "wind-uwnd-250-2010-7                        float64\n",
      "wind-uwnd-250-2010-8                        float64\n",
      "wind-uwnd-250-2010-9                        float64\n",
      "wind-uwnd-250-2010-10                       float64\n",
      "wind-uwnd-250-2010-11                       float64\n",
      "wind-uwnd-250-2010-12                       float64\n",
      "wind-uwnd-250-2010-13                       float64\n",
      "wind-uwnd-250-2010-14                       float64\n",
      "wind-uwnd-250-2010-15                       float64\n",
      "wind-uwnd-250-2010-16                       float64\n",
      "wind-uwnd-250-2010-17                       float64\n",
      "wind-uwnd-250-2010-18                       float64\n",
      "wind-uwnd-250-2010-19                       float64\n",
      "wind-uwnd-250-2010-20                       float64\n",
      "mjo1d__phase                                float64\n",
      "mjo1d__amplitude                            float64\n",
      "mei__mei                                    float64\n",
      "mei__meirank                                float64\n",
      "mei__nip                                    float64\n",
      "wind-hgt-850-2010-1                         float64\n",
      "wind-hgt-850-2010-2                         float64\n",
      "wind-hgt-850-2010-3                         float64\n",
      "wind-hgt-850-2010-4                         float64\n",
      "wind-hgt-850-2010-5                         float64\n",
      "wind-hgt-850-2010-6                         float64\n",
      "wind-hgt-850-2010-7                         float64\n",
      "wind-hgt-850-2010-8                         float64\n",
      "wind-hgt-850-2010-9                         float64\n",
      "wind-hgt-850-2010-10                        float64\n",
      "sst-2010-1                                  float64\n",
      "sst-2010-2                                  float64\n",
      "sst-2010-3                                  float64\n",
      "sst-2010-4                                  float64\n",
      "sst-2010-5                                  float64\n",
      "sst-2010-6                                  float64\n",
      "sst-2010-7                                  float64\n",
      "sst-2010-8                                  float64\n",
      "sst-2010-9                                  float64\n",
      "sst-2010-10                                 float64\n",
      "wind-hgt-500-2010-1                         float64\n",
      "wind-hgt-500-2010-2                         float64\n",
      "wind-hgt-500-2010-3                         float64\n",
      "wind-hgt-500-2010-4                         float64\n",
      "wind-hgt-500-2010-5                         float64\n",
      "wind-hgt-500-2010-6                         float64\n",
      "wind-hgt-500-2010-7                         float64\n",
      "wind-hgt-500-2010-8                         float64\n",
      "wind-hgt-500-2010-9                         float64\n",
      "wind-hgt-500-2010-10                        float64\n",
      "icec-2010-1                                 float64\n",
      "icec-2010-2                                 float64\n",
      "icec-2010-3                                 float64\n",
      "icec-2010-4                                 float64\n",
      "icec-2010-5                                 float64\n",
      "icec-2010-6                                 float64\n",
      "icec-2010-7                                 float64\n",
      "icec-2010-8                                 float64\n",
      "icec-2010-9                                 float64\n",
      "icec-2010-10                                float64\n",
      "wind-uwnd-925-2010-1                        float64\n",
      "wind-uwnd-925-2010-2                        float64\n",
      "wind-uwnd-925-2010-3                        float64\n",
      "wind-uwnd-925-2010-4                        float64\n",
      "wind-uwnd-925-2010-5                        float64\n",
      "wind-uwnd-925-2010-6                        float64\n",
      "wind-uwnd-925-2010-7                        float64\n",
      "wind-uwnd-925-2010-8                        float64\n",
      "wind-uwnd-925-2010-9                        float64\n",
      "wind-uwnd-925-2010-10                       float64\n",
      "wind-uwnd-925-2010-11                       float64\n",
      "wind-uwnd-925-2010-12                       float64\n",
      "wind-uwnd-925-2010-13                       float64\n",
      "wind-uwnd-925-2010-14                       float64\n",
      "wind-uwnd-925-2010-15                       float64\n",
      "wind-uwnd-925-2010-16                       float64\n",
      "wind-uwnd-925-2010-17                       float64\n",
      "wind-uwnd-925-2010-18                       float64\n",
      "wind-uwnd-925-2010-19                       float64\n",
      "wind-uwnd-925-2010-20                       float64\n",
      "wind-hgt-10-2010-1                          float64\n",
      "wind-hgt-10-2010-2                          float64\n",
      "wind-hgt-10-2010-3                          float64\n",
      "wind-hgt-10-2010-4                          float64\n",
      "wind-hgt-10-2010-5                          float64\n",
      "wind-hgt-10-2010-6                          float64\n",
      "wind-hgt-10-2010-7                          float64\n",
      "wind-hgt-10-2010-8                          float64\n",
      "wind-hgt-10-2010-9                          float64\n",
      "wind-hgt-10-2010-10                         float64\n",
      "wind-hgt-100-2010-1                         float64\n",
      "wind-hgt-100-2010-2                         float64\n",
      "wind-hgt-100-2010-3                         float64\n",
      "wind-hgt-100-2010-4                         float64\n",
      "wind-hgt-100-2010-5                         float64\n",
      "wind-hgt-100-2010-6                         float64\n",
      "wind-hgt-100-2010-7                         float64\n",
      "wind-hgt-100-2010-8                         float64\n",
      "wind-hgt-100-2010-9                         float64\n",
      "wind-hgt-100-2010-10                        float64\n",
      "wind-vwnd-925-2010-1                        float64\n",
      "wind-vwnd-925-2010-2                        float64\n",
      "wind-vwnd-925-2010-3                        float64\n",
      "wind-vwnd-925-2010-4                        float64\n",
      "wind-vwnd-925-2010-5                        float64\n",
      "wind-vwnd-925-2010-6                        float64\n",
      "wind-vwnd-925-2010-7                        float64\n",
      "wind-vwnd-925-2010-8                        float64\n",
      "wind-vwnd-925-2010-9                        float64\n",
      "wind-vwnd-925-2010-10                       float64\n",
      "wind-vwnd-925-2010-11                       float64\n",
      "wind-vwnd-925-2010-12                       float64\n",
      "wind-vwnd-925-2010-13                       float64\n",
      "wind-vwnd-925-2010-14                       float64\n",
      "wind-vwnd-925-2010-15                       float64\n",
      "wind-vwnd-925-2010-16                       float64\n",
      "wind-vwnd-925-2010-17                       float64\n",
      "wind-vwnd-925-2010-18                       float64\n",
      "wind-vwnd-925-2010-19                       float64\n",
      "wind-vwnd-925-2010-20                       float64\n",
      "BSh                                           int64\n",
      "BSk                                           int64\n",
      "BWh                                           int64\n",
      "BWk                                           int64\n",
      "Cfa                                           int64\n",
      "Cfb                                           int64\n",
      "Csa                                           int64\n",
      "Csb                                           int64\n",
      "Dfa                                           int64\n",
      "Dfb                                           int64\n",
      "Dfc                                           int64\n",
      "Dsb                                           int64\n",
      "Dsc                                           int64\n",
      "Dwa                                           int64\n",
      "Dwb                                           int64\n",
      "month_number                                  int64\n",
      "season_number                                 int64\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Check that start date is still in date time data type\n",
    "with pd.option_context('display.max_rows', None, 'display.max_columns', None):\n",
    "    print(test_data.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "966b6bb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# No pd.read_csv converted startdate back into an object\n",
    "# Convert start date to datetime data type for test data\n",
    "test_data['startdate'] = pd.to_datetime(test_data['startdate'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "b1b5bbb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index                                                int64\n",
      "lat                                                float64\n",
      "lon                                                float64\n",
      "startdate                                   datetime64[ns]\n",
      "contest-pevpr-sfc-gauss-14d__pevpr                 float64\n",
      "nmme0-tmp2m-34w__cancm30                           float64\n",
      "nmme0-tmp2m-34w__cancm40                           float64\n",
      "nmme0-tmp2m-34w__ccsm30                            float64\n",
      "nmme0-tmp2m-34w__ccsm40                            float64\n",
      "nmme0-tmp2m-34w__cfsv20                            float64\n",
      "nmme0-tmp2m-34w__gfdlflora0                        float64\n",
      "nmme0-tmp2m-34w__gfdlflorb0                        float64\n",
      "nmme0-tmp2m-34w__gfdl0                             float64\n",
      "nmme0-tmp2m-34w__nasa0                             float64\n",
      "nmme0-tmp2m-34w__nmme0mean                         float64\n",
      "contest-wind-h10-14d__wind-hgt-10                  float64\n",
      "nmme-tmp2m-56w__cancm3                             float64\n",
      "nmme-tmp2m-56w__cancm4                             float64\n",
      "nmme-tmp2m-56w__ccsm3                              float64\n",
      "nmme-tmp2m-56w__ccsm4                              float64\n",
      "nmme-tmp2m-56w__cfsv2                              float64\n",
      "nmme-tmp2m-56w__gfdl                               float64\n",
      "nmme-tmp2m-56w__gfdlflora                          float64\n",
      "nmme-tmp2m-56w__gfdlflorb                          float64\n",
      "nmme-tmp2m-56w__nasa                               float64\n",
      "nmme-tmp2m-56w__nmmemean                           float64\n",
      "contest-rhum-sig995-14d__rhum                      float64\n",
      "nmme-prate-34w__cancm3                             float64\n",
      "nmme-prate-34w__cancm4                             float64\n",
      "nmme-prate-34w__ccsm3                              float64\n",
      "nmme-prate-34w__ccsm4                              float64\n",
      "nmme-prate-34w__cfsv2                              float64\n",
      "nmme-prate-34w__gfdl                               float64\n",
      "nmme-prate-34w__gfdlflora                          float64\n",
      "nmme-prate-34w__gfdlflorb                          float64\n",
      "nmme-prate-34w__nasa                               float64\n",
      "nmme-prate-34w__nmmemean                           float64\n",
      "contest-wind-h100-14d__wind-hgt-100                float64\n",
      "nmme0-prate-56w__cancm30                           float64\n",
      "nmme0-prate-56w__cancm40                           float64\n",
      "nmme0-prate-56w__ccsm30                            float64\n",
      "nmme0-prate-56w__ccsm40                            float64\n",
      "nmme0-prate-56w__cfsv20                            float64\n",
      "nmme0-prate-56w__gfdlflora0                        float64\n",
      "nmme0-prate-56w__gfdlflorb0                        float64\n",
      "nmme0-prate-56w__gfdl0                             float64\n",
      "nmme0-prate-56w__nasa0                             float64\n",
      "nmme0-prate-56w__nmme0mean                         float64\n",
      "nmme0-prate-34w__cancm30                           float64\n",
      "nmme0-prate-34w__cancm40                           float64\n",
      "nmme0-prate-34w__ccsm30                            float64\n",
      "nmme0-prate-34w__ccsm40                            float64\n",
      "nmme0-prate-34w__cfsv20                            float64\n",
      "nmme0-prate-34w__gfdlflora0                        float64\n",
      "nmme0-prate-34w__gfdlflorb0                        float64\n",
      "nmme0-prate-34w__gfdl0                             float64\n",
      "nmme0-prate-34w__nasa0                             float64\n",
      "nmme0-prate-34w__nmme0mean                         float64\n",
      "contest-slp-14d__slp                               float64\n",
      "contest-wind-vwnd-925-14d__wind-vwnd-925           float64\n",
      "nmme-prate-56w__cancm3                             float64\n",
      "nmme-prate-56w__cancm4                             float64\n",
      "nmme-prate-56w__ccsm3                              float64\n",
      "nmme-prate-56w__ccsm4                              float64\n",
      "nmme-prate-56w__cfsv2                              float64\n",
      "nmme-prate-56w__gfdl                               float64\n",
      "nmme-prate-56w__gfdlflora                          float64\n",
      "nmme-prate-56w__gfdlflorb                          float64\n",
      "nmme-prate-56w__nasa                               float64\n",
      "nmme-prate-56w__nmmemean                           float64\n",
      "contest-pres-sfc-gauss-14d__pres                   float64\n",
      "contest-wind-uwnd-250-14d__wind-uwnd-250           float64\n",
      "nmme-tmp2m-34w__cancm3                             float64\n",
      "nmme-tmp2m-34w__cancm4                             float64\n",
      "nmme-tmp2m-34w__ccsm3                              float64\n",
      "nmme-tmp2m-34w__ccsm4                              float64\n",
      "nmme-tmp2m-34w__cfsv2                              float64\n",
      "nmme-tmp2m-34w__gfdl                               float64\n",
      "nmme-tmp2m-34w__gfdlflora                          float64\n",
      "nmme-tmp2m-34w__gfdlflorb                          float64\n",
      "nmme-tmp2m-34w__nasa                               float64\n",
      "nmme-tmp2m-34w__nmmemean                           float64\n",
      "contest-prwtr-eatm-14d__prwtr                      float64\n",
      "contest-wind-vwnd-250-14d__wind-vwnd-250           float64\n",
      "contest-precip-14d__precip                         float64\n",
      "contest-wind-h850-14d__wind-hgt-850                float64\n",
      "contest-wind-uwnd-925-14d__wind-uwnd-925           float64\n",
      "contest-wind-h500-14d__wind-hgt-500                float64\n",
      "cancm30                                            float64\n",
      "cancm40                                            float64\n",
      "ccsm30                                             float64\n",
      "ccsm40                                             float64\n",
      "cfsv20                                             float64\n",
      "gfdlflora0                                         float64\n",
      "gfdlflorb0                                         float64\n",
      "gfdl0                                              float64\n",
      "nasa0                                              float64\n",
      "nmme0mean                                          float64\n",
      "elevation__elevation                                 int64\n",
      "wind-vwnd-250-2010-1                               float64\n",
      "wind-vwnd-250-2010-2                               float64\n",
      "wind-vwnd-250-2010-3                               float64\n",
      "wind-vwnd-250-2010-4                               float64\n",
      "wind-vwnd-250-2010-5                               float64\n",
      "wind-vwnd-250-2010-6                               float64\n",
      "wind-vwnd-250-2010-7                               float64\n",
      "wind-vwnd-250-2010-8                               float64\n",
      "wind-vwnd-250-2010-9                               float64\n",
      "wind-vwnd-250-2010-10                              float64\n",
      "wind-vwnd-250-2010-11                              float64\n",
      "wind-vwnd-250-2010-12                              float64\n",
      "wind-vwnd-250-2010-13                              float64\n",
      "wind-vwnd-250-2010-14                              float64\n",
      "wind-vwnd-250-2010-15                              float64\n",
      "wind-vwnd-250-2010-16                              float64\n",
      "wind-vwnd-250-2010-17                              float64\n",
      "wind-vwnd-250-2010-18                              float64\n",
      "wind-vwnd-250-2010-19                              float64\n",
      "wind-vwnd-250-2010-20                              float64\n",
      "wind-uwnd-250-2010-1                               float64\n",
      "wind-uwnd-250-2010-2                               float64\n",
      "wind-uwnd-250-2010-3                               float64\n",
      "wind-uwnd-250-2010-4                               float64\n",
      "wind-uwnd-250-2010-5                               float64\n",
      "wind-uwnd-250-2010-6                               float64\n",
      "wind-uwnd-250-2010-7                               float64\n",
      "wind-uwnd-250-2010-8                               float64\n",
      "wind-uwnd-250-2010-9                               float64\n",
      "wind-uwnd-250-2010-10                              float64\n",
      "wind-uwnd-250-2010-11                              float64\n",
      "wind-uwnd-250-2010-12                              float64\n",
      "wind-uwnd-250-2010-13                              float64\n",
      "wind-uwnd-250-2010-14                              float64\n",
      "wind-uwnd-250-2010-15                              float64\n",
      "wind-uwnd-250-2010-16                              float64\n",
      "wind-uwnd-250-2010-17                              float64\n",
      "wind-uwnd-250-2010-18                              float64\n",
      "wind-uwnd-250-2010-19                              float64\n",
      "wind-uwnd-250-2010-20                              float64\n",
      "mjo1d__phase                                       float64\n",
      "mjo1d__amplitude                                   float64\n",
      "mei__mei                                           float64\n",
      "mei__meirank                                       float64\n",
      "mei__nip                                           float64\n",
      "wind-hgt-850-2010-1                                float64\n",
      "wind-hgt-850-2010-2                                float64\n",
      "wind-hgt-850-2010-3                                float64\n",
      "wind-hgt-850-2010-4                                float64\n",
      "wind-hgt-850-2010-5                                float64\n",
      "wind-hgt-850-2010-6                                float64\n",
      "wind-hgt-850-2010-7                                float64\n",
      "wind-hgt-850-2010-8                                float64\n",
      "wind-hgt-850-2010-9                                float64\n",
      "wind-hgt-850-2010-10                               float64\n",
      "sst-2010-1                                         float64\n",
      "sst-2010-2                                         float64\n",
      "sst-2010-3                                         float64\n",
      "sst-2010-4                                         float64\n",
      "sst-2010-5                                         float64\n",
      "sst-2010-6                                         float64\n",
      "sst-2010-7                                         float64\n",
      "sst-2010-8                                         float64\n",
      "sst-2010-9                                         float64\n",
      "sst-2010-10                                        float64\n",
      "wind-hgt-500-2010-1                                float64\n",
      "wind-hgt-500-2010-2                                float64\n",
      "wind-hgt-500-2010-3                                float64\n",
      "wind-hgt-500-2010-4                                float64\n",
      "wind-hgt-500-2010-5                                float64\n",
      "wind-hgt-500-2010-6                                float64\n",
      "wind-hgt-500-2010-7                                float64\n",
      "wind-hgt-500-2010-8                                float64\n",
      "wind-hgt-500-2010-9                                float64\n",
      "wind-hgt-500-2010-10                               float64\n",
      "icec-2010-1                                        float64\n",
      "icec-2010-2                                        float64\n",
      "icec-2010-3                                        float64\n",
      "icec-2010-4                                        float64\n",
      "icec-2010-5                                        float64\n",
      "icec-2010-6                                        float64\n",
      "icec-2010-7                                        float64\n",
      "icec-2010-8                                        float64\n",
      "icec-2010-9                                        float64\n",
      "icec-2010-10                                       float64\n",
      "wind-uwnd-925-2010-1                               float64\n",
      "wind-uwnd-925-2010-2                               float64\n",
      "wind-uwnd-925-2010-3                               float64\n",
      "wind-uwnd-925-2010-4                               float64\n",
      "wind-uwnd-925-2010-5                               float64\n",
      "wind-uwnd-925-2010-6                               float64\n",
      "wind-uwnd-925-2010-7                               float64\n",
      "wind-uwnd-925-2010-8                               float64\n",
      "wind-uwnd-925-2010-9                               float64\n",
      "wind-uwnd-925-2010-10                              float64\n",
      "wind-uwnd-925-2010-11                              float64\n",
      "wind-uwnd-925-2010-12                              float64\n",
      "wind-uwnd-925-2010-13                              float64\n",
      "wind-uwnd-925-2010-14                              float64\n",
      "wind-uwnd-925-2010-15                              float64\n",
      "wind-uwnd-925-2010-16                              float64\n",
      "wind-uwnd-925-2010-17                              float64\n",
      "wind-uwnd-925-2010-18                              float64\n",
      "wind-uwnd-925-2010-19                              float64\n",
      "wind-uwnd-925-2010-20                              float64\n",
      "wind-hgt-10-2010-1                                 float64\n",
      "wind-hgt-10-2010-2                                 float64\n",
      "wind-hgt-10-2010-3                                 float64\n",
      "wind-hgt-10-2010-4                                 float64\n",
      "wind-hgt-10-2010-5                                 float64\n",
      "wind-hgt-10-2010-6                                 float64\n",
      "wind-hgt-10-2010-7                                 float64\n",
      "wind-hgt-10-2010-8                                 float64\n",
      "wind-hgt-10-2010-9                                 float64\n",
      "wind-hgt-10-2010-10                                float64\n",
      "wind-hgt-100-2010-1                                float64\n",
      "wind-hgt-100-2010-2                                float64\n",
      "wind-hgt-100-2010-3                                float64\n",
      "wind-hgt-100-2010-4                                float64\n",
      "wind-hgt-100-2010-5                                float64\n",
      "wind-hgt-100-2010-6                                float64\n",
      "wind-hgt-100-2010-7                                float64\n",
      "wind-hgt-100-2010-8                                float64\n",
      "wind-hgt-100-2010-9                                float64\n",
      "wind-hgt-100-2010-10                               float64\n",
      "wind-vwnd-925-2010-1                               float64\n",
      "wind-vwnd-925-2010-2                               float64\n",
      "wind-vwnd-925-2010-3                               float64\n",
      "wind-vwnd-925-2010-4                               float64\n",
      "wind-vwnd-925-2010-5                               float64\n",
      "wind-vwnd-925-2010-6                               float64\n",
      "wind-vwnd-925-2010-7                               float64\n",
      "wind-vwnd-925-2010-8                               float64\n",
      "wind-vwnd-925-2010-9                               float64\n",
      "wind-vwnd-925-2010-10                              float64\n",
      "wind-vwnd-925-2010-11                              float64\n",
      "wind-vwnd-925-2010-12                              float64\n",
      "wind-vwnd-925-2010-13                              float64\n",
      "wind-vwnd-925-2010-14                              float64\n",
      "wind-vwnd-925-2010-15                              float64\n",
      "wind-vwnd-925-2010-16                              float64\n",
      "wind-vwnd-925-2010-17                              float64\n",
      "wind-vwnd-925-2010-18                              float64\n",
      "wind-vwnd-925-2010-19                              float64\n",
      "wind-vwnd-925-2010-20                              float64\n",
      "BSh                                                  int64\n",
      "BSk                                                  int64\n",
      "BWh                                                  int64\n",
      "BWk                                                  int64\n",
      "Cfa                                                  int64\n",
      "Cfb                                                  int64\n",
      "Csa                                                  int64\n",
      "Csb                                                  int64\n",
      "Dfa                                                  int64\n",
      "Dfb                                                  int64\n",
      "Dfc                                                  int64\n",
      "Dsb                                                  int64\n",
      "Dsc                                                  int64\n",
      "Dwa                                                  int64\n",
      "Dwb                                                  int64\n",
      "month_number                                         int64\n",
      "season_number                                        int64\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Check that start date is still in date time data type\n",
    "with pd.option_context('display.max_rows', None, 'display.max_columns', None):\n",
    "    print(test_data.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "6757808a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set index to datetime feature of start date and sort by startdate\n",
    "# Make copy to keep training data with original index\n",
    "time_test_data = test_data.copy()\n",
    "time_test_data.set_index('startdate', inplace=True)\n",
    "time_test_data.sort_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "49df2ef2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>lat</th>\n",
       "      <th>lon</th>\n",
       "      <th>contest-pevpr-sfc-gauss-14d__pevpr</th>\n",
       "      <th>nmme0-tmp2m-34w__cancm30</th>\n",
       "      <th>nmme0-tmp2m-34w__cancm40</th>\n",
       "      <th>nmme0-tmp2m-34w__ccsm30</th>\n",
       "      <th>nmme0-tmp2m-34w__ccsm40</th>\n",
       "      <th>nmme0-tmp2m-34w__cfsv20</th>\n",
       "      <th>nmme0-tmp2m-34w__gfdlflora0</th>\n",
       "      <th>...</th>\n",
       "      <th>Csb</th>\n",
       "      <th>Dfa</th>\n",
       "      <th>Dfb</th>\n",
       "      <th>Dfc</th>\n",
       "      <th>Dsb</th>\n",
       "      <th>Dsc</th>\n",
       "      <th>Dwa</th>\n",
       "      <th>Dwb</th>\n",
       "      <th>month_number</th>\n",
       "      <th>season_number</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>startdate</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2022-11-01</th>\n",
       "      <td>375734</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>339.88</td>\n",
       "      <td>30.88</td>\n",
       "      <td>30.92</td>\n",
       "      <td>29.17</td>\n",
       "      <td>31.02</td>\n",
       "      <td>29.47</td>\n",
       "      <td>30.93</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-11-01</th>\n",
       "      <td>404099</td>\n",
       "      <td>0.954545</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>224.64</td>\n",
       "      <td>16.43</td>\n",
       "      <td>17.98</td>\n",
       "      <td>14.80</td>\n",
       "      <td>15.92</td>\n",
       "      <td>14.35</td>\n",
       "      <td>13.83</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-11-01</th>\n",
       "      <td>394827</td>\n",
       "      <td>0.681818</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>417.74</td>\n",
       "      <td>22.07</td>\n",
       "      <td>24.84</td>\n",
       "      <td>21.59</td>\n",
       "      <td>23.14</td>\n",
       "      <td>20.91</td>\n",
       "      <td>20.28</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-11-01</th>\n",
       "      <td>386287</td>\n",
       "      <td>0.454545</td>\n",
       "      <td>0.766667</td>\n",
       "      <td>411.23</td>\n",
       "      <td>28.95</td>\n",
       "      <td>33.25</td>\n",
       "      <td>29.80</td>\n",
       "      <td>29.09</td>\n",
       "      <td>26.27</td>\n",
       "      <td>30.82</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-11-01</th>\n",
       "      <td>390679</td>\n",
       "      <td>0.590909</td>\n",
       "      <td>0.266667</td>\n",
       "      <td>433.60</td>\n",
       "      <td>23.09</td>\n",
       "      <td>23.65</td>\n",
       "      <td>21.23</td>\n",
       "      <td>24.18</td>\n",
       "      <td>21.78</td>\n",
       "      <td>22.14</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 260 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             index       lat       lon  contest-pevpr-sfc-gauss-14d__pevpr  \\\n",
       "startdate                                                                    \n",
       "2022-11-01  375734  0.000000  0.833333                              339.88   \n",
       "2022-11-01  404099  0.954545  0.100000                              224.64   \n",
       "2022-11-01  394827  0.681818  0.600000                              417.74   \n",
       "2022-11-01  386287  0.454545  0.766667                              411.23   \n",
       "2022-11-01  390679  0.590909  0.266667                              433.60   \n",
       "\n",
       "            nmme0-tmp2m-34w__cancm30  nmme0-tmp2m-34w__cancm40  \\\n",
       "startdate                                                        \n",
       "2022-11-01                     30.88                     30.92   \n",
       "2022-11-01                     16.43                     17.98   \n",
       "2022-11-01                     22.07                     24.84   \n",
       "2022-11-01                     28.95                     33.25   \n",
       "2022-11-01                     23.09                     23.65   \n",
       "\n",
       "            nmme0-tmp2m-34w__ccsm30  nmme0-tmp2m-34w__ccsm40  \\\n",
       "startdate                                                      \n",
       "2022-11-01                    29.17                    31.02   \n",
       "2022-11-01                    14.80                    15.92   \n",
       "2022-11-01                    21.59                    23.14   \n",
       "2022-11-01                    29.80                    29.09   \n",
       "2022-11-01                    21.23                    24.18   \n",
       "\n",
       "            nmme0-tmp2m-34w__cfsv20  nmme0-tmp2m-34w__gfdlflora0  ...  Csb  \\\n",
       "startdate                                                         ...        \n",
       "2022-11-01                    29.47                        30.93  ...    0   \n",
       "2022-11-01                    14.35                        13.83  ...    0   \n",
       "2022-11-01                    20.91                        20.28  ...    0   \n",
       "2022-11-01                    26.27                        30.82  ...    0   \n",
       "2022-11-01                    21.78                        22.14  ...    0   \n",
       "\n",
       "            Dfa  Dfb  Dfc  Dsb  Dsc  Dwa  Dwb  month_number  season_number  \n",
       "startdate                                                                   \n",
       "2022-11-01    0    0    0    0    0    0    0            11              4  \n",
       "2022-11-01    0    0    0    0    1    0    0            11              4  \n",
       "2022-11-01    0    0    0    0    0    0    0            11              4  \n",
       "2022-11-01    0    0    0    0    0    0    0            11              4  \n",
       "2022-11-01    0    1    0    0    0    0    0            11              4  \n",
       "\n",
       "[5 rows x 260 columns]"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "time_test_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "a2df096c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(31354, 260)"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Check to make sure test_data and X are of the same shape - Good to go!\n",
    "time_test_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "e8717fe2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(375734, 260)"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b83cd6eb",
   "metadata": {},
   "source": [
    "### Fit RFR on Full training_data from WiDS\n",
    "- We can use the full X and y dataset to train our model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "bc9439b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Instatiate\n",
    "RFRmodel_final = RandomForestRegressor(n_estimators=100, max_features = None, \\\n",
    "                                 verbose = 2, random_state=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "61c66ec3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building tree 1 of 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:   47.1s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building tree 2 of 100\n",
      "building tree 3 of 100\n",
      "building tree 4 of 100\n",
      "building tree 5 of 100\n",
      "building tree 6 of 100\n",
      "building tree 7 of 100\n",
      "building tree 8 of 100\n",
      "building tree 9 of 100\n",
      "building tree 10 of 100\n",
      "building tree 11 of 100\n",
      "building tree 12 of 100\n",
      "building tree 13 of 100\n",
      "building tree 14 of 100\n",
      "building tree 15 of 100\n",
      "building tree 16 of 100\n",
      "building tree 17 of 100\n",
      "building tree 18 of 100\n",
      "building tree 19 of 100\n",
      "building tree 20 of 100\n",
      "building tree 21 of 100\n",
      "building tree 22 of 100\n",
      "building tree 23 of 100\n",
      "building tree 24 of 100\n",
      "building tree 25 of 100\n",
      "building tree 26 of 100\n",
      "building tree 27 of 100\n",
      "building tree 28 of 100\n",
      "building tree 29 of 100\n",
      "building tree 30 of 100\n",
      "building tree 31 of 100\n",
      "building tree 32 of 100\n",
      "building tree 33 of 100\n",
      "building tree 34 of 100\n",
      "building tree 35 of 100\n",
      "building tree 36 of 100\n",
      "building tree 37 of 100\n",
      "building tree 38 of 100\n",
      "building tree 39 of 100\n",
      "building tree 40 of 100\n",
      "building tree 41 of 100\n",
      "building tree 42 of 100\n",
      "building tree 43 of 100\n",
      "building tree 44 of 100\n",
      "building tree 45 of 100\n",
      "building tree 46 of 100\n",
      "building tree 47 of 100\n",
      "building tree 48 of 100\n",
      "building tree 49 of 100\n",
      "building tree 50 of 100\n",
      "building tree 51 of 100\n",
      "building tree 52 of 100\n",
      "building tree 53 of 100\n",
      "building tree 54 of 100\n",
      "building tree 55 of 100\n",
      "building tree 56 of 100\n",
      "building tree 57 of 100\n",
      "building tree 58 of 100\n",
      "building tree 59 of 100\n",
      "building tree 60 of 100\n",
      "building tree 61 of 100\n",
      "building tree 62 of 100\n",
      "building tree 63 of 100\n",
      "building tree 64 of 100\n",
      "building tree 65 of 100\n",
      "building tree 66 of 100\n",
      "building tree 67 of 100\n",
      "building tree 68 of 100\n",
      "building tree 69 of 100\n",
      "building tree 70 of 100\n",
      "building tree 71 of 100\n",
      "building tree 72 of 100\n",
      "building tree 73 of 100\n",
      "building tree 74 of 100\n",
      "building tree 75 of 100\n",
      "building tree 76 of 100\n",
      "building tree 77 of 100\n",
      "building tree 78 of 100\n",
      "building tree 79 of 100\n",
      "building tree 80 of 100\n",
      "building tree 81 of 100\n",
      "building tree 82 of 100\n",
      "building tree 83 of 100\n",
      "building tree 84 of 100\n",
      "building tree 85 of 100\n",
      "building tree 86 of 100\n",
      "building tree 87 of 100\n",
      "building tree 88 of 100\n",
      "building tree 89 of 100\n",
      "building tree 90 of 100\n",
      "building tree 91 of 100\n",
      "building tree 92 of 100\n",
      "building tree 93 of 100\n",
      "building tree 94 of 100\n",
      "building tree 95 of 100\n",
      "building tree 96 of 100\n",
      "building tree 97 of 100\n",
      "building tree 98 of 100\n",
      "building tree 99 of 100\n",
      "building tree 100 of 100\n",
      "CPU times: user 1h 18min 57s, sys: 20.8 s, total: 1h 19min 18s\n",
      "Wall time: 1h 19min 54s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed: 79.9min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomForestRegressor(max_features=None, random_state=32, verbose=2)"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "#Fit Model to complete training set\n",
    "RFRmodel_final.fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "6ead4c3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:    5.8s finished\n"
     ]
    }
   ],
   "source": [
    "# Obtain predictions from RFR_final model with complete test_data from WiDS\n",
    "pred = RFRmodel_final.predict(time_test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "3592a73b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gather predictions and index for submission dataframe\n",
    "submission = pd.DataFrame(data = {'Predictions':pred,'Index':time_test_data['index']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "9723f708",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resort values by index\n",
    "submission=submission.sort_values(by='Index', ascending=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "161aab11",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Predictions</th>\n",
       "      <th>Index</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>startdate</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2022-11-01</th>\n",
       "      <td>28.385288</td>\n",
       "      <td>375734</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-11-02</th>\n",
       "      <td>28.325583</td>\n",
       "      <td>375735</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-11-03</th>\n",
       "      <td>28.530644</td>\n",
       "      <td>375736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-11-04</th>\n",
       "      <td>28.483178</td>\n",
       "      <td>375737</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-11-05</th>\n",
       "      <td>28.648197</td>\n",
       "      <td>375738</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-12-27</th>\n",
       "      <td>3.478709</td>\n",
       "      <td>407083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-12-28</th>\n",
       "      <td>3.660469</td>\n",
       "      <td>407084</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-12-29</th>\n",
       "      <td>3.361483</td>\n",
       "      <td>407085</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-12-30</th>\n",
       "      <td>2.882080</td>\n",
       "      <td>407086</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-12-31</th>\n",
       "      <td>2.885870</td>\n",
       "      <td>407087</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>31354 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            Predictions   Index\n",
       "startdate                      \n",
       "2022-11-01    28.385288  375734\n",
       "2022-11-02    28.325583  375735\n",
       "2022-11-03    28.530644  375736\n",
       "2022-11-04    28.483178  375737\n",
       "2022-11-05    28.648197  375738\n",
       "...                 ...     ...\n",
       "2022-12-27     3.478709  407083\n",
       "2022-12-28     3.660469  407084\n",
       "2022-12-29     3.361483  407085\n",
       "2022-12-30     2.882080  407086\n",
       "2022-12-31     2.885870  407087\n",
       "\n",
       "[31354 rows x 2 columns]"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check\n",
    "submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "939f7d27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop start date index\n",
    "submission = submission.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "1aa99779",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>startdate</th>\n",
       "      <th>Predictions</th>\n",
       "      <th>Index</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2022-11-01</td>\n",
       "      <td>28.385288</td>\n",
       "      <td>375734</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2022-11-02</td>\n",
       "      <td>28.325583</td>\n",
       "      <td>375735</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2022-11-03</td>\n",
       "      <td>28.530644</td>\n",
       "      <td>375736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2022-11-04</td>\n",
       "      <td>28.483178</td>\n",
       "      <td>375737</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2022-11-05</td>\n",
       "      <td>28.648197</td>\n",
       "      <td>375738</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31349</th>\n",
       "      <td>2022-12-27</td>\n",
       "      <td>3.478709</td>\n",
       "      <td>407083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31350</th>\n",
       "      <td>2022-12-28</td>\n",
       "      <td>3.660469</td>\n",
       "      <td>407084</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31351</th>\n",
       "      <td>2022-12-29</td>\n",
       "      <td>3.361483</td>\n",
       "      <td>407085</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31352</th>\n",
       "      <td>2022-12-30</td>\n",
       "      <td>2.882080</td>\n",
       "      <td>407086</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31353</th>\n",
       "      <td>2022-12-31</td>\n",
       "      <td>2.885870</td>\n",
       "      <td>407087</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>31354 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       startdate  Predictions   Index\n",
       "0     2022-11-01    28.385288  375734\n",
       "1     2022-11-02    28.325583  375735\n",
       "2     2022-11-03    28.530644  375736\n",
       "3     2022-11-04    28.483178  375737\n",
       "4     2022-11-05    28.648197  375738\n",
       "...          ...          ...     ...\n",
       "31349 2022-12-27     3.478709  407083\n",
       "31350 2022-12-28     3.660469  407084\n",
       "31351 2022-12-29     3.361483  407085\n",
       "31352 2022-12-30     2.882080  407086\n",
       "31353 2022-12-31     2.885870  407087\n",
       "\n",
       "[31354 rows x 3 columns]"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "5342c428",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1000x700 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Visualize mean daily prediction temps over startdate\n",
    "plt.figure()\n",
    "submission.groupby(by=['startdate']).mean()['Predictions'].plot()\n",
    "plt.title('Mean Predicted Temperature/Target vs. Date of Test_Data')\n",
    "plt.ylabel('Target - Mean Temp - contest-tmp2m-14d__tmp2m - Celsius')\n",
    "plt.xlabel('Start Date')\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30852ef8",
   "metadata": {},
   "source": [
    "These temps look reasonable and we will proceed with the submission. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "f3001e3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename predictions to target variable as in sample solution from WiDS\n",
    "submission = submission.rename(columns = {'Predictions':'contest-tmp2m-14d__tmp2m'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "3677d4a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop start date to match sample solution\n",
    "submission = submission.drop(['startdate'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "047dd902",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>contest-tmp2m-14d__tmp2m</th>\n",
       "      <th>Index</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>28.385288</td>\n",
       "      <td>375734</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>28.325583</td>\n",
       "      <td>375735</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>28.530644</td>\n",
       "      <td>375736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>28.483178</td>\n",
       "      <td>375737</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>28.648197</td>\n",
       "      <td>375738</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31349</th>\n",
       "      <td>3.478709</td>\n",
       "      <td>407083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31350</th>\n",
       "      <td>3.660469</td>\n",
       "      <td>407084</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31351</th>\n",
       "      <td>3.361483</td>\n",
       "      <td>407085</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31352</th>\n",
       "      <td>2.882080</td>\n",
       "      <td>407086</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31353</th>\n",
       "      <td>2.885870</td>\n",
       "      <td>407087</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>31354 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       contest-tmp2m-14d__tmp2m   Index\n",
       "0                     28.385288  375734\n",
       "1                     28.325583  375735\n",
       "2                     28.530644  375736\n",
       "3                     28.483178  375737\n",
       "4                     28.648197  375738\n",
       "...                         ...     ...\n",
       "31349                  3.478709  407083\n",
       "31350                  3.660469  407084\n",
       "31351                  3.361483  407085\n",
       "31352                  2.882080  407086\n",
       "31353                  2.885870  407087\n",
       "\n",
       "[31354 rows x 2 columns]"
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check\n",
    "submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "fa6b2139",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in sample solution to compare format\n",
    "sample_solution = pd.read_csv('data/sample_solution.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "0acf51cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>contest-tmp2m-14d__tmp2m</th>\n",
       "      <th>index</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>27.073876</td>\n",
       "      <td>375734</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>25.109308</td>\n",
       "      <td>375735</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>22.557390</td>\n",
       "      <td>375736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>25.572875</td>\n",
       "      <td>375737</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20.781073</td>\n",
       "      <td>375738</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31349</th>\n",
       "      <td>28.303967</td>\n",
       "      <td>407083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31350</th>\n",
       "      <td>26.635933</td>\n",
       "      <td>407084</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31351</th>\n",
       "      <td>27.057762</td>\n",
       "      <td>407085</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31352</th>\n",
       "      <td>26.871066</td>\n",
       "      <td>407086</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31353</th>\n",
       "      <td>21.253714</td>\n",
       "      <td>407087</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>31354 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       contest-tmp2m-14d__tmp2m   index\n",
       "0                     27.073876  375734\n",
       "1                     25.109308  375735\n",
       "2                     22.557390  375736\n",
       "3                     25.572875  375737\n",
       "4                     20.781073  375738\n",
       "...                         ...     ...\n",
       "31349                 28.303967  407083\n",
       "31350                 26.635933  407084\n",
       "31351                 27.057762  407085\n",
       "31352                 26.871066  407086\n",
       "31353                 21.253714  407087\n",
       "\n",
       "[31354 rows x 2 columns]"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check - Our submission is in the same format as the sample solution. \n",
    "sample_solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "c98309fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Export Submission\n",
    "submission.to_csv('data/submission_mack.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "id": "1446c2a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Export time_test_data\n",
    "time_test_data.to_csv('data/time_test_data_clean.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "230b4252",
   "metadata": {},
   "source": [
    "# Notebook 2 Conclusion\n",
    "Workflow Initial MLPRegressor --> MLPRegressor Optimization --> MLPR vs. RFR Comparison--> RFR Baseline WiDS Submission\n",
    "\n",
    "We have refined our ability to prepare and evaluate models in this notebook.  We adopted a data reduction strategy to reduce each parameter's data type to minimize memory load and run times. We determined that the regressor models have quicker run times on my local machine over Google Collab.  We can look into other methods of running cloud-based models such as AWS if run times becoming a major in inhibition to advancement.  \n",
    "\n",
    "My task as part of my WiDS team was to evaluate the Random Forest Regressor and Multi-layer Perceptron Regressor models. Both models have proven effective in predictive regression time series modeling.  \n",
    "\n",
    "After optimizing the MLPRegressor, we see that the Random Forest Regressor optimized in Notebook 1 achieved better scoring metrics.  We wanted to make sure to get in a submission to WiDS to establish our baseline and know that we could properly format a submission.\n",
    "\n",
    "After fitting our optimized RFR with the full `training_data` from WiDS, we ran the WiDS `test_data` through the RFR model to gain predictions for our baseline submission.  We successfully made the submission to the WiDS Kaggle site.  WiDS evaluated our predictions on the actual target mean temperatures and we scored a root mean squared error of 1.91 degrees celsius.  This gives us a good baseline, and we will feel confident formatting submissions in the future. \n",
    "\n",
    "We will need to carry out a variety of steps in successive notebooks to achieve better results. \n",
    "\n",
    "**Next Steps**\n",
    "- Feature engineering\n",
    "    - We will research steps to boost the predictive power of our features.\n",
    "- More comprehensive model evaluation\n",
    "    - We will need to extend our grid searches to other regression models.  \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
